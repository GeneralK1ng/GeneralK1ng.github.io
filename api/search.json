[{"id":"8bd50a6b3cea7ecea2a8063e70b7409b","title":"正规方程","content":"引入在机器学习和线性回归中，我们经常需要通过训练数据来学习参数，以便建立一个能够准确预测目标变量的模型。前面我们已经介绍了梯度下降法，这是一种常用的优化算法，可以帮助我们找到最小化成本函数的参数值。\n除了梯度下降法，还有一种有趣而简洁的方法可以解决线性回归问题，它被称为正规方程（The Normal Equations）。正规方程提供了一种通过代数方法直接求解最优参数的方式，而不需要像梯度下降法那样迭代更新。\n让我们以一个生动的例子来理解正规方程。假设你是一名厨师，你想要制作一道美味的蛋糕。你知道蛋糕的味道取决于配料的种类和数量。你希望找到一个最佳的配料组合，使得蛋糕的口感和味道达到最佳。\n为了解决这个问题，你决定进行一系列实验。你准备了不同数量和种类的配料，并且每次制作蛋糕后，让一群品尝师评价蛋糕的口感。你记录下每个实验中使用的配料种类和数量，以及对应的评分。\n现在，你的目标是通过这些实验数据，找到一种最佳的配料组合，以获得最佳的蛋糕口感。你想要建立一个线性模型，通过配料的数量和种类预测口感评分。这个问题就可以转化为一个线性回归问题。\n正规方程提供了一种直接求解线性回归参数的方法。它的原理类似于代数方程的求解过程。通过对训练数据进行数学运算，我们可以得到一个公式，可以直接计算出最优的参数值。\n正规方程不需要像梯度下降法那样进行迭代更新，因此在某些情况下，它可能更加高效。然而，正规方程也有一些限制，例如当特征数量非常大时，计算复杂度会增加。\n在接下来的部分，我们将详细介绍正规方程的原理和应用。正规方程为我们提供了一种有趣而直接的方式来解决线性回归问题，让我们一起探索吧！\n正规方程正规方程是一种通过代数方法直接求解线性回归参数的方法，而不需要像梯度下降法那样进行迭代更新。它的原理是通过最小化成本函数，找到使得预测值与实际值之间差异最小的参数值。\n为了理解正规方程的原理，让我们再回到蛋糕制作的例子。你已经进行了一系列实验，记录了不同配料组合的口感评分。现在，你想要找到最佳的配料组合，使得蛋糕的口感评分最高。\n回忆一下线性回归模型的表示形式：。我们的目标是找到一组最优的参数，使得尽可能接近实际的口感评分。\n我们定义成本函数来衡量预测值与实际值之间的差异。对于线性回归问题，我们通常使用平方差误差（SSE）作为成本函数，即，其中是训练样本的数量。\n现在，我们的目标是找到最优的参数，使得成本函数最小化。而正规方程就提供了一种求解最优参数的解析解。具体来说，我们通过对关于参数的导数进行求解，并将其设置为零来最小化。\n在进行矩阵表示时，我们将训练样本的特征向量表示为矩阵，其中每一行代表一个样本的特征，每一列代表一个特征维度。类似地，我们将实际值表示为向量。那么，线性回归模型可以写成矩阵形式：。\n应用矩阵微积分的概念，我们可以求解成本函数关于参数的导数。这个导数称为梯度（gradient），用表示。当梯度为零时，我们得到正规方程的解。\n通过代数计算，我们可以得到正规方程的表达式：。这个表达式直接给出了最优参数的解析解。\n正规方程的优点在于它不需要进行迭代更新，可以直接得到最优参数的解析解。然而，它的计算复杂度取决于特征的数量，当特征数量非常大时，求解逆矩阵的计算可能变得耗时。\n在实际应用中，我们可以根据问题的特点选择使用梯度下降法还是正规方程。梯度下降法适用于大规模数据集和高维特征空间，而正规方程适用于小规模数据集和低维特征空间。\n矩阵导数矩阵导数是矩阵微积分中的重要概念，用于描述函数对矩阵变量的导数。在矩阵导数中，我们将函数从一个行列的矩阵映射到实数，定义为。为了求解矩阵导数，我们需要计算函数相对于矩阵的偏导数。矩阵导数本身也是一个行列的矩阵，其中元素表示函数对的偏导数。\n例如，假设是一个2行2列的矩阵，函数：定义为：在这个例子中，表示矩阵的元素。\n我们可以通过计算偏导数来得到矩阵导数的表达式。根据定义，我们计算对每个的偏导数，然后将它们组合成矩阵的形式。对于我们的例子，我们可以得到如下结果：这个结果展示了矩阵导数的计算方式。每个元素都是相应偏导数的结果。例如，元素是对的偏导数，元素是对的偏导数，以此类推。\n最小二乘法再探讨借助矩阵导数的工具，现在让我们通过闭式解来找到使最小化的的值。我们首先将用矩阵-向量表示法重新书写。\n给定一个训练集，定义设计矩阵为一个行列的矩阵（实际上是行列，如果我们包括截距项），其行包含训练示例的输入值：此外，让是一个维向量，包含来自训练集的所有目标值：现在，由于，我们可以很容易地验证：因此，利用向量的性质，我们有 ：最后，为了最小化，让我们找到它相对于的导数。因此，我们有：在第三步中，我们使用了的事实，在第五步中使用了和的事实，其中是对称矩阵。为了最小化，我们将其导数设为零，得到正规方程：因此，最小化的值可以通过以下方程的闭式解给出：\n\n\n\n\n\n\n\n\n\n请注意，在上述步骤中，我们隐含地假设是可逆矩阵。在计算逆矩阵之前，可以进行检查。如果线性无关的样本数量少于特征数量，或者特征不是线性无关的，则将不可逆。即使在这种情况下，也有可能通过额外的技术来“修正”这种情况，但为了简洁起见，我们在这里省略了这些内容。\n举例我们再回到最开始那个做烘焙的例子，假设你是一位厨师，想要研究面包的烘焙时间和温度之间的关系。你收集了一系列实验数据，记录下了烘焙时间和使用的温度。现在，你想要找到一个数学模型来预测未来的烘焙时间。这时，线性回归和最小二乘法就能派上用场了。\n现在，让我们使用矩阵导数的工具来重新表达成本函数。首先，我们定义设计矩阵，它是一个行列的矩阵，其中每一行包含一个实验样本的特征值（在我们的例子中就是温度）。我们还定义目标向量，它是一个维向量，包含对应每个实验样本的烘焙时间。\n我们收集了一些数据，记录了不同温度下烘焙面包所需的时间。现在我们要使用这些数据来训练一个线性回归模型，以便我们可以根据温度来预测烘焙时间。\n首先，让我们创建一个虚构的数据集（这是我用程序随机生成的）。假设我们有以下数据：\n\nClick to see more\n\n\n\n温度（摄氏度）\n烘焙时间（分钟）\n\n\n\n168.7\n41.4\n\n\n197.5\n52.2\n\n\n186.6\n45.0\n\n\n179.9\n43.7\n\n\n157.8\n42.8\n\n\n157.8\n32.0\n\n\n152.9\n31.9\n\n\n193.3\n45.8\n\n\n180.1\n41.0\n\n\n185.4\n48.7\n\n\n\n\n\n\n\n\n现在，我们将数据表示为输入特征矩阵和目标变量向量。是一个包含温度特征的矩阵，是对应的烘焙时间。\n接下来，我们将为矩阵添加截距项列。这样，矩阵的第一列将始终为1，以表示截距(intercept)。\n现在，我们可以使用正规方程来求解参数。正规方程的公式为：\n这个公式会给出使得模型最优拟合数据的参数值。\n控制台会输出：\nIntercept: -19.32\nSlope: 0.35\n\n\n\n\n\n为了应用这个公式，我们需要使用Python进行计算。我们可以使用NumPy库来进行矩阵运算。让我们来看看如何在Python中计算正规方程的闭式解：\n\nClick to see more\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)  # 设置随机种子，以便结果可复现\n\n# 创建输入特征 X（温度）和目标变量 y（烘焙时间）\nX = np.random.uniform(150, 200, 10).reshape(-1, 1)  # 温度范围在150到200之间\ny = 10 + 0.2 * X + np.random.normal(0, 5, 10).reshape(-1, 1)  # 烘焙时间=10 + 0.2*温度 + 噪声\n\n# 添加截距项列（全为1）到 X 矩阵中\nX = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n\n# 计算参数 theta\ntheta = np.linalg.inv(X.T @ X) @ X.T @ y\n\nintercept = theta[0][0]\nslope = theta[1][0]\nprint(f\"Intercept: {intercept:.2f}\")\nprint(f\"Slope: {slope:.2f}\")\n\n\n# 绘制数据点\nplt.scatter(X[:, 1], y, label=\"Data\")\n\n# 绘制拟合的线性回归模型\nx_line = np.linspace(150, 200, 100)\ny_line = intercept + slope * x_line\nplt.plot(x_line, y_line, color=\"red\", label=\"Linear Regression\")\n\n# 添加标签和图例\nplt.xlabel(\"Temperature\")\nplt.ylabel(\"Baking Time\")\nplt.legend()\n\n# 显示图形\nplt.show()\n\n\n\n\n运行这段代码，我们会得到参数的值。在这个例子中，我们会得到两个参数：截距项参数和温度参数。这些参数表示烘焙时间与温度之间的线性关系。\n一旦我们求解出参数，我们就可以使用它来进行预测。给定一个新的温度值，我们可以通过计算来预测对应的烘焙时间。\n这节的内容会比较难以理解，希望可以反复阅读，如有疑问欢迎提出，说实话，我都有些没看懂。\n","slug":"The-normal-equations","date":"2023-07-17T04:09:05.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"eee27f691084ae51bee755f5fe41f548","title":"LMS算法","content":"欢迎来到新的一部分！现在我们将介绍一种非常有趣的算法，它被称为最小均方（Least Mean Squares，LMS）算法。这是一种用于优化线性回归模型的算法，它可以帮助我们找到最佳的参数组合，使得我们的预测结果与实际观测值之间的差异最小化。\nLMS算法实际上是一个非常聪明的算法，它的灵感来自于我们人类在学习过程中的一种思维方式。想象一下，当你在学习骑自行车或者学习弹吉他时，你并不会一次就掌握所有技巧。相反，你会不断地试验、调整和改进，直到你的动作越来越接近完美。\nLMS算法的原理也是类似的。它通过逐步调整模型的参数来最小化成本函数，就像我们逐步调整我们的动作来提高技能一样。在每一步中，LMS算法会计算出当前参数设置下的成本函数值，并根据这个值来调整参数，以便使下一次迭代的预测结果更接近实际观测值。这个过程就像是在不断地微调模型，让它的预测能够更准确地拟合实际数据。\n一个有趣的比喻是，想象你是一名音乐家，正在调音吉他。你会先弹奏一根弦，然后通过调整琴弦的张力来使音高趋近于理想的音高。LMS算法的工作方式类似于这个过程，它会在每一步中微调模型的参数，使得预测结果逐渐接近实际观测值，就像音高逐渐趋近理想音高一样。\nLMS算法是一种非常强大和常用的优化算法，特别适用于解决线性回归问题。它不仅可以用于预测骑行时间，还可以应用于各种其他领域，如金融、医疗和天气预测等。通过不断地微调参数，LMS算法帮助我们找到最佳的模型参数，使我们的预测结果更加准确和可靠。\nLMS算法我们继续探讨LMS算法！让我们首先回顾一下我们的目标：选择合适的参数来最小化成本函数。为了实现这一目标，我们需要一个搜索算法，该算法从一个”初始猜测”开始选择，然后通过迭代改变以使逐渐变小，直到我们收敛到使最小化的值。\n\n\n\n\n\n\n\n\n\n我们使用符号“a := b”来表示一种操作（在计算机程序中），该操作将变量a的值设置为等于变量b的值。换句话说，这个操作会用b的值覆盖a的值。相反，当我们在断言一个事实时，即a的值等于b的值，我们会写成“a = b”。\n那么，我们如何在每次迭代中更新参数呢？这就是LMS算法的精髓所在。它使用一种称为梯度下降的方法，以最陡的下降方向更新参数。具体而言，在每次迭代中，我们将参数更新为，其中是学习率。\n为了更好地理解LMS算法的更新规则，让我们以一个简单的例子来说明。假设我们只有一个训练样本，其中是我们的输入特征，是对应的目标值。我们的目标是根据输入预测出目标值。我们将使用线性模型来进行预测。\n这是一个非常自然的算法，它重复地朝着最陡的下降方向迈出一步。为了实现这个算法，我们需要计算出右侧的偏导数项。让我们首先计算出在只有一个训练样本的情况下的结果，这样我们就可以忽略的定义中的求和符号。为了更新参数和，我们需要计算成本函数对于每个参数的偏导数。通过计算，我们得到偏导数的表达式为：。这表明参数的更新量与误差项成比例，以及输入特征。根据这个结果，我们可以得到LMS算法的更新规则：这个规则被称为LMS更新规则（LMS代表“最小均方差”），也被称为Widrow-Hoff学习规则。这个规则具有几个看起来自然而直观的特性。例如，更新的幅度与误差项成比例；因此，例如，如果我们遇到一个训练样本，我们的预测几乎与的实际值相匹配，那么我们发现几乎不需要改变参数；相反，如果我们的预测与有较大的误差（即相距较远），那么参数将会有较大的变化。\n梯度下降法当我们使用机器学习算法解决问题时，经常需要最小化一个函数。梯度下降法（Gradient Descent）是一种常用的优化算法，用于找到函数的最小值。\n让我们通过一个生动的例子来解释梯度下降法。假设你是一位登山爱好者，目标是从山顶下到山脚的最短路径。你置身于山顶，但是你没有任何线索告诉你应该往哪个方向走。你面前一片云雾茫茫，你看不到山脚。然而，你手上有一个高度计可以告诉你当前的海拔高度。\n你的目标是找到一条最快的下山路径。你知道山的地形图呈现出一种斜率或坡度。你也知道下山的最陡峭方向就是当前位置的负梯度方向。\n梯度下降法的思想类似于登山者的行动。你观察当前位置的海拔高度，并朝着最陡峭的下坡方向迈出一步。然后，你再次观察新位置的海拔高度，继续朝着最陡峭的下坡方向迈出一步。你不断重复这个过程，逐步接近山脚。\n在数学中，我们使用类似的思想来最小化函数。假设我们有一个函数，我们希望找到它的最小值。梯度下降法通过迭代地计算函数的梯度，沿着梯度的反方向更新参数，以使函数的值逐渐减小。\n函数的梯度是一个向量，指示函数在给定点上最陡峭的上升方向。我们的目标是朝着最陡峭的下降方向前进，因此我们朝着梯度的反方向更新参数。\n\n\n\n\n\n\nTIP\n这个过程可以表示为以下步骤：\n\n初始化参数：选择一个初始的参数值作为起点。\n计算梯度：计算函数在当前参数值处的梯度。梯度告诉我们函数在该点上升的方向和速度。\n更新参数：根据梯度的方向和一个称为学习率的调整参数，更新参数值。\n重复步骤2和步骤3，直到达到停止条件。停止条件可以是达到最大迭代次数、参数变化很小或函数值达到某个阈值。\n\n\n\n通过迭代更新参数，梯度下降法能够找到函数的局部最小值或全局最小值，这取决于函数的性质和初始参数的选择。\n梯度下降法在机器学习中扮演重要角色，它是许多算法的基础，例如线性回归、逻辑回归和神经网络。它允许我们通过最小化成本函数来调整模型的参数，使模型更好地拟合训练数据。\n批量梯度下降我们已经推导出了适用于单个训练样本的LMS更新规则。但是，对于包含多个训练样本的训练集，我们可以通过将坐标更新组合成向量形式来简化更新规则：\n​\t\t\t\t重复直到收敛 {}\n通过将坐标的更新组合成向量θ的更新，我们可以以更简洁的方式重新编写更新式（1.1）：这种形式的更新规则被称为批量梯度下降，因为它在每次迭代中考虑整个训练集。需要注意的是，对于线性回归的优化问题，由于成本函数是一个凸二次函数，不存在局部最小值，只有一个全局最小值。因此，梯度下降算法总是会收敛到全局最小值（前提是学习率不要设置得太大）。\n\n\n当然python的代码如下，你可以自己尝试\n\nClick to see more\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 定义二次函数\ndef quadratic_function(x, y):\n    return x**2 + y**2\n\n# 定义二次函数的偏导数\ndef gradient(x, y):\n    return np.array([2*x, 2*y])\n\n# 定义梯度下降函数\ndef gradient_descent(gradient, initial_point, learning_rate, num_iterations):\n    path = [initial_point]\n    point = initial_point\n\n    for _ in range(num_iterations):\n        grad = gradient(*point)\n        point = point - learning_rate * grad\n        path.append(point)\n\n    return np.array(path)\n\n# 定义绘制等高线图的函数\ndef plot_contour(func):\n    x = np.linspace(-5, 5, 100)\n    y = np.linspace(-5, 5, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = func(X, Y)\n\n    plt.figure(figsize=(8, 6))\n    plt.contour(X, Y, Z, levels=20)\n    plt.colorbar()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Contour Plot')\n    plt.grid(True)\n\n# 设置初始点、学习率和迭代次数\ninitial_point = np.array([3.0, 4.0])\nlearning_rate = 0.1\nnum_iterations = 20\n\n# 运行梯度下降算法\npath = gradient_descent(gradient, initial_point, learning_rate, num_iterations)\n\n# 绘制等高线图和路径\nplot_contour(quadratic_function)\nplt.plot(path[:, 0], path[:, 1], '-ro')\nplt.show()\n\n\n\n首先，图像展示了一个二维平面，其中轴和轴代表二次函数的输入变量的取值范围。二次函数由公式 给出，其中等高线表示函数值相等的点。等高线的形状呈现出圆形，因为函数是关于和的平方和的形式。\n其次，等高线的颜色表示函数值的大小。颜色条(colorbar)位于图像右侧，它显示了颜色与函数值的对应关系。颜色越深表示函数值越小，而颜色越浅表示函数值越大。在这个示例中，我们选择了20个等高线水平线，因此你可以看到等高线从内部圆开始，逐渐向外部圆扩展。\n接下来，红色的路径表示梯度下降算法的路径。我们使用初始点 (3.0, 4.0)作为起始点，并选择学习率为0.1，执行了20次迭代。梯度下降算法根据当前点的梯度信息来更新下一个点的位置，直到达到指定的迭代次数。红色路径显示了从初始点开始，沿着梯度下降方向逐步更新点的位置的过程。你可以看到路径开始在较陡峭的地方，然后逐渐向梯度变小的区域移动，最终趋近于函数的最小值(0, 0)。\n面积与房价我们继续研究梯度下降算法的不同变体！让我们先来看看批量梯度下降算法在拟合房屋价格预测模型时得到的结果。根据我们之前的数据集，通过运行批量梯度下降，我们得到了参数值和，它们可以用于构建线性模型来预测房屋价格，其中表示房屋的居住面积。如果我们绘制作为（面积）的函数，并结合训练数据，我们会得到上面的图表。这个模型给出了一个大致符合数据趋势的直线。\n\n\n当然，代码如下\n\nClick to see more\n\timport numpy as np\nimport matplotlib.pyplot as plt\n\n# 数据集\nareas = np.array([2104, 1600, 2400, 1416, 3000, 1985, 1534, 1427, 1380, 1494, 1940, 2000, 1890, 4478, 1268, 2300, 1760, 1450, 3100, 2250, 2132, 2596, 1850, 2680, 1956, 1604, 2020, 2730, 2008, 1537, 2500, 1560, 2120, 2200, 1638, 2540, 2200, 2070, 2005, 1900, 2380, 1320, 2190, 2340, 2678, 1966, 1570, 1852, 2454, 2205, 2450])\nprices = np.array([400, 330, 369, 232, 540, 320, 267, 199, 245, 347, 334, 383, 339, 699, 259, 410, 350, 315, 590, 410, 399, 459, 325, 480, 349, 285, 365, 525, 375, 295, 450, 280, 389, 425, 315, 475, 408, 382, 350, 380, 420, 230, 394, 416, 540, 385, 279, 360, 485, 405, 450])\n\n# 数据归一化（特征缩放）\nareas_normalized = (areas - np.mean(areas)) / np.std(areas)\nprices_normalized = (prices - np.mean(prices)) / np.std(prices)\n\n# 添加偏置项\nX = np.column_stack((np.ones(len(areas_normalized)), areas_normalized))\n\n# 初始化参数\ntheta = np.zeros(2)\nalpha = 0.01\niterations = 1500\n\n# 定义代价函数\ndef compute_cost(X, y, theta):\n    m = len(y)\n    h = np.dot(X, theta)\n    J = (1 / (2 * m)) * np.sum((h - y) ** 2)\n    return J\n\n# 批量梯度下降\ndef gradient_descent(X, y, theta, alpha, iterations):\n    m = len(y)\n    J_history = []\n    for _ in range(iterations):\n        h = np.dot(X, theta)\n        theta = theta - (alpha / m) * np.dot(X.T, h - y)\n        cost = compute_cost(X, y, theta)\n        J_history.append(cost)\n    return theta, J_history\n\n# 运行批量梯度下降算法\ntheta, J_history = gradient_descent(X, prices_normalized, theta, alpha, iterations)\n\n# 绘制拟合曲线\nx_values = np.linspace(-2, 2, 100)\ny_values = theta[0] + theta[1] * x_values\n\n\n# 绘制数据点和拟合曲线\nplt.scatter(areas_normalized, prices_normalized, label='Training Data')\nplt.plot(x_values, y_values, color='red', label='Linear Regression')\nplt.xlabel('Normalized Area')\nplt.ylabel('Normalized Price')\nplt.title('House Prices vs. Area (Linear Regression)')\nplt.legend()\nplt.show()\n\n\n\n\n继续考虑卧室数量接下来，如果我们考虑卧室数量作为输入特征之一，我们可以使用批量梯度下降来拟合模型并得到参数值，，。通过将卧室数量纳入考虑，我们可以构建一个更复杂的模型来预测房屋价格。\n\n\n代码如下\n\nClick to see more\n\timport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# 数据集\nareas = np.array([2104, 1600, 2400, 1416, 3000, 1985, 1534, 1427, 1380, 1494, 1940, 2000, 1890, 4478, 1268, 2300, 1760, 1450, 3100, 2250, 2132, 2596, 1850, 2680, 1956, 1604, 2020, 2730, 2008, 1537, 2500, 1560, 2120, 2200, 1638, 2540, 2200, 2070, 2005, 1900, 2380, 1320, 2190, 2340, 2678, 1966, 1570, 1852, 2454, 2205, 2450])\nbedrooms = np.array([3,3,3,2,4,4,3,3,3,3,4,3,3,5,3,4,3,3,4,4,4,3,4,4,3,3,4,4,3,3,4,4,4,3,3,4,3,4,3,3,4,2,4,3,4,4,3,3,4,4,4])\nprices = np.array([400, 330, 369, 232, 540, 320, 267, 199, 245, 347, 334, 383, 339, 699, 259, 410, 350, 315, 590, 410, 399, 459, 325, 480, 349, 285, 365, 525, 375, 295, 450, 280, 389, 425, 315, 475, 408, 382, 350, 380, 420, 230, 394, 416, 540, 385, 279, 360, 485, 405, 450])\n\n# 特征缩放（归一化）\nareas_normalized = (areas - np.mean(areas)) / np.std(areas)\nbedrooms_normalized = (bedrooms - np.mean(bedrooms)) / np.std(bedrooms)\nprices_normalized = (prices - np.mean(prices)) / np.std(prices)\n\n# 构建设计矩阵\nX = np.column_stack((np.ones(len(areas_normalized)), areas_normalized, bedrooms_normalized))\n\n# 初始化参数\ntheta = np.zeros(3)\nalpha = 0.01\niterations = 1500\n\n# 定义代价函数\ndef compute_cost(X, y, theta):\n    m = len(y)\n    h = np.dot(X, theta)\n    J = (1 / (2 * m)) * np.sum((h - y) ** 2)\n    return J\n\n# 批量梯度下降\ndef gradient_descent(X, y, theta, alpha, iterations):\n    m = len(y)\n    J_history = []\n    for _ in range(iterations):\n        h = np.dot(X, theta)\n        theta = theta - (alpha / m) * np.dot(X.T, h - y)\n        cost = compute_cost(X, y, theta)\n        J_history.append(cost)\n    return theta, J_history\n\n# 运行批量梯度下降算法\ntheta, J_history = gradient_descent(X, prices_normalized, theta, alpha, iterations)\n\n# 输出最终回归表达式\ntheta0 = theta[0]\ntheta1 = theta[1]\ntheta2 = theta[2]\nprint(f\"最终回归表达式: hθ(x) = {theta0:.2f} + {theta1:.2f}x1 + {theta2:.2f}x2\")\n\n# 绘制拟合曲面\nx1_values = np.linspace(-2, 2, 100)\nx2_values = np.linspace(-2, 2, 100)\nx1, x2 = np.meshgrid(x1_values, x2_values)\ny_values = theta0 + theta1 * x1 + theta2 * x2\n\n# 绘制数据点和拟合曲面\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(areas_normalized, bedrooms_normalized, prices_normalized, c='b', marker='o')\nax.plot_surface(x1, x2, y_values, color='r', alpha=0.5)\nax.set_xlabel('Normalized Area')\nax.set_ylabel('Normalized Bedrooms')\nax.set_zlabel('Normalized Price')\nax.set_title('House Prices vs. Area and Bedrooms (Linear Regression)')\nplt.show()\n\n\n\n\n你可以看到，通过添加更多的特征，我们可以得到更准确的房屋价格预测模型。这说明特征选择对于机器学习模型的性能至关重要。\n随机梯度下降除了批量梯度下降，还有一种非常有效的替代方法，它被称为随机梯度下降。这种方法的算法如下：\nLoop{    for  to ,{\n​\t\t\t\t\t\t​\t\t}\n}\n通过将坐标的更新组合成向量θ的更新，我们可以以更简洁的方式重新编写更新式:在这个算法中，我们遍历整个训练集，并且在每次遇到一个训练样本时，只使用该样本的误差梯度来更新参数。这个算法被称为随机梯度下降或增量梯度下降。与批量梯度下降不同的是，随机梯度下降可以立即开始取得进展，并且在每个样本上都继续进行更新。这使得随机梯度下降比批量梯度下降更快地将参数接近最小值。然而，需要注意的是，随机梯度下降可能无法收敛到真正的最小值，参数会在最小值周围波动，但是在实践中，这些接近最小值的参数值通常是对真实最小值的合理近似。因此，尤其是在训练集很大的情况下，随机梯度下降比批量梯度下降更受欢迎。\n通过这些不同的梯度下降算法，我们可以有效地拟合参数，从而得到准确的预测模型。在接下来的部分，我将继续更新我在机器学习部分自学的笔记，希望各位可以一起交流！\n","slug":"LMS","date":"2023-07-16T09:18:07.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"b6c077dbe8f34883367bdebb8559e973","title":"探索机器学习的魅力：从斯坦福大学CS229课程开始","content":"机器学习是当今科技领域的热门话题，而斯坦福大学的CS229课程则是深入学习机器学习的绝佳门户。这篇文章将带您踏上机器学习的征程，通过探索CS229课程中的第一个主题——监督学习，揭开这个令人着迷的领域的神秘面纱。无论您是初学者还是有一定经验的机器学习从业者，本文将带您以生动活泼且专业的方式了解监督学习的基本原理、应用领域以及最新的研究动态。\n（当然我也是小白，这只是我在学习过程当中的笔记 &gt;_&lt;）\n监督学习：预测房价的例子让我们从几个监督学习问题的例子入手。假设我们有一个数据集，其中包含了50栋房屋的居住面积和价格。我们可以将这些数据制成表格如下：\n\nClick to see more\n\n\n\n居住面积（feet^2）\n价格（1000$s）\n\n\n\n2104\n400\n\n\n1600\n330\n\n\n2400\n369\n\n\n1416\n232\n\n\n3000\n540\n\n\n1985\n320\n\n\n1534\n267\n\n\n1427\n199\n\n\n1380\n245\n\n\n1494\n347\n\n\n1940\n334\n\n\n2000\n383\n\n\n1890\n339\n\n\n4478\n699\n\n\n1268\n259\n\n\n2300\n410\n\n\n1760\n350\n\n\n1450\n315\n\n\n3100\n590\n\n\n2250\n410\n\n\n2132\n399\n\n\n2596\n459\n\n\n1850\n325\n\n\n2680\n480\n\n\n1956\n349\n\n\n1604\n285\n\n\n2020\n365\n\n\n2730\n525\n\n\n2008\n375\n\n\n1537\n295\n\n\n2500\n450\n\n\n1560\n280\n\n\n2120\n389\n\n\n2200\n425\n\n\n1638\n315\n\n\n2540\n475\n\n\n2200\n408\n\n\n2070\n382\n\n\n2005\n350\n\n\n1900\n380\n\n\n2380\n420\n\n\n1320\n230\n\n\n2190\n394\n\n\n2340\n416\n\n\n2678\n540\n\n\n1966\n385\n\n\n1570\n279\n\n\n1852\n360\n\n\n2454\n485\n\n\n2205\n405\n\n\n2450\n450\n\n\n\n\n我们可以用图表来展示这些数据：\n\n\n有了这样的数据，我们可以提出一个问题：基于波特兰其他房屋的居住面积，如何预测它们的价格呢？\n为了在接下来的讨论中使用统一的符号，我们将使用表示“输入”变量（在这个例子中是生活区域的大小），也称为输入特征；表示我们试图预测的“输出”或目标变量（价格）。对于一对，我们称其为训练示例（training example）。我们将学习的数据集，由n个训练示例的列表；构成，被称为训练集（training set）。请注意，上标“(i)”仅用于表示训练集的索引，与求幂无关。我们还将使用表示输入值的空间，表示输出值的空间。在这个例子中，。\n为了更正式地描述监督学习问题，我们的目标是给定一个训练集，学习一个函数：，使得成为y的“良好”预测器。从图表中可以看出，这个过程如下所示：\n\n\n当我们试图预测的目标变量是连续的，例如在房屋价格的例子中，我们将这种学习问题称为回归问题。回归问题的目标是建立一个模型，能够对连续的目标变量进行预测。通过分析各种特征和输入变量之间的关系，我们可以推断出目标变量的数值。在回归问题中，我们通常尝试拟合一条曲线或平面，以最好地表示数据的趋势和模式。\n相比之下，当目标变量只能取少量离散值时（比如，根据居住面积预测一个住宅是房子还是公寓），我们将这种学习问题称为分类问题。分类问题的目标是根据输入变量的特征将样本分配到预定义的离散类别中。在分类问题中，我们建立一个分类器，该分类器根据输入变量的特征预测目标变量的类别。为了将不同类别的样本区分开来，分类问题通常涉及使用统计技术或机器学习算法构建决策边界。\n线性回归让我们使我们的住房示例更加丰富，加入一个稍微复杂的数据集，我们还知道每栋房子的卧室数量。\n在这个数据集中，除了居住面积外，我们还有每个房子的卧室数量。我们可以将数据表示为以下表格：\n\nClick to see more\n\n\n\n居住面积（feet^2）\n卧室数量\n价格（1000$s）\n\n\n\n2104\n3\n400\n\n\n1600\n3\n330\n\n\n2400\n3\n369\n\n\n1416\n2\n232\n\n\n3000\n4\n540\n\n\n1985\n4\n320\n\n\n1534\n3\n267\n\n\n1427\n3\n199\n\n\n1380\n3\n245\n\n\n1494\n3\n347\n\n\n1940\n4\n334\n\n\n2000\n3\n383\n\n\n1890\n3\n339\n\n\n4478\n5\n699\n\n\n1268\n3\n259\n\n\n2300\n4\n410\n\n\n1760\n3\n350\n\n\n1450\n3\n315\n\n\n3100\n4\n590\n\n\n2250\n4\n410\n\n\n2132\n4\n399\n\n\n2596\n3\n459\n\n\n1850\n4\n325\n\n\n2680\n4\n480\n\n\n1956\n3\n349\n\n\n1604\n3\n285\n\n\n2020\n4\n365\n\n\n2730\n4\n525\n\n\n2008\n3\n375\n\n\n1537\n3\n295\n\n\n2500\n4\n450\n\n\n1560\n4\n280\n\n\n2120\n4\n389\n\n\n2200\n3\n425\n\n\n1638\n3\n315\n\n\n2540\n4\n475\n\n\n2200\n3\n408\n\n\n2070\n4\n382\n\n\n2005\n3\n350\n\n\n1900\n3\n380\n\n\n2380\n4\n420\n\n\n1320\n2\n230\n\n\n2190\n4\n394\n\n\n2340\n3\n416\n\n\n2678\n4\n540\n\n\n1966\n4\n385\n\n\n1570\n3\n279\n\n\n1852\n3\n360\n\n\n2454\n4\n485\n\n\n2205\n4\n405\n\n\n2450\n4\n450\n\n\n\n\n在这里，我们将表示为二维向量，包括居住面积和卧室数量。例如，表示训练集中第个房子的居住面积，表示卧室数量。（在设计学习问题时，我们可以自行选择包括哪些特征。如果我们收集住房数据，还可以考虑包括其他特征，如壁炉数量、卫生间数量等。稍后我们将详细讨论特征选择，但现在我们将特征视为给定的。）\n为了进行监督学习，我们需要决定如何在计算机中表示函数/假设。作为初始选择，我们假设我们决定将近似为的线性函数：在这个表达式中，是参数（也被称为权重），用于参数化从到的线性函数空间。当没有混淆的风险时，我们将在中省略θ的下标，并将其简化为。为了简化表示法，我们还引入一个约定，将设置为1（这是截距项），这样我们可以表示为：在上面的表达式中，我们将和都视为向量，其中是输入变量的数量（不包括）。现在，给定一个训练集，我们如何选择或学习参数？一个合理的方法似乎是使得尽可能接近，至少对于我们拥有的训练样本来说是如此。为了形式化这个问题，我们定义了一个函数，用于衡量每个值对应的与相应的之间的接近程度。我们定义了成本函数：让我们逐步解释成本函数的每个部分：\n\n 代表我们的模型对于输入样本  的预测值。\n 是对应于输入样本  的实际观测值。\n 表示我们的模型预测值与实际观测值之间的差异，即误差。\n 将误差进行平方，这是为了消除误差的正负号，并将大误差的影响放大，以更好地衡量它们对总体误差的贡献。\n 对所有训练样本的误差平方进行求和，得到总体误差的度量。\n 为了计算方便，我们对总体误差进行了归一化，除以2。这不会影响最终优化的结果，因为我们的目标是最小化成本函数，而不是具体的数值。\n\n因此，成本函数  衡量了模型预测值与实际观测值之间的差异的平方和的一半。我们的目标是通过调整参数  的值，使得成本函数的值最小化，从而使模型的预测尽可能接近实际观测值。\n如果听不懂，没关系，我们来举一个非常简单的例子来理解成本函数到底是什么玩意。当我们建立一个模型时，我们希望它能够根据输入数据预测出正确的结果。然而，在训练模型时，我们的预测可能与实际结果有一些差距。成本函数的作用就是衡量这些预测差距的大小。\n假设你是一名学生，每天都要骑自行车去上学。你决定根据骑行时间来预测你到达学校所需的时间。你记录了过去一周的骑行时间和实际到达学校的时间，如下所示：\n\n\n\n骑行时间（分钟）\n到达时间（分钟）\n\n\n\n20\n25\n\n\n25\n30\n\n\n30\n35\n\n\n35\n40\n\n\n40\n45\n\n\n现在，你想建立一个模型，根据骑行时间预测到达时间。你选择线性函数来表示模型，即假设到达时间与骑行时间之间存在一种线性关系。\n假设我们的模型为 ，其中  表示到达时间的预测值， 表示骑行时间。我们的目标是找到最佳的参数  和 ，使得模型的预测结果尽可能接近实际观测值。\n为了衡量模型的预测与实际观测值之间的差距，我们使用成本函数。在这个例子中，我们使用均方误差（Mean Squared Error，MSE）作为成本函数。MSE 的计算方式是将每个预测值与对应的实际观测值之间的差距平方，并取所有差距平方的平均值。\n现在，我们来计算一下成本函数的值。假设我们选择了一组参数  和 。我们可以将这些参数代入模型，并计算出每个骑行时间对应的预测值。\n\n\n\n骑行时间（分钟）\n到达时间（分钟）\n预测到达时间（分钟）\n差距（预测-实际）\n差距平方\n\n\n\n20\n25\n30\n-5\n25\n\n\n25\n30\n35\n-5\n25\n\n\n30\n35\n40\n-5\n25\n\n\n35\n40\n45\n-5\n25\n\n\n40\n45\n50\n-5\n25\n\n\n现在，我们将差距平方的平均值作为成本函数的值。在这种情况下，成本函数的计算如下：我们的目标是通过调整参数  和  的值，使得成本函数的值最小化。这意味着我们希望找到最佳的参数组合，使得模型的预测结果与实际观测值之间的差距最小化。\n希望这个例子能够更加清晰地解释成本函数的概念。成本函数用于衡量模型预测与实际观测之间的差距，并帮助我们找到最佳的参数组合。\n如果你之前接触过线性回归，你可能会注意到这个熟悉的最小二乘成本函数，它导致了普通最小二乘回归模型。无论你之前是否见过它，我们将继续讨论，并最终展示它是一个更广泛的算法家族中的特例。同时，这个成本函数的意义是衡量我们的预测与实际数据之间的差距。较小的成本值表示我们的预测较接近真实数据，而较大的成本值表示预测与真实数据之间的差距较大。\n如何有任何疑问或者建议欢迎在评论区评论&gt; &lt;!!!\n","slug":"初识机器学习","date":"2023-07-16T07:02:18.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"d99fcab655887d4444838841a030a1e7","title":"正项级数","content":"这是一个非常重要的课题，我们将开始学习关于正级数，请专心听讲。\n正项级数的定义正项级数是一种无穷级数，其项均为正实数。它的形式如下：其中为级数的第个项。\np级数p级数是指形如的级数，其中是一个正实数。该级数以分母的指数来命名。\np级数具有以下性质：\n\n如果，则p级数收敛。\n如果，则p级数发散。\n\n\n\n这个结果的证明基于积分测试。为了理解为什么这是正确的，我们可以考虑函数。这个函数在区间上连续、正值且递减，因此我们可以应用积分测试来得到级数的结果：对积分进行计算，我们得到：$$\\int_{1}^{\\infty} \\frac{1}{x^p}dx  \\left{\\right.$$因此，p级数收敛当且仅当，当时发散。\n对于，p级数的和可以用黎曼ζ函数（Riemann zeta function）来表示，它的定义为：黎曼ζ函数具有许多有趣的性质，并与数论和复分析等其他数学领域有着密切的联系。\n敛散性检验有几个用于确定正项级数收敛或发散的收敛性测试。以下是其中几个常用的测试：\n1. 比较判别法比较判别法是一种通过将待定级数与已知收敛性的另一个级数进行比较来确定级数的收敛性或发散性的方法。比较法的规则如下：\n假设  和  是具有正项的级数，并且对所有 ，满足 。\n\n如果  收敛，则  也收敛。\n如果  发散，则  也发散。\n\n换句话说，如果级数  的项始终小于或等于级数  的项，并且  收敛，那么  必定收敛。相反地，如果  发散，那么  也一定发散。\n比较测试经常用于将给定的级数与 p-级数进行比较，因为 p-级数的收敛性是众所周知的。具体而言，如果我们有一个形如  的级数，以及另一个级数 ，其中的项始终小于或等于 ，那么我们可以使用比较测试来确定  的收敛性。\n\n\n\n\n\n\n\n\n\n\n\n我们常用的一些参考无限级数有：\n\n几何级数\n调和级数\np-级数\n\n2. 极限比较判别法极限比较判别法是用于确定级数的收敛性或发散性的另一种方法。与比较法类似，它涉及将给定的级数与已知收敛性的另一个级数进行比较。然而，极限比较测试在选择要进行比较的级数方面更加灵活。\n假设  和  是具有正项的级数。令 ，其中  是一个有限的正数或者是 。\n\n如果 ，则  和  要么都收敛，要么都发散。\n如果  并且  收敛，则  也收敛。\n如果  并且  发散，则  也发散。\n\n极限比较测试在选择要进行比较的级数方面更加灵活，因为我们只需要项的比值收敛到一个有限的正数。这意味着我们通常可以找到一个更易处理的级数来进行比较，相较于比较测试的情况而言。\n3. 比值判别法比值判别法是一种用于判断级数的收敛性或发散性的测试方法。比值法规定如下：\n假设  是一个具有正项的级数，令 （这个极限可能存在也可能不存在）。\n\n如果 ，则  绝对收敛。\n如果  或 ，则  发散。\n如果  或极限不存在，则比值测试无法确定收敛性或发散性，我们需要使用其他测试方法。\n\n从直观上讲，比值测试将级数的项与具有公比  的几何级数的项进行比较。如果 ，那么级数的项的衰减速度比一个收敛的几何级数的项更快，因此该级数收敛。如果 ，那么级数的项的增长速度比一个发散的几何级数的项更快，因此该级数发散。如果 ，那么级数的项的衰减速度与一个收敛的几何级数的项相同，因此测试是不确定的。\n4. 根值判别法根值判别法是一种用于判断级数的收敛性或发散性的测试方法。根值判别法规定如下：\n假设  是一个具有正项的级数，令 （这个极限可能存在也可能不存在）。\n\n如果 ，则  绝对收敛。\n如果  或 ，则  发散。\n如果  或极限不存在，则根值测试无法确定收敛性或发散性，我们需要使用其他测试方法。\n\n从直观上讲，根值测试将级数的项与一个收敛的几何级数的项进行比较，该几何级数的公比为 。如果 ，那么级数的项的衰减速度比一个收敛的几何级数的项更快，因此该级数收敛。如果 ，那么级数的项的增长速度比一个发散的几何级数的项更快，因此该级数发散。如果 ，那么级数的项的衰减速度与一个收敛的几何级数的项相同，因此测试是不确定的。\n这章节的内容非常重要，因为我们随后提到的许多概念和扩展都是基于这一章的结论，所以一定要仔细阅读。\n","slug":"正项级数","date":"2023-07-15T09:07:26.000Z","categories_index":"","tags_index":"笔记,Math","author_index":"General_K1ng"},{"id":"8c5051666bb20a88e04fe467e55f2664","title":"无穷级数的性质","content":"欢迎来到无穷级数的奇妙世界！在这一章中，我们将探索无穷级数的性质，包括收敛、发散以及我们可以对级数进行的代数运算。理解这些性质对于数学和科学的许多领域都是至关重要的，它使我们能够做出准确的预测，解决重要的问题，并发展新的计算方法。所以，让我们一起深入探索无穷级数的惊人性质吧。\n几个重要的无穷级数除了探索无穷级数的性质，我们还将介绍和研究一些在数学中最重要的级数。其中包括几何级数，在微积分中有许多重要的应用，以及调和级数，它是一个经典的发散级数的例子。我们还将研究其他重要的级数，如交错级数和泰勒级数，在数学和科学的许多领域都有广泛的应用。所以，准备好一起探索无穷级数的性质，以及数学中一些最重要的级数吧！\n几何级数几何级数是一种特殊类型的无穷级数，其中每一项都是前一项的常数倍（说白了就是等比数列）。几何级数的一般形式为其中  是第一项， 是公比。几何级数在数学中很重要，在科学和工程中也有许多应用。\n推导通项公式为了推导出几何级数的求和公式，我们从考虑级数的部分和开始。设  为级数的前  项和，那么我们可以将方程的两边都乘以  得到将第二个方程从第一个方程中减去，我们得到这可以简化为如果 ，我们可以将两边都除以  得到这个公式给出了几何级数的前  项和。\n要求解无穷几何级数的和，我们取  趋向于无穷的极限：如果 ，那么当  趋向于无穷时， 趋向于零，所以极限简化为这就是当  时无穷几何级数的和的公式。\n几何级数的敛散性几何级数的收敛性取决于公比  的值。我们可以将几何级数分为三类：\n\n如果 ，那么级数绝对收敛。这意味着级数收敛且和是有限的。\n如果 ，那么级数可能收敛，也可能发散。在这种情况下，我们需要观察级数的项的行为来确定收敛性或发散性。\n如果 ，那么级数发散。这意味着级数的和是无穷的。\n\n例如，考虑级数\n这是一个公比为 ， 的几何级数。由于 ，该级数绝对收敛，和为\n\n\n\n\n\n\n\n\n\n这个图像生动地展示了为什么这个几何级数的和是2。当然，为了方便起见，我们让级数的第一项从  开始。每次将图像分成两半并取剩余部分的一半，即四分之一，依此类推，我们发现图像的总面积仍然是1，直到无穷项。\n几何级数的应用几何级数在数学、科学和工程中有许多应用。例如，它们可以用来模拟指数增长或衰减，如人口增长或放射性物质的衰变。它们还可以用于计算某些类型的积分和近似函数。\n结论几何级数是微积分中的一个基本概念，在各个领域中都有许多应用。理解几何级数的收敛和发散对于使用闭合解找到级数的和非常重要。\n调和级数调和级数是数学中一个众所周知的级数，它在许多不同的背景下自然出现，包括微积分、数论和物理学。特别地，它是以下形式的级数：\n调和级数之所以有趣，是因为它是一个发散的级数，也就是说它没有有限的和。这可以通过检查级数的部分和来看出，随着添加更多的项，部分和会无限增长。\n敛散性的证明为了证明调和级数的发散性，我们可以使用积分测试。积分测试表明，如果函数  对于所有  是正的、递减的和连续的，并且对于所有 ，有 ，那么级数  和不定积分  要么都收敛，要么都发散。\n对于调和级数，我们可以选择 ，它满足积分测试的条件。不定积分  可以计算为：由于该积分发散，调和级数也必定发散。\n\n\n可以看到，每个矩形的面积都是 ，所以前  个矩形的总面积是 。随着  的增加，矩形的面积逐渐接近 ，因此阶梯状的图形逐渐趋近于斜率为  的曲线。最终，当  趋向于无穷时，矩形的面积趋向于 ，阶梯状的图形趋向于曲线 。\n这种可视化可以帮助我们更好地理解级数的行为以及它与自然对数的关系。\n当然，如果你无法理解这个图像以及以上所说的证明过程，没关系，你只需要记住调和级数是发散的。\n调和级数的应用调和级数的发散性在数学和科学中有许多重要的应用，例如：\n\n调和级数在数论中被用来研究质数的分布。级数的发散意味着质数是无穷多的，这是数论中的一个基本结果。\n调和级数的发散性在物理学中也有重要的应用，特别是在电场的研究中。点电荷在电场中的电势能与点电荷与电场中所有其他电荷之间距离的倒数之和成正比。这个和等价于调和级数，它的发散性意味着点电荷的电势能是无限大的。\n\n结论总之，调和级数是数学和科学中重要而有趣的级数。它是一个发散的级数，也就是说它没有有限的和。我们可以使用积分测试来证明级数的发散性，而级数的发散性在数论和物理学中有重要的应用。\n无穷级数的一些性质\n如果给定的无穷级数  收敛，那么任何形式为  的级数，其中  是常数，也将收敛。这个性质可以通过序列极限的定义得出。\n\n\n\n\n\n\n\n\n\n\n\n我们可以看到  收敛，并且  也收敛，它们的收敛性与  相同。\n因此，这个定理可以用来证明  收敛，因为它是通过将  乘以常数 0.5 而得到的。\n\n为了证明这个性质，我们可以设  是级数  的部分和序列， 是级数  的部分和序列。那么我们有：和将第一个等式乘以 ，我们得到：现在，设  是序列  的极限，即 。由于级数  收敛，我们知道极限  是有限的。因此，根据极限的代数性质，我们有：类似地，序列  的极限为：因此，级数  也收敛，其和为 。\n综上所述，无穷级数的线性性质表明，如果给定的无穷级数收敛，那么通过将原级数的项乘以一个常数得到的任何级数也将收敛，其和将是该常数与原级数和的乘积。这个性质可以通过序列极限的定义和极限的代数性质得到。\n\n\n\n如果  和  收敛，那么  也收敛。\n\n\n\n\n\n\n\n\n\n\n\n\n\n我们可以看到  和  都收敛，而  也收敛，它们的收敛性与  和  相同。\n因此，这个定理可以用来证明  收敛，因为它是  和  的和。\n\n要证明这个结果，我们可以使用以下步骤：\n\n令 。那么  是由  和  对应的项相加得到的级数。\n\n由于  和  都收敛，它们的部分和序列  和  也收敛。即， 和 。\n\n我们想要证明  收敛。为了做到这一点，我们需要证明  的部分和序列  收敛。也就是说，我们需要证明  存在。\n\n我们可以将  表示为  和  的形式：\n\n根据极限的性质，我们有：\n\n因此，我们已经证明了  的部分和序列  收敛，因此级数  收敛。\n\n由于  被定义为 ，所以这个结果对于  和  两种情况都成立。也就是说，如果  和  收敛，那么  和  也收敛。\n\n\n\n\n\n对于仍然在任意添加括号后保持收敛的无穷级数\n\n​\t\t如果您有Python环境，我强烈建议您运行我提供的”Verifying_Property_3” Python文件，您可以看到在不同括号下的图像的差异。\n\n要了解这个结论为什么成立，考虑一个收敛于极限的无穷级数。也就是说，偏和数列，其中，当趋向于无穷时收敛于。\n现在，假设我们以任意方式给级数的项添加括号，也就是说，我们用括号将某些项分组，但不改变项的顺序。例如，我们可以写成：或者或者任何其他方式的分组。\n让我们用表示新的偏和数列，其中是前个分组的和。例如，在上述第一种分组中，我们有，，，依此类推。\n现在，考虑级数中的任意两个相邻的分组。我们将第一个分组中的项称为，将第二个分组中的项称为。那么这两个分组的和为：根据加法的结合律，我们可以重新排列这个和为：也就是说，我们可以以任何希望的方式将这些项分组，得到的和仍然相同。因此，新的分组的偏和数列与原始的偏和数列相同。换句话说，在收敛级数的项上添加括号不会改变级数的极限。\n因此，我们已经证明了对于收敛的无穷级数，在其项上添加任何括号后，级数仍然收敛，且极限不变。\n\n\n​\t\t需要注意的是，这个性质只适用于收敛的级数。对于发散的级数，重新排列项可能会导致不同的收敛性质。\n\n通过删除、添加或更改有限项，级数的收敛性不会改变，但和可能会改变。\n\n\n要正式证明这个性质，让我们考虑一个无穷级数，其中是级数的第个项。我们想要证明，如果我们通过添加、删除或更改任意有限数量的项来修改这个级数，级数的收敛性或发散性不会改变。\n首先，让我们考虑添加或删除有限数量的项的情况。设为原级数的原和，为修改后级数的新和。我们可以将表示为两个级数的和：第一个级数是原级数删除或添加有限数量的项后得到的，第二个级数是由被删除或添加的项组成的有限级数。形式上，我们可以写成：这里，是正整数，是非负整数，是修改后级数的第个项。\n由于原级数收敛于，我们有：现在，让我们考虑修改后的级数。修改后级数的第一部分收敛于与原级数相同的极限，因为它们之间只有有限数量的项不同。第二部分是一个有限级数，因此它收敛于一个有限和。因此，修改后的级数也收敛于和。\n因此，我们已经证明，如果我们通过添加或删除有限数量的项来修改一个无穷级数，级数的收敛性或发散性不会改变。\n接下来，让我们考虑更改有限数量的项的情况。假设我们通过将第个项更改为来修改级数，其中。设和分别为原和和修改后的和。那么，我们可以写成：由于和之间的差是一个有限数，级数的收敛性或发散性不会改变。然而，级数的实际和是不同的。\n总而言之，”通过删除、添加或更改有限项，级数的收敛性不会改变，但和可能会改变”是无穷级数理论中的一个基本结果。它告诉我们，我们可以通过添加、删除或更改有限数量的项来修改一个无穷级数，而不影响其收敛性或发散性。然而，级数的实际和可能会有所不同。\n\n\n\n级数收敛的必要条件是。换句话说，如果是一个收敛的级数，那么。\n\n\n要证明这个结果，假设是一个收敛的级数。根据定义，这意味着偏和序列收敛到某个有限极限。\n我们可以将级数的第个项表示为两个相邻偏和的差：将两边取极限，当时，我们得到：这里我们利用了序列收敛于的事实，因此和。\n因此，我们已经证明了如果是一个收敛的级数，那么。\n值得注意的是，这个结果的逆命题不一定成立。也就是说，仅仅因为，并不意味着是收敛的。例如，调和级数是发散的，尽管。因此，条件只是收敛的必要条件，而不是充分条件。\n\n\n当然，我们要求你不必记住或完全理解这些性质的推导过程，这些只是为了帮助你理解。在考试中，你只需要记住加粗的文字，这应该不难，对吗？\n","slug":"无穷级数的性质","date":"2023-07-15T07:46:56.000Z","categories_index":"","tags_index":"笔记,Math","author_index":"General_K1ng"},{"id":"fca5d0f1b5b938fde80803f8dc301aa3","title":"我的博客之旅：学习、分享与成长","content":"\n\n\n\n\n\n\n\n\n欢迎来到我的博客！我是金洪来，目前就读于西交利物浦大学，专业是信息与计算科学（ICS）。在我充满好奇心和激情的探索中，我决定搭建这个博客，与大家分享我的学习经历、见解和成果。对我来说，这是一次意义非凡的旅程，我希望能够与你们一起成长、互相学习和建立联系。\n自我介绍我是一个对新兴技术充满热情的学生，同时也热爱广泛阅读。在我信息与计算科学的学习中，我不仅学习了编程语言和算法，还探索了许多有趣的领域，如人工智能、数据科学和网络安全。我迷恋于技术的力量和它所带来的无限可能。通过博客，我希望能够与大家分享我的学习经验、项目经历以及对技术和生活的思考。\n搭建博客的动机我为什么突然决定搭建博客呢？首先，学习新技术时，我发现将所学知识记录下来是一种非常有效的方法，可以帮助我更好地理解和巩固所学内容。通过博客，我可以分享这些知识，并与志同道合的人们交流和互动，从中收获更多的见解和观点。其次，博客可以成为一个平台，让我与专业人士和技术大佬们建立联系。我希望能够从他们的经验中获得启发和指导，进一步提升自己的技术能力和职业发展。最重要的是，我相信博客是一个分享和学习的场所，我希望能够通过博客与读者们一起成长，共同进步。\n目标受众我的博客欢迎所有学生、志同道合的朋友、技术大佬和技术小白们。无论你是正在学习编程的初学者，还是想深入了解某个领域的专业人士，我都希望我的博客能够为你提供有价值的信息和灵感。在我的博客中，你将找到关于技术教程、项目经验、学习心得，以及我对生活和工作的见解和体验分享。我希望能够为你提供有趣且实用的内容，无论你身在何处、经历如何，我都希望我的博客能够成为你学习和成长的伙伴。\n博客内容计划在我的博客中，你将看到各种内容，涵盖了技术教程、项目经验、学习笔记以及我对生活和工作的见解和体验。我计划分享一些简单易懂的技术教程，帮助初学者快速入门。通过清晰的代码示例和详细的解释，我希望能够让你轻松理解复杂的概念和技术。同时，我也会分享一些我在项目中遇到的挑战和解决方法，以及学习过程中的心得体会。我相信通过分享这些经验，我们可以相互帮助和共同成长。此外，我会不定期地分享一些生活感悟和工作经验，希望能够给你带来一些启发和思考。\n学习和成长通过搭建博客，我期望自己能够不断学习和成长。通过与读者的互动和交流，我相信我可以提升自己的学习能力和沟通技巧。我希望能够给你提供有价值的内容和支持，帮助你解决问题、开拓思路。同时，我也希望能够通过博客启发更多的人，鼓励他们追求自己的梦想，勇敢地探索未知的领域。在这个博客的旅程中，我将不断挑战自我，拓宽自己的知识边界，并将这些经验和成长与大家分享。\n结语感谢你花时间阅读我的博客介绍。我希望你能够加入我的博客之旅，与我一起探索技术的奇妙世界，共同成长和学习。如果你对我的博客感兴趣或有任何问题、建议或想法，请随时与我联系。我期待与你在博客中互动，并希望我的博客能够对你有所帮助。谢谢！\n","slug":"我的第一篇博客","date":"2023-07-14T09:16:53.000Z","categories_index":"","tags_index":"随笔","author_index":"General_K1ng"}]