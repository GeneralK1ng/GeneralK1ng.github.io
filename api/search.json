[{"id":"68643f4a3340f18395afd7c596807d57","title":"高斯判别分析","content":"高斯判别分析（GDA）让我们来讨论生成学习算法中的一个重要模型——高斯判别分析（Gaussian Discriminant Analysis，简称 GDA）。在这个模型中，我们假设数据的条件概率  符合多元正态分布。在深入探讨 GDA 模型之前，我们先简要介绍一下多元正态分布的性质。\n多元正态分布在  维空间中的多元正态分布，也被称为多元高斯分布，由均值向量  和协方差矩阵  参数化，其中  是对称且半正定的。我们用  来表示这个分布，其概率密度函数如下所示：在上面的方程中，“”表示矩阵的行列式。\n对于服从  分布的随机变量 ，其均值是 ，这并不令人意外：对于一个向量值随机变量 ，协方差定义为 ，这推广了实值随机变量的方差概念。协方差也可以定义为 。（你应该能够自行证明这两个定义是等价的。）如果 ，那么：以下是一些高斯分布密度函数的示例：\n\n\n最左边的图显示的是均值为零（即2x1的零向量）且协方差矩阵（2x2的单位矩阵）的高斯分布。这个分布也被称为标准正态分布。\n中间的图显示的是均值为零且的高斯分布。\n最右边的图显示的是均值为零且的高斯分布。我们可以观察到，随着变得更大，高斯分布变得更加“扩展”，而随着变得更小，分布则变得更“压缩”。\n\n让我们看更多的例子：\n\n上面的图显示了均值为0的高斯分布，并分别显示了它们的协方差矩阵：左侧的图显示了熟悉的标准正态分布，我们可以看到随着协方差矩阵中的非对角元素增加，密度趋向于线（由给出）。当我们查看同样三个密度的等高线图时，这一点更加清晰：\n\n以下是通过变化生成的另一组示例：\n\n上面使用的图分别是:从左边和中间的图中，我们可以看到通过减小协方差矩阵的非对角元素，密度再次变得“压缩”，但是方向相反。最后，当我们更一般地改变参数时，等高线将形成椭圆（右边的图显示了一个例子）。\n作为我们最后的一组例子，我们固定，通过改变，我们可以将密度的均值移动到不同的位置。\n\n上面的数字是使用生成的，分别是\n高斯判别分析模型当我们面对一个分类问题，其中输入特征是连续值随机变量时，我们可以使用高斯判别分析（Gaussian Discriminant Analysis，GDA）模型，它使用多元正态分布来建模。该模型为：写出分布，这是：在这里，我们的模型参数是、、和。（请注意，尽管有两个不同的均值向量和，但通常使用一个协方差矩阵来应用该模型。）数据的对数似然是由以下公式给出：通过对参数最大化，我们可以找到参数的最大似然估计：从图示上看，该算法的过程如下所示：\n\n在图中显示了训练集，以及对每个类别中的数据进行拟合的两个高斯分布的等高线。注意，这两个高斯分布的等高线具有相同的形状和方向，因为它们共享协方差矩阵，但它们具有不同的均值和。图中还显示了给出决策边界的直线，该边界使得。在边界的一侧，我们将预测是最可能的结果，而在另一侧，我们将预测。\n讨论：GDA和逻辑回归GDA模型与逻辑回归之间有着有趣的关系。事实上，如果我们将  看作是  的函数，我们会发现它可以写成以下形式：\n在这里， 是 、、 和  的一些合适函数。这恰好是逻辑回归——一个判别算法——用来建模  的形式。\n\n\n\n\n\n\n\n\n\n这里使用了重新定义右侧的  的约定，通过添加额外的坐标 ，将它们变成  维向量\n那么我们什么时候更喜欢一种模型而不是另一种模型呢？一般来说，当在同一数据集上训练时，GDA和逻辑回归会给出不同的决策边界。那么哪个模型更好呢？\n我们刚刚证明了如果  是多元高斯分布（具有共享的 ），那么  必定遵循一个逻辑函数。然而，反过来并不成立；即  是一个逻辑函数并不意味着  是多元高斯分布。这表明GDA对数据做出了比逻辑回归更强的建模假设。事实上，当这些建模假设正确时，GDA将对数据拟合得更好，是一个更优的模型。特别地，当  确实是高斯分布（具有共享的 ）时，GDA是渐近有效的。简单来说，这意味着在非常大的训练集（大 n）的极限情况下，没有比GDA更好的算法（例如在准确估计  方面）。特别地，可以证明在这种情况下，GDA将优于逻辑回归；而更一般地说，即使对于较小的训练集大小，我们通常也会预期GDA更优。\n相比之下，逻辑回归通过做出明显较弱的假设，更加健壮且对错误的建模假设不太敏感。有许多不同的假设集会导致  采取逻辑函数的形式。例如，如果 ，并且 ，那么  将是逻辑函数。在这样的泊松分布数据上，逻辑回归也会表现良好。但是，如果我们在这样的数据上使用GDA，并将高斯分布拟合到这样的非高斯分布数据上，结果将变得不那么可预测，GDA可能（或者可能不会）表现良好。\n总结起来：GDA做出了更强的建模假设，在建模假设正确或近似正确的情况下更具数据效率（即需要较少的训练数据来学习得更好）。逻辑回归做出了较弱的假设，对于与建模假设的偏差更具有鲁棒性。具体而言，当数据确实是非高斯分布时，在大规模数据集的极限情况下，逻辑回归几乎总是比GDA表现更好。因此，在实践中，逻辑回归比GDA更常用。（我们接下来讨论的朴素贝叶斯算法也涉及有关判别模型与生成模型的类似考虑，但朴素贝叶斯算法仍被认为是一种非常优秀且广受欢迎的分类算法。）\n","slug":"Gaussian-discriminant-analysis","date":"2023-07-22T06:04:23.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"6e9ecff97c79fc9b7ee5419e3db98187","title":"生成式学习算法","content":"生成式学习算法：从动物外貌特征到分类预测到目前为止，我们已经讨论了一些学习算法，这些算法主要关注于建模条件分布 ，也就是在给定输入  的情况下输出  的概率分布。例如，逻辑回归使用  来建模 ，其中  是 sigmoid 函数。在这次笔记中，我们将探讨一种不同类型的学习算法。\n判别式学习算法在考虑一个分类问题时，我们希望根据动物的一些特征来学习如何区分大象 (y = 1) 和狗 (y = 0)。判别式学习算法，例如逻辑回归或感知机算法，试图找到一条直线，即决策边界，来将大象和狗分开。然后，为了对新的动物进行分类，算法会检查它在决策边界的哪一侧，并根据结果进行预测。\n生成式学习算法：全新的方法现在，我们来探讨一种全新的学习方法。首先，我们可以构建一个描述大象外貌特征的模型。然后，我们可以构建一个独立的描述狗外貌特征的模型。最后，当我们需要对一个新的动物进行分类时，我们可以将其与大象模型进行匹配，并将其与狗模型进行匹配，从而确定新动物更像我们在训练集中见过的大象还是更像狗。\n生成式学习算法 vs. 判别式学习算法刚才我们提到的直接学习  的算法（例如逻辑回归）或者直接学习从输入空间  到标签 {0, 1} 的映射（例如感知机算法），都属于判别式学习算法。而在这里，我们将讨论一类试图对条件概率 （以及类先验 ）进行建模的算法，这些算法被称为生成式学习算法。例如，对于一个表示示例是狗（0）还是大象（1）的问题， 建模了狗的特征分布，而  建模了大象的特征分布。\n从条件概率到后验概率在建模了  （称为类先验）和条件概率  之后，我们的算法可以使用贝叶斯规则来推导给定  的后验概率 ：这里，分母  由  给出（您应该能够从概率的标准性质验证这一点），因此也可以用我们已经学到的  和  的数量来表示。实际上，如果我们正在计算  以进行预测，那么我们实际上不需要计算分母，因为分母  只是用于归一化 ，以确保其概率总和为1。但由于我们只关心预测的类别，而不是具体的概率值，我们可以忽略归一化项，直接计算分子部分来得到预测的类别。\n","slug":"Generative-learning-algorithms","date":"2023-07-22T05:45:25.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"944f20daa7bfde2d1440fd8c10bf6bf0","title":"（实战）鸢尾花数据集的三分类","content":"上一章的难度相比各位估计应该也是云里雾里吧，那么这一章就来实战一下如何运用softmax回归来进行多分类吧！刚好顺便来弥补一下之前我们在对鸢尾花进行分类的时候只运用了二分类的遗憾，今天，我们就直接开始，三分类！\nSoftmax回归概念回顾首先我们先回顾一下什么是Softmax回归，它是一种用于多分类问题的分类算法，也称为多类别逻辑回归或多项逻辑回归。Softmax回归广泛用于机器学习和深度学习中，尤其在图像识别、自然语言处理和语音识别等任务上。\n简单来说，softmax回归将输入的样本通过线性变换和一个softmax函数映射为类别的概率分布。它主要包含两个步骤：\n\n线性变换：对于给定的输入样本，首先通过一个线性变换计算每个类别的得分。这个得分可以看作是输入样本属于每个类别的权重。\nSoftmax函数：然后，将得分通过softmax函数转换为概率分布。Softmax函数可以将原始得分转换为非负且和为1的概率值，这样可以表示每个类别的概率。\n\n假设我们有n个类别，对于第i个类别，它的得分为。那么Softmax函数的计算如下：其中，是指数函数，是求和函数。\n最终，输入样本属于第i个类别的概率为。可以选择概率最高的类别作为预测结果。\n在训练过程中，通常使用交叉熵损失函数来衡量预测结果和真实标签之间的差异，并通过梯度下降等优化算法来更新模型的参数，使得预测结果更接近真实情况。\n好像还是有些难懂，那再用人话来说一遍吧。\n人话版本1. 什么是分类问题？\n首先，什么是分类问题。在机器学习中，分类问题是指将事物分为不同的类别。比如，我们可以将动物分为猫、狗和鸟三个类别，或者将邮件分为垃圾邮件和非垃圾邮件两个类别。\n2. 什么是Softmax回归？\n它是一种机器学习算法，用于解决分类问题。假设我们有很多特征，比如动物的体重、体长和年龄，我们想要根据这些特征把动物分成不同的类别，比如猫、狗和鸟。Softmax回归可以帮助我们做这件事。\n3. 如何工作？\nSoftmax回归的工作方式有两个关键步骤：\n步骤一：计算得分\n对于给定的一个动物，我们会用一些数学计算得到它属于每个类别的得分。这个得分可以理解为，每个类别有多大的可能性与这个动物匹配。\n步骤二：转换为概率\n有了得分后，我们需要将它们转换成概率。概率是一个介于0到1之间的数值，表示某个动物属于某个类别的可能性有多大。Softmax回归使用一个特殊的函数，称为softmax函数，来做这个转换。它会把得分转换成概率，确保所有类别的概率加起来等于1。\n4. 如何做预测？\n在训练阶段，我们会用一堆已知类别的动物数据来让机器学习算法学习。它会通过调整一些参数来找到最佳的方式来预测动物的类别。\n然后，在预测阶段，当我们有一个新的动物数据时，我们会用训练好的模型，通过同样的计算和转换过程，得到动物属于每个类别的概率。最后，我们会选择概率最高的类别作为预测结果，就像猫、狗和鸟中选择概率最高的那个类别。\n怎么计算得分假设我们有一个分类问题，有个类别需要进行分类。对于给定的输入样本，我们会为每个类别分配一个得分（也称为logit），表示输入样本属于该类别的可能性大小。\n对于第个类别，得分的计算是通过将输入样本的特征与相应的权重进行线性组合得到的。假设输入样本有个特征（即特征向量的长度为），则该类别的得分可以表示为：其中， 是与每个特征相关联的权重， 是输入样本的特征值， 是偏置项（也称为截距项）。\n对于输入样本，我们需要为每个类别计算一个得分。\n怎么转换成概率得到每个类别的得分后，我们需要将它们转换为概率，以便进行分类。\n我们使用softmax函数来执行这个转换。对于第个类别的得分，它在Softmax函数中的概率表示为：其中，是指数函数，它会将得分转换为非负数。是求和函数，对所有类别的得分进行求和。\n通过这个计算，我们可以获得每个类别的概率。这些概率的和总是等于1，因为softmax函数的特性保证了这一点。\n最后，在预测阶段，我们选择具有最高概率的类别作为预测结果。例如，如果softmax函数给出了猫的概率为0.8，狗的概率为0.15，鸟的概率为0.05，那么我们将预测这个输入样本属于”猫”类别。\n实战开始选取模块与库那么还是老样子，选择我们应该用那些库，其实跟上次的都差不多。\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n首先，我们导入了所需的库，包括 pandas 用于数据处理，numpy 用于数值计算，matplotlib.pyplot 用于绘图，以及一些用于机器学习的模块如 train_test_split 用于数据集划分，StandardScaler 用于特征缩放，accuracy_score、confusion_matrix 和 classification_report 用于评估分类模型的性能。\n读取文件# 读取数据\ndata = pd.read_csv('iris.csv') # 你的文件路径！\n\n然后，我们使用 pandas 读取了一个名为 “iris.csv” 的数据集，该数据集包含了鸢尾花（Iris）的一些测量数据以及其所属的物种标签。\n跟上次一样的数据预处理过程，不过这一次我们还需要干一个事情就是将Species列转换成数值，使用One-hot编码。那么什么是One-hot编码呢？\nOne-hot编码在softmax回归中，我们需要将类别标签转换成一种称为One-hot编码的形式。One-hot编码是一种向量表示方法，将一个类别表示为一个向量，其中只有一个元素是1，其他元素都是0。例如，山鸢尾类别可以表示为[1, 0, 0]，变色鸢尾类别可以表示为[0, 1, 0]，维吉尼亚鸢尾类别可以表示为[0, 0, 1]。\n# 将Species列转换成数值，使用One-hot编码\ndata = pd.get_dummies(data, columns=['Species'])\n\n我们将数据集中的 “Species” 列进行了转换，并使用 One-hot 编码将其转换成了数值形式，这是因为机器学习算法通常只接受数值输入。\n数据预处理其实这一步真的跟之前二分类那一次都差不多了，无非是分离标签，分离特征，划分训练集和预测集，最后再把特征缩放一下。\n# 将特征和标签进行分离\nX = data.drop(['Species_setosa', 'Species_versicolor', 'Species_virginica'], axis=1).values\ny = data[['Species_setosa', 'Species_versicolor', 'Species_virginica']].values\n\n我们将数据集中的特征和标签进行了分离。在这个数据集中，特征是花的测量数据，而标签是表示鸢尾花所属物种的 One-hot 编码形式。\n# 将数据集划分为训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n我们使用 train_test_split 函数将数据集划分为训练集和测试集。训练集用于模型的训练，测试集用于评估模型的性能。这里将数据集划分成 80% 的训练集和 20% 的测试集，并且使用 random_state 参数设置了随机种子，以确保每次划分的结果都是相同的，便于我们复现结果。\n# 特征缩放\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n接着，我们对特征进行了缩放处理，这是为了确保不同特征的数值范围一致，从而避免某些特征对模型训练的影响过大。这里使用了 StandardScaler 类进行标准化处理，将特征缩放到均值为 0、方差为 1 的标准正态分布。\nSoftmax函数就是我们上面所说的那个，我在这里再放一下：我们翻译成代码就是\n# 定义Softmax函数\ndef softmax(scores):\n    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n定义 Softmax 函数，这是一个常用的激活函数，用于将模型输出转换为类别概率。它将原始的线性输出（也称为 “scores“）转换为一个概率分布，确保所有概率值在 0 到 1 之间，并且它们的和等于 1。这里的实现避免了数值不稳定性问题，通过减去每个样本的最大分数，可以使指数运算更稳定。\n损失函数：交叉熵这里我们需要再定义一个函数，我们称之为交叉熵函数，为了训练模型，我们需要定义一个损失函数来衡量预测结果和真实标签之间的差异。在softmax回归中，我们使用交叉熵损失函数来衡量预测概率和真实标签之间的距离。我们的目标是最小化交叉熵损失函数，使得模型的预测尽可能接近真实情况。\n# 定义交叉熵损失函数\ndef cross_entropy_loss(y_true, y_pred):\n    num_samples = y_true.shape[0]\n    log_likelihood = -np.log(y_pred[range(num_samples), np.argmax(y_true, axis=1)])\n    loss = np.sum(log_likelihood) / num_samples\n    return loss\n\n交叉熵是常用的分类问题损失函数，它通过计算模型输出概率与真实标签之间的差异来评估模型性能。这里的实现使用了 NumPy 数组运算，避免了显式的循环操作，提高了计算效率。\n训练函数这一步，我们需要使用我们的老朋友梯度下降算法来优化模型参数（权重和偏置）。梯度下降算法会根据损失函数的梯度方向，逐步更新模型的参数，以使损失函数逐渐减小。\n通过迭代训练过程，我们的模型将学习如何将输入样本映射到合适的类别，并且我们可以使用训练好的模型进行预测。\n# 定义训练函数\ndef train_softmax_regression(X, y, learning_rate, epochs):\n    num_samples, num_features = X.shape\n    num_classes = y.shape[1]\n\n    # 初始化权重和偏置\n    W = np.random.randn(num_features, num_classes)\n    b = np.zeros((1, num_classes))\n\n    # 记录每个epoch的损失\n    losses = []\n\n    for epoch in range(epochs):\n        # 前向传播\n        scores = np.dot(X, W) + b\n        probabilities = softmax(scores)\n\n        # 计算损失\n        loss = cross_entropy_loss(y, probabilities)\n        losses.append(loss)\n\n        # 反向传播\n        error = probabilities - y\n        dW = np.dot(X.T, error) / num_samples\n        db = np.sum(error, axis=0, keepdims=True) / num_samples\n\n        # 参数更新\n        W -= learning_rate * dW\n        b -= learning_rate * db\n\n    return W, b, losses\n\n我们定义了训练函数 train_softmax_regression。该函数使用批量梯度下降法来训练 Softmax 回归模型。在训练过程中，我们根据损失函数的梯度更新模型的权重和偏置，不断优化模型以使其更好地拟合训练数据。同时，我们还记录了每个 epoch 的损失值，以便后续的可视化和分析。\n开始调用设置学习率和迭代次数这一步跟之前也非常像，无非是梯度下降嘛\n# 设置学习率和迭代次数\nlearning_rate = 0.01\nepochs = 1000\n\n# 调用训练函数，训练Softmax回归模型\nW, b, losses = train_softmax_regression(X_train_scaled, y_train, learning_rate, epochs)\n\n在这里，我们设置了学习率和迭代次数，并调用训练函数 train_softmax_regression 对模型进行训练。通过不断地迭代优化权重和偏置，模型将逐渐适应训练数据。\n模型预测# 使用训练好的模型进行预测\nscores_test = np.dot(X_test_scaled, W) + b\nprobabilities_test = softmax(scores_test)\ny_pred = np.argmax(probabilities_test, axis=1)\n\n现在，我们使用训练好的模型对测试集进行预测。首先，我们计算测试集的模型输出分数（即未经过 Softmax 函数处理的值），然后将其转换为概率分布，最后根据概率值选择最可能的类别作为预测结果。\n模型评估设置这里我们需要写一些代码，方便我们后期对模型进行评估\n准确率# 计算准确率\naccuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\nprint(\"准确率：\", accuracy)\n\n接下来，我们计算模型的准确率，即模型在测试集上的分类正确率。这里使用了 accuracy_score 函数，它将预测结果与真实标签进行比较，并输出分类的准确率。\n分类报告与混淆矩阵# 输出分类报告和混淆矩阵\nprint(\"\\n分类报告：\")\nprint(classification_report(np.argmax(y_test, axis=1), y_pred))\n\nprint(\"\\n混淆矩阵：\")\nprint(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n\n最后，我们输出分类报告和混淆矩阵，用于详细评估模型在各个类别上的分类性能。分类报告提供了精确率、召回率和 F1 分数等指标，而混淆矩阵展示了模型预测结果与真实标签之间的对应关系。\n\n\n\n\n\n\n\n\n\n混淆矩阵是分类器在测试集上的分类结果的矩阵表示。矩阵的行表示真实类别，列表示预测类别。对角线上的元素表示正确分类的样本数，非对角线上的元素表示错误分类的样本数。\n可视化处理这一步我们就需要进行可视化处理，因为程序写一大堆，不如图像来的直观，便捷。\n损失函数# 可视化损失值随着迭代次数的变化\nplt.plot(range(epochs), losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n\n我们使用 matplotlib 库绘制了损失值随着迭代次数的变化曲线图，用于可视化模型训练的过程和损失值的收敛情况。这个图可以帮助我们判断模型是否在训练中得到了有效的优化。\n结果散点图我们需要绘制分类结果的散点图和预测结果的散点图。散点图将数据集中的样本点可视化为不同颜色和符号的点，以显示不同类别的分布和模型的分类结果。\n# 准备绘制子图的布局\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\naxes = axes.ravel()\n\n首先，我们创建了一个 2x2 的子图布局，即总共有 4 个子图。plt.subplots(2, 2) 返回一个包含 2 行 2 列的 Figure 对象和一个 Axes 对象数组。然后，通过 axes.ravel() 将该数组转换为一维数组，以便更方便地对每个子图进行操作。\n# 绘制每个子图\nfor i in range(3):\n    # 绘制分类结果的散点图\n    axes[i].scatter(X_train[y_train[:, 0] == 1, i], X_train[y_train[:, 0] == 1, i + 1], label='Class 1 (Train)', marker='o', c='blue')\n    axes[i].scatter(X_train[y_train[:, 1] == 1, i], X_train[y_train[:, 1] == 1, i + 1], label='Class 2 (Train)', marker='s', c='green')\n    axes[i].scatter(X_train[y_train[:, 2] == 1, i], X_train[y_train[:, 2] == 1, i + 1], label='Class 3 (Train)', marker='^', c='red')\n\n    # 绘制预测结果的散点图，使用不同的颜色和符号\n    axes[i].scatter(X_test[y_pred == 0, i], X_test[y_pred == 0, i + 1], label='Class 1 (Test)', marker='x', c='blue', alpha=0.5)\n    axes[i].scatter(X_test[y_pred == 1, i], X_test[y_pred == 1, i + 1], label='Class 2 (Test)', marker='x', c='green', alpha=0.5)\n    axes[i].scatter(X_test[y_pred == 2, i], X_test[y_pred == 2, i + 1], label='Class 3 (Test)', marker='x', c='red', alpha=0.5)\n\n    axes[i].set_xlabel(data.columns[i])\n    axes[i].set_ylabel(data.columns[i + 1])\n    axes[i].legend()\n\n\n接着，我们使用循环遍历每个子图，并在每个子图中绘制两个类别的散点图：一个是分类结果的散点图，另一个是预测结果的散点图。我们使用 scatter 函数绘制散点图，其中包含了训练集和测试集中不同类别的样本点。\n对于分类结果的散点图，我们分别使用不同的颜色和符号来表示每个类别。蓝色圆点表示 “Class 1”，绿色正方形表示 “Class 2”，红色三角形表示 “Class 3”。我们从训练集 X_train 和标签 y_train 中选择相应的样本点，根据标签的 One-hot 编码来确定样本的类别。\n对于预测结果的散点图，我们使用虚线的 X 符号来表示。同样，我们根据预测结果 y_pred 来选择测试集 X_test 中的样本点，并根据预测的类别来确定样本所属的类别。\n在每个子图中，我们设置了 x 轴和 y 轴的标签，分别为数据集的不同特征列，用以标识每个散点图中数据点的位置。同时，我们添加了图例来说明不同类别的符号和颜色含义。\n\n# 设置整体图的标题和坐标轴\nplt.suptitle('Classification and Prediction Results')\nplt.tight_layout()\nplt.show()\n\n最后，我们为整体图设置了标题 “Classification and Prediction Results”，使用 suptitle 函数来实现。同时，我们通过 tight_layout() 函数调整子图之间的布局，使得图像更美观。最后，使用 plt.show() 来显示绘制的图像。\n完整代码如下\n\nClick to see more\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n\n# 读取数据\ndata = pd.read_csv('iris.csv') # 记得改自己文件的路径\n\n# 将Species列转换成数值，使用One-hot编码\ndata = pd.get_dummies(data, columns=['Species'])\n\n# 将特征和标签进行分离\nX = data.drop(['Species_setosa', 'Species_versicolor', 'Species_virginica'], axis=1).values\ny = data[['Species_setosa', 'Species_versicolor', 'Species_virginica']].values\n\n# 将数据集划分为训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 特征缩放\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ndef softmax(scores):\n    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\ndef cross_entropy_loss(y_true, y_pred):\n    num_samples = y_true.shape[0]\n    log_likelihood = -np.log(y_pred[range(num_samples), np.argmax(y_true, axis=1)])\n    loss = np.sum(log_likelihood) / num_samples\n    return loss\n\ndef train_softmax_regression(X, y, learning_rate, epochs):\n    num_samples, num_features = X.shape\n    num_classes = y.shape[1]\n\n    # 初始化权重和偏置\n    W = np.random.randn(num_features, num_classes)\n    b = np.zeros((1, num_classes))\n\n    # 记录每个epoch的损失\n    losses = []\n\n    for epoch in range(epochs):\n        # 前向传播\n        scores = np.dot(X, W) + b\n        probabilities = softmax(scores)\n\n        # 计算损失\n        loss = cross_entropy_loss(y, probabilities)\n        losses.append(loss)\n\n        # 反向传播\n        error = probabilities - y\n        dW = np.dot(X.T, error) / num_samples\n        db = np.sum(error, axis=0, keepdims=True) / num_samples\n\n        # 参数更新\n        W -= learning_rate * dW\n        b -= learning_rate * db\n\n    return W, b, losses\n\nlearning_rate = 0.01\nepochs = 1000\n\nW, b, losses = train_softmax_regression(X_train_scaled, y_train, learning_rate, epochs)\n\n# 预测\nscores_test = np.dot(X_test_scaled, W) + b\nprobabilities_test = softmax(scores_test)\ny_pred = np.argmax(probabilities_test, axis=1)\n\n# 准确率\naccuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\nprint(\"准确率：\", accuracy)\n\n# 分类报告和混淆矩阵\nprint(\"\\n分类报告：\")\nprint(classification_report(np.argmax(y_test, axis=1), y_pred))\n\nprint(\"\\n混淆矩阵：\")\nprint(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n\n# 可视化损失\nplt.plot(range(epochs), losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n\n# 准备绘制子图的布局\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\naxes = axes.ravel()\n# 绘制每个子图\nfor i in range(3):\n    # 绘制分类结果的散点图\n    axes[i].scatter(X_train[y_train[:, 0] == 1, i], X_train[y_train[:, 0] == 1, i + 1], label='Class 1 (Train)', marker='o', c='blue')\n    axes[i].scatter(X_train[y_train[:, 1] == 1, i], X_train[y_train[:, 1] == 1, i + 1], label='Class 2 (Train)', marker='s', c='green')\n    axes[i].scatter(X_train[y_train[:, 2] == 1, i], X_train[y_train[:, 2] == 1, i + 1], label='Class 3 (Train)', marker='^', c='red')\n\n    # 绘制预测结果的散点图，使用不同的颜色和符号\n    axes[i].scatter(X_test[y_pred == 0, i], X_test[y_pred == 0, i + 1], label='Class 1 (Test)', marker='x', c='blue', alpha=0.5)\n    axes[i].scatter(X_test[y_pred == 1, i], X_test[y_pred == 1, i + 1], label='Class 2 (Test)', marker='x', c='green', alpha=0.5)\n    axes[i].scatter(X_test[y_pred == 2, i], X_test[y_pred == 2, i + 1], label='Class 3 (Test)', marker='x', c='red', alpha=0.5)\n\n    axes[i].set_xlabel(data.columns[i])\n    axes[i].set_ylabel(data.columns[i + 1])\n    axes[i].legend()\n\n# 设置整体图的标题和坐标轴\nplt.suptitle('Classification and Prediction Results')\nplt.tight_layout()\nplt.show()\n\n\n\n结果评估图像评估先来看我们第一张损失函数图像\n\n可以看得出，随着我们迭代次数的增加，模型的损失在不断的减小，符合我们的预期，可以说是一个很好的模型。那么我们接着往下看。\n\n这张图当中可以看得出来，训练集的数据已经用不同颜色和形状分布在各个地方，我们也能发现我们的预测集的x也合理的分布在他们所在的区域。\n当然最开始我是只画了一张图像的，就是第一张，我只对比了Sepal.Width和Sepal.Length两个维度下，三个品种之间的关系，你可以发现绿色和红色纠缠在一起，并不能很好的区分。是因为我们只画两个维度，但是实际模型考虑了四个维度，限于二维图像的表示限度，所以我就多画了几个图，尽可能多的考虑每个维度之间不同的结果，其实就可以发现分类其实是成功了的。\n当然我也画了三维图像，那个更加明显，不过，给各位留作思考吧！\n嘿嘿以后再说吧，其实目前我们已经可以很明显的看出来三者是可以区分的。\n控制台评估我们也生成了一份报告，如果图像并不能很好的分析的话，我们可以看实际的数据报告是什么样的。\n准确率： 0.9333333333333333\n\n分类报告：\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       0.89      0.89      0.89         9\n           2       0.91      0.91      0.91        11\n\n    accuracy                           0.93        30\n   macro avg       0.93      0.93      0.93        30\nweighted avg       0.93      0.93      0.93        30\n\n\n混淆矩阵：\n[[10  0  0]\n [ 0  8  1]\n [ 0  1 10]]\n\n\n准确率： 准确率是分类器预测正确的样本数占总样本数的比例。在这个例子中，准确率为0.9333333333333333，即约为93.33%。\n\n分类报告： 分类报告提供了关于每个类别的精确率（precision）、召回率（recall）和F1-score的信息。精确率是分类器预测为某一类别的样本中真实属于该类别的比例，召回率是真实属于某一类别的样本中被分类器预测为该类别的比例，F1-score是精确率和召回率的调和平均值。macro avg表示各类别的平均值，weighted avg表示按样本数加权的平均值。\n在这个例子中，对于类别0，精确率和召回率都为1.00，F1-score也为1.00；对于类别1，精确率和召回率都为0.89，F1-score为0.89；对于类别2，精确率和召回率都为0.91，F1-score为0.91。整体的macro avg和weighted avg的准确率、召回率和F1-score都在0.93左右。\n\n混淆矩阵： 混淆矩阵是分类器在测试集上的分类结果的矩阵表示。矩阵的行表示真实类别，列表示预测类别。对角线上的元素表示正确分类的样本数，非对角线上的元素表示错误分类的样本数。\n在这个例子中，对于类别0，有10个样本被正确分类；对于类别1，有8个样本被正确分类，1个样本被错误分类为类别2；对于类别2，有10个样本被正确分类，1个样本被错误分类为类别1。可以看到，大部分样本被正确分类，但仍有一些样本被错误分类。\n\n\n综合来看，这个分类器在测试集上的表现还是不错的，准确率达到了约93.33%。各类别的精确率、召回率和F1-score也都较为接近，说明模型对于不同类别的预测表现都较为稳定。但仍然有少数样本被错误分类，可能需要进一步优化模型，调整参数或特征，以进一步提高分类器的性能。\n其实大家可以多跑几次，会发现每次都会有些小差距，但是准确率一般都在91%~97%附近，还是蛮不错的模型了噢！\n任务留个小任务吧。\n思考：在多维度数据下，有什么方法可以更好的数据可视化？在这次分类中我们就可以发现一张二维图像可能很难看出三者之间的分类情况，何况我画了三张，对比了每两个维度三者之间的关系才可以勉强分析，那么有什么好方法呢？\n","slug":"irisSoftmax","date":"2023-07-21T10:46:33.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"6d0e0a76d24875617f81e74389647d3a","title":"构建GLM","content":"这节我们就要开始构造GLM了。\n假设您想建立一个模型，根据一些特征x（比如商店促销活动、最近的广告、天气、星期几等）来估计在任何给定的小时内到达您的商店（或您的网站上的页面浏览次数）的顾客数量y。我们知道泊松分布通常是用于描述访问者数量的好模型。有了这个信息，我们该如何为我们的问题构建一个模型呢？幸运的是，泊松分布是指数分布族的一种，因此我们可以应用广义线性模型（GLM）。在本节中，我们将描述一种构建GLM模型来解决这类问题的方法。\n问题背景首先，让我们考虑一个分类或回归问题，我们希望根据特征 x 预测随机变量 y 的值。为了推导适用于这个问题的GLM，我们做出以下三个假设：\n\n假设一： 在给定 x 和参数 θ 的情况下，y | x; θ 的分布服从某个指数族分布，其参数为 η。\n假设二： 在给定 x 的情况下，我们的目标是预测 T(y) 的期望值，即 *E[y | x]*。在我们的大多数示例中，我们有 T(y) = y，这意味着我们希望我们学习的假设 h 所输出的预测 h(x) 满足 *h(x) = E[y | x]*。（需要注意的是，这个假设对于逻辑回归和线性回归中的  的选择是成立的。例如，在逻辑回归中，我们有 。）\n假设三： 自然参数 η 和输入 x 之间是线性相关的：。（或者，如果 η 是向量值的，则 。）\n\n值得注意的是，第三个假设可能看起来较不充分，并且更应该被视为设计GLM的“选择”，而不是严格的假设。这三个假设/设计选择使我们能够推导出一类非常优雅的学习算法，即GLMs，具有许多理想的特性，如易于学习。此外，由此产生的模型通常非常有效，适用于对 y 的不同类型分布进行建模；例如，我们很快将展示逻辑回归和普通最小二乘法都可以作为GLMs的特定实例推导出来。\n普通最小二乘法（Ordinary Least Squares）为了证明普通最小二乘法是GLM模型族的一个特例，我们首先考虑目标变量 y（在GLM术语中也称为响应变量）为连续型的情况。我们将 y 给定 x 的条件分布建模为高斯分布 ，，其中 μ 可能依赖于 x。这样，我们就选择了上面提到的指数族分布中的分布作为高斯分布。\n正如之前提到的，将高斯分布表示为指数族分布时，我们有 。所以我们有：\n1. 首先，根据假设2，我们有:这表示我们的目标是预测 y 的条件期望值。\n2. 其次，由于我们选择的指数族分布是高斯分布，所以根据高斯分布的性质，其期望值 μ 就等于 η，即:3. 接下来，根据假设3，我们有:这表示 η 和输入 x 之间是线性相关的。\n将这些信息结合起来，我们得到：这就证明了普通最小二乘法是GLM模型族的一个特例。在普通最小二乘法中，我们的目标是最小化预测值与真实值之间的平方差，从而得到拟合效果最优的线性模型。普通最小二乘法是线性回归问题中最常见和经典的解决方法。在下一节，我们将继续探讨GLM模型族的其他实例，例如逻辑回归。\n逻辑回归（Logistic regression）接下来，我们将探讨逻辑回归。在这种情况下，我们对二元分类感兴趣，因此 y 只能取 0 或 1。考虑到 y 是二值的，选择伯努利分布族来建模给定 x 条件下 y 的条件分布是很自然的。将伯努利分布表示为指数族分布时，我们得到 。此外，注意到如果 ，则 。按照与普通最小二乘法相似的推导，我们得到：\n1. 首先，根据假设2，我们有：这表示我们的目标是预测 y 的条件期望值。\n2. 其次，由于我们选择的指数族分布是伯努利分布，所以根据伯努利分布的性质，其期望值 φ 就等于  函数的规范响应函数，即：3. 接下来，根据假设3，我们有：这表示 η 和输入 x 之间是线性相关的。\n将这些信息结合起来，我们得到：这就给出了逻辑回归的假设函数形式 。如果您曾经好奇我们是如何得到逻辑函数  的形式的，现在这就是答案：一旦我们假设 y 在给定 x 的条件下服从伯努利分布，逻辑函数的形式就是GLMs和指数族分布所定义的结果。\n此外，为了引入一些更多的术语，将分布的均值表示为自然参数的函数的函数 g（）称为规范响应函数。它的逆函数  被称为规范链接函数。因此，对于高斯族，规范响应函数是恒等函数；而对于伯努利分布，规范响应函数是逻辑函数。\n\n\n\n\n\n\n\n\n\n很多文献使用g来表示链接函数，而来表示响应函数；但是，我们在这里使用的符号，继承自早期的机器学习文献，将更符合本课程其他部分使用的符号。\nSoftmax回归（Softmax regression）让我们再来看一个GLM的例子，即Softmax回归。这个模型适用于多类别分类问题，其中响应变量 y 可以取 k 个值中的任意一个，即 y ∈ {1, 2, . . . , k}。例如，我们不再只将电子邮件分类为垃圾邮件或非垃圾邮件（二元分类问题），而是希望将其分类为三类，比如垃圾邮件、个人邮件和工作邮件。虽然响应变量仍然是离散的，但现在可以取多于两个值。因此，我们将其建模为多项式分布。\n参数化多项式分布为了参数化包含 k 个可能结果的多项式分布，我们可以使用 k 个参数  来指定每个结果的概率。但是，这些参数是冗余的，即不独立的（因为知道任意 k-1 个  将唯一确定最后一个，因为它们必须满足 ）。因此，我们只使用 k-1 个参数  来参数化多项式分布，其中 ，而 。为了方便起见，我们还可以让 ，但是我们应该记住这不是一个参数，并且完全由  确定。\n表示为指数族分布为了将多项式分布表示为指数族分布，我们定义  如下：，，，，，与之前的例子不同，这里我们不再有 ；而且， 现在是一个 k - 1 维向量，而不是一个实数。我们用  来表示向量  的第 i 个元素。\n指示函数我们再引入一个非常有用的符号。指示函数 1{·} 在其参数为真时取值为 1，在其参数为假时取值为 0（1{True} = 1，1{False} = 0）。例如，1{2 = 3} = 0，1{3 = 5 - 2} = 1。因此，我们还可以将  与  之间的关系表示为 (。（在继续阅读之前，请确保你理解了这是为什么！）此外，我们有 。\n多项式分布是指数族的成员现在，我们准备展示多项式分布是指数族的成员。我们有以下推导：\n\n其中：\n\n\n这样，我们完成了将多项式分布表述为指数族分布的过程。\n链接函数和响应函数我们得到了链接函数的定义（对于 i = 1, . . . , k）为：为方便起见，我们还定义了 。为了求反函数并得到响应函数，我们有：这意味着 ，将其代回公式得到响应函数为：这个从 η 到 φ 的函数被称为Softmax函数，它是一个非常有用的函数，常用于多类别分类问题中，用于将线性输出转换为类别概率。\n完成 Softmax 回归为了完成我们的模型，我们使用之前给出的第三个假设，即  与  呈线性关系。因此，我们有 （对于 ），其中 是我们模型的参数。为了方便起见，我们还可以定义 ，这样 ，如之前所述。因此，我们的模型假设给定  时  的条件分布为：这个适用于 y ∈ {1, . . . , k} 的分类问题的模型被称为 softmax 回归。它是逻辑回归的一种推广。\n模型输出我们的假设将输出：换句话说，我们的假设输出对于每个值 i = 1, . . . , k 的估计概率 。（尽管上面定义的  只有 k − 1 维，很显然  可以通过计算  获得。）\n参数拟合最后，我们讨论参数拟合。与我们之前推导普通最小二乘法和逻辑回归时类似，如果我们有一个包含 n 个示例的训练集 {}，并希望学习该模型的参数 ，我们首先将写出对数似然函数：得到上面第二行的式子，我们使用了之前给出的  的定义。现在，我们可以通过最大化  关于  来获得参数的最大似然估计，使用梯度上升或牛顿法等方法进行优化。\n\n\n\n\n\n\n\n\n\n这个部分主要讲解了Softmax回归模型的建立过程和数学推导。如果不懂，没有关系。\n深入了解数学推导对于机器学习的初学者并非必须，但它有助于加深对机器学习算法和模型的理解。对于初学者，重点应该放在理解基本概念和算法的直觉上，而不是过于深入的数学推导。\n理解数学推导可以帮助你更好地理解算法是如何工作的、为什么选择特定的方法以及它们的局限性。这对于在实际问题中选择合适的模型、调整超参数以及解决算法的性能问题非常有帮助。此外，深入了解数学推导还可以帮助你更好地理解机器学习文献和研究论文，从而保持对新进展的敏感性。\n然而，对于初学者来说，过度关注数学细节可能会让学习过程变得繁琐，甚至让人望而却步。如果你对数学不是特别熟悉，你可以先掌握机器学习的基本概念、常用算法和实践技巧。一旦你对机器学习有了较好的理解，再逐步深入学习数学推导也是一种不错的学习路径。\n最重要的是保持学习的兴趣和动力。你可以根据自己的兴趣和学习目标来决定学习深入数学推导的程度。有时候，实际动手实践和解决实际问题可能比过多纠结于数学推导更加有意义。不断实践和尝试在真实数据上应用机器学习算法将有助于你更快地掌握这门领域。\n","slug":"ConstructingGLMs","date":"2023-07-20T08:10:30.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"9397670a03548da92beb1eda8943120b","title":"指数族","content":"为了逐步理解广义线性模型（GLM），我们从定义指数族分布开始！指数族分布是一类神奇的分布，它具有以下形式：其中：\n\ny 是随机变量的取值；\nη 是参数向量（也称为典范参数）；\nT(y) 是充分统计量（sufficient statistic）的向量（对于我们考虑的分布，通常有T(y) = y）；\na(η) 是对数分区函数（log partition function）；\nb(y) 是规范化因子（normalizing factor）。\n\n不要被公式吓到，让我们看看它们的可爱之处！\n 就像一枚魔法徽章，它确保分布  在所有 y 上的和或积等于 1，这样我们就能有一个完美的概率分布。\n这个指数族分布包含许多我们耳熟能详的朋友，比如正态分布、伯努利分布、泊松分布等。而在广义线性模型中，我们将利用指数族分布的特性来构建适用于回归和分类问题的模型。\n探索不同的分布族在 GLM 世界中，固定 T、a 和 b 的选择定义了一个由 η 参数化的分布族（或集合）。当我们改变 η 时，我们会得到这个族中的不同分布。每个族族中的成员都有自己的个性和特点，让我们带着好奇心探索一番！\n伯努利分布和高斯分布是指数族分布的宠儿！让我们看看伯努利分布和高斯分布是指数族分布的两个魔法示例。\n伯努利分布：它是二分类问题中的常客！伯努利分布的均值为 ，写作 Bernoulli()。它规定了  上的分布，使得  和 。随着  的变化，我们会得到具有不同均值的伯努利分布。我们将展示通过改变  获得的这类伯努利分布恰好是指数家族分布。\n我们将Bernoulli分布写成：因此，自然参数由给出。有趣的是，如果我们通过将表示为的函数来反演这个定义，我们会得到。这就是熟悉的sigmoid函数！这在我们将逻辑回归推导为GLM时再次出现。\n将伯努利分布表达成指数家族分布为了将伯努利分布完整地表述为指数家族分布，我们还有以下参数：这表明伯努利分布可以使用适当的T、a和b的选择来写成以上公式的形式。\n高斯分布（正态分布）让我们来研究一下高斯分布。咦，还记得我们推导线性回归时提到的那个方差  吗？其实它对最终的  和  是没有影响的，所以我们可以大胆地选择  的任意值，而不改变任何结果。为了简化接下来的推导，我们就设定  吧！\n\n\n\n\n\n\n\n\n\n嗯，其实如果我们保留  作为一个变量，高斯分布也可以归入指数分布族。在这种情况下， 是一个依赖于  和  的二维向量。然而，在广义线性模型（GLM）中，我们可以通过考虑指数分布族的更一般定义来处理  参数：。这里的  被称为离散参数，对于高斯分布，。但是，为了简便起见，我们在这里只考虑了我们之前假定的情况。\n\n好的，现在让我们来看看高斯分布的奇妙之处：接下来，让我们来看看高斯分布在指数族中的参数表示：指数分布族中还有许多其他分布：多项式分布（稍后我们会看到），泊松分布（用于建模计数数据；也请参阅问题集）；伽马分布和指数分布（用于建模连续、非负的随机变量，比如时间间隔）；贝塔分布和狄利克雷分布（用于概率分布）等等。\n在下一节中，我们将向你展示一个通用的“方法”，用于构建模型，其中 （在给定  和  的情况下）来自上述任意分布之一。\n","slug":"The-exponential-family","date":"2023-07-20T07:17:04.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"005491f74262b9b4c835e159a3d618ac","title":"一些题外话","content":"这章节的在课程的笔记里面就是叫题外话，因为和下一节都比较短，所以我就放到一起来记录。\n题外话：感知机学习算法咱们现在稍微偏离一下主题，来简单探讨一种历史上颇具趣味的算法，同时在后面学习理论的时候也会再次回到它。这个算法是从逻辑回归方法修改而来，以强制其输出值为 0 或 1。为了实现这个目标，我们自然而然地改变了函数 g 的定义，将其设定为阈值函数：如果我们继续使用之前定义的 ，但使用这个修改过的 g 函数定义，并采用更新规则：那么我们得到了感知机学习算法。\n感知机学习算法是机器学习中的一个简单算法，用于解决二分类问题。它的灵感来源于神经科学中的感知神经元概念，由 Frank Rosenblatt 于 1957 年提出。\n算法的目标是找到一个超平面，将数据分为两个类别。假设输入样本是一个  维特征向量 ，超平面的表达式为 ，其中  是需要学习的参数向量。对于任意输入 ，如果 ，则预测为正类；如果 ，则预测为负类。\n算法的更新规则如下：\n\n初始化  为一个随机向量或零向量。\n\n对于每个训练样本 ，其中  是标签（1 或 -1）：\na. 计算预测值：。\nb. 如果  和  异号，则更新 ：，其中  是学习率。\n\n\n算法将持续迭代以上步骤，直到所有样本被正确分类或达到预定的迭代次数。需要注意的是，感知机算法只能解决线性可分问题，如果数据线性不可分，则无法收敛。\n在 1960 年代，这个”感知机”被认为是描述大脑中个体神经元工作的一个简略模型。虽然感知机在外观上可能与之前讨论过的其他算法相似，但它实际上与逻辑回归和最小二乘线性回归有很大的不同；特别是，为感知机的预测赋予有意义的概率解释，或将其推导为最大似然估计算法都是非常困难的。不过，它作为学习理论的一个出发点，将为我们的分析提供有趣的帮助。\n另一种最大化的算法现在我们回到使用sigmoid函数作为  的逻辑回归，并讨论一种不同的最大化  的算法。\n让我们首先考虑用牛顿法来寻找函数的零点。假设我们有一个函数 ，我们希望找到一个值 ，使得 。这里， 是一个实数。牛顿法的更新公式如下：这个方法有一个直观的解释，我们可以将其看作是通过一个与当前猜测  处的  相切的线性函数来近似函数 ，然后求解该线性函数为零的位置，并将下一个猜测  设为该位置。\n下面是牛顿法的示意图：\n\n在最左边的图中，我们看到函数  的曲线以及直线 。我们试图找到 ，使得 ；在该例子中， 的值约为 1.3。假设我们将算法的初始值设为 。牛顿法接着拟合了一个与  处的  相切的直线，并求解该直线的零点。（中间的图）这给出了下一个猜测的 ，约为 2.8。最右边的图显示了再进行一次迭代的结果，将  更新为约 1.8。经过几次迭代后，我们迅速接近 。\n牛顿法提供了一种找到  的方法。那么，如果我们想要用它来最大化某个  函数呢？ 函数的极大值对应于其一阶导数  等于零的点。因此，我们可以令 ，然后使用相同的算法来最大化 ，得到更新规则：（思考一下：如果我们想要用牛顿法来最小化而不是最大化一个函数，这将如何改变？）\n最后，在逻辑回归的设置中， 是一个向量，因此我们需要将牛顿法推广到这种多维情况（也称为牛顿-拉夫逊方法）。推广到多维的公式如下：其中， 通常表示  对于  的偏导数的向量； 是一个  的矩阵（实际上是 ，假设我们包括截距项），称为Hessian 矩阵，其元素由以下公式给出：通常情况下，牛顿法 比（批量）梯度下降收敛得更快，并且需要较少的迭代次数来接近最小值。然而，一次牛顿法的迭代可能比一次梯度下降的迭代更昂贵，因为它需要找到和求逆一个  的 Hessian 矩阵；但只要  不是太大，整体上牛顿法通常更加高效。当牛顿法应用于最大化逻辑回归的对数似然函数  时，得到的方法也称为 Fisher scoring 方法。\n广义线性模型下一章节我们就要学习到广义线性模型，到目前为止，我们已经看到了一个回归示例和一个分类示例。在回归示例中，我们有，而在分类示例中，，其中和是和的合适定义函数。在下一节当中，我们将展示这两种方法都是更广泛的模型家族——广义线性模型（Generalized Linear Models，GLMs）的特例。我们还将展示如何导出和应用GLM家族中的其他模型来解决其他分类和回归问题。\n","slug":"Some-Digression","date":"2023-07-20T06:30:32.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"6ad60e40b1cfbe85f1c534ea3d1910e1","title":"（实战）鸢尾花数据集的二分类","content":"所以说第一行打不了字是吧，莫名其妙。（不用管这一行，刚刚发现我的编辑器第一行莫名其妙打不了字，所以你看到的这是第二行，很烦）\n昨天我学习了逻辑回归，今天就要实战一下啦！因为光看纸上的知识总感觉有点枯燥，咱们动手试试，把学到的知识化为力量吧！今天我们要玩的游戏是逻辑回归，而游戏场地就是鸢尾花的数据集。\n数据集：鸢尾花大冒险咱们先打开Excel，看看我们要玩的鸢尾花数据集是什么样子滴！第一列是索引，我们直接删掉啦！（记得一定要删掉哦！不然程序会迷失方向，不信你试试！不过别问我为什么知道… T T）\n\n可以发现，第一行是表头数据，分别有Sepal.Length（萼片长度），Sepal.Width（萼片宽度），Petal.Length（花瓣长度），Petal.Width（花瓣宽度），Species（种类），这几个，而Species中分为三个种类，分别是“setosa”，“versicolor”和“virginica”，分别是“小山鸢尾”、“变色鸢尾”和“维吉尼亚鸢尾”。\n准备阶段：选取我们的“法宝”首先，咱们得明确自己的任务和目标哦：我们要用逻辑回归算法解决一个二分类问题，把这个法宝应用于鸢尾花数据集。目标是训练逻辑回归模型，并查看它在测试集上的“施法”效果。\n这里咱们可要动用一些神奇的“法宝”呢！数据处理用强大的Pandas，数值计算可凭借老生常谈的numpy库，train_test_split可用来把数据分为训练集和测试集，LogisticRegression则是我们要打造的逻辑回归模型，还有accuracy_score、precision_score和recall_score这三把尺子，它们能帮我们测量评估指标的准确度哦！最后，还有matplotlib.pyplot可将数据可视化，让我们一起瞧瞧吧！\n所以准备工作差不多就这些，先把所需要的模块导入。\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nimport matplotlib.pyplot as plt\n\n魔法准备：定义一些有趣的“法术”现在，我们要定义一些有趣的“法术”来玩逻辑回归啦！这些“法术”都是数学公式变身的，让Python来帮我们算算就好啦！\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef log_likelihood(features, target, weights):\n    scores = np.dot(features, weights)\n    ll = np.sum(target * scores - np.log(1 + np.exp(scores)))\n    return ll\n\ndef logistic_regression(features, target, num_steps, learning_rate):\n    intercept = np.ones((features.shape[0], 1))\n    features = np.hstack((intercept, features))\n    weights = np.zeros(features.shape[1])\n\n    for step in range(num_steps):\n        scores = np.dot(features, weights)\n        predictions = sigmoid(scores)\n\n        output_error_signal = target - predictions\n        gradient = np.dot(features.T, output_error_signal)\n        weights += learning_rate * gradient\n\n        # Print log-likelihood every 100 steps\n        if step % 100 == 0:\n            print(f\"Step {step}, Log-Likelihood: {log_likelihood(features, target, weights)}\")\n\n    return weights\n\n第一个“法术”叫做sigmoid，它是逻辑回归中的魔法函数，负责把输入的数值变成0到1之间的可爱小数哦！\n接下来是log_likelihood，“法术”嘛，它能算出对数似然性，看起来有点高深，其实是帮我们衡量预测结果的好坏程度！\n最后一个“法术”是logistic_regression，这个可厉害了！它用梯度上升算法，一步步地训练逻辑回归模型，就像咱们在山上慢慢攀登一样！它还会在每爬100步的时候，打印出对数似然性，让我们知道自己的成长进度哦！\n大冒险准备：数据预处理怎么个处理法，其实好像没什么需要处理的，无非就是读取，存储，几个简单的转换？差不多吧。\n# 加载数据集并分成训练集和测试集\ndata_path = 'iris.csv' # 你的路径！\ndata = pd.read_csv(data_path)\n\n# 将'setosa'标记为1，其他两种花标记为0，实现二分类问题\ndata['Species'] = data['Species'].map({'setosa': 1, 'versicolor': 0, 'virginica': 0})\n\n# 提取特征和标签\nfeatures = data.drop('Species', axis=1)\ntarget = data['Species']\n\n# 分割数据集\ntrain_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.2, random_state=42)\n\n首先，我们要加载数据集，它就像我们的宝藏地图一样，指引着我们前进的方向。我们用pd.read_csv()将数据从csv文件中读取出来，并把花的种类标记为1和0，就像宝藏上的标记一样，让我们可以区分哪些是我们要找的，哪些是普通的岩石。瞧，鸢尾花的种类有点像三个魔法宝石，我们要把其中一个宝石标记为1，其他两个标记为0，这样它们就变成了我们要寻找的目标。\n接下来，我们得把宝藏中的特征和目标提取出来。特征就像地图上的路线，目标就像我们要找的宝藏。咱们把特征放进features变量，把目标放进target变量，这样咱们就能对它们进行后续的处理啦。\n鸢尾花大冒险的第一步就是分割数据集，将宝藏地图分成训练集和测试集，这样我们才能在训练中不断成长，最后测试一下自己的探险成果。通过train_test_split，我们将数据集分成了两部分，80%的数据用来训练，20%的数据留作测试，而随机种子random_state=42则确保我们在不同的时候玩同样的游戏。\n参数：选个好武器，准备战斗# 使用训练集来训练逻辑回归模型\nnum_steps = 1000\nlearning_rate = 0.01\nweights = logistic_regression(train_features, train_target, num_steps, learning_rate)\n\n接下来，我们要选择适合鸢尾花大冒险的武器了。先把自己的装备整理一下，我们需要设定一些超级厉害的“法宝”：迭代次数num_steps和学习率learning_rate，这些会决定我们在冒险中的表现。好了，现在我们就准备好了，可以正式进入鸢尾花的世界了！\n“法宝”已经准备齐全，现在我们需要通过logistic_regression函数，调用那些魔法“法术”来进行模型的训练。这个过程就像在修炼魔法一样，每一步都在增强我们的力量。训练过程中，我们会在每爬100步的时候，打印出当前的对数似然性，这样我们就能看到自己的成长历程。\n你也可以在这部分通过调整迭代次数和学习率来观察不同情况下的模型表现。\n探索成果，评估表现！test_intercept = np.ones((test_features.shape[0], 1))\ntest_features = np.hstack((test_intercept, test_features))\ntest_predictions = sigmoid(np.dot(test_features, weights))\ntest_predictions = np.round(test_predictions)\n\naccuracy = accuracy_score(test_target, test_predictions)\nprecision = precision_score(test_target, test_predictions)\nrecall = recall_score(test_target, test_predictions)\n\nprint(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n\n这一部分代码使用训练后的模型权重对测试集进行预测，并计算评估指标。首先，将测试集的特征矩阵添加一列全为1的列作为截距，然后通过逻辑函数sigmoid和权重矩阵计算预测结果。为了得到最终的分类结果，将预测概率四舍五入为0或1。\n最后，计算预测结果与测试集真实标签之间的准确率、精确率和召回率，并将结果打印输出。\n\n\n\n\n\n\nTIP\n这里我再来贴一下什么是准确率，精确率和召回率。\n\n准确率（Accuracy）： 准确率是最常用的模型性能指标之一，它表示模型正确预测的样本数与总样本数之间的比例。即：(预测正确的样本数) / (总样本数)。准确率越高，表示模型的整体性能越好。\n精确率（Precision）： 精确率是针对预测为正例的样本而言的，它表示在所有预测为正例的样本中，模型正确预测为正例的比例。即：(真正例数) / (真正例数 + 假正例数)。精确率高表示模型对正例的预测较准确。\n召回率（Recall）： 召回率是针对实际为正例的样本而言的，它表示在所有实际为正例的样本中，模型正确预测为正例的比例。即：(真正例数) / (真正例数 + 假负例数)。召回率高表示模型对正例的识别能力较强。\n\n\n\n数据可视化由于鸢尾花数据集有四个特征，我们可以选择任意两个特征来进行可视化处理。\nfeature_names = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\ntarget_names = ['setosa', 'versicolor/virginica']\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 15))\n\nfor idx, ax in enumerate(axes.flat):\n    i, j = idx // 2, idx % 2\n    ax.scatter(data[data['Species'] == 1][feature_names[i]], data[data['Species'] == 1][feature_names[j]], label='setosa')\n    ax.scatter(data[data['Species'] == 0][feature_names[i]], data[data['Species'] == 0][feature_names[j]], label='versicolor/virginica')\n    ax.scatter(test_features[test_target == 1][:, i+1], test_features[test_target == 1][:, j+1], marker='o', edgecolors='red', facecolors='none', label='setosa (predicted)')\n    ax.scatter(test_features[test_target == 0][:, i+1], test_features[test_target == 0][:, j+1], marker='x', color='red', label='versicolor/virginica (predicted)')\n    ax.set_xlabel(feature_names[i])\n    ax.set_ylabel(feature_names[j])\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n最后这一部分代码进行了数据可视化处理，使用散点图展示了不同特征组合下的分类结果。这部分代码使用了matplotlib.pyplot库，创建了6个子图，并在每个子图中绘制了鸢尾花数据集中的两个特征。蓝色的点代表’Setosa’类别，橙色的点代表’Versicolor’和’Virginica’类别。预测集的结果用红色的圆圈（’o’）表示’Setosa’类别，用红色的叉（’x’）表示’Versicolor’和’Virginica’类别。\n完整代码如下\n\nClick to see more\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nimport matplotlib.pyplot as plt\n\n# 1. 逻辑回归算法的代码实现\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef log_likelihood(features, target, weights):\n    scores = np.dot(features, weights)\n    ll = np.sum(target * scores - np.log(1 + np.exp(scores)))\n    return ll\n\ndef logistic_regression(features, target, num_steps, learning_rate):\n    intercept = np.ones((features.shape[0], 1))\n    features = np.hstack((intercept, features))\n    weights = np.zeros(features.shape[1])\n\n    for step in range(num_steps):\n        scores = np.dot(features, weights)\n        predictions = sigmoid(scores)\n\n        output_error_signal = target - predictions\n        gradient = np.dot(features.T, output_error_signal)\n        weights += learning_rate * gradient\n\n        # Print log-likelihood every 100 steps\n        if step % 100 == 0:\n            print(f\"Step {step}, Log-Likelihood: {log_likelihood(features, target, weights)}\")\n\n    return weights\n\n# 2. 选择一个适合的二分类数据集\n# 在此示例中，我们将使用鸢尾花数据集\n\n# 3. 加载数据集并分成训练集和测试集\ndata_path = 'iris.csv' # 你的路径！！！\ndata = pd.read_csv(data_path)\n\n# 将'setosa'标记为1，其他两种花标记为0，实现二分类问题\ndata['Species'] = data['Species'].map({'setosa': 1, 'versicolor': 0, 'virginica': 0})\n\n# 提取特征和标签\nfeatures = data.drop('Species', axis=1)\ntarget = data['Species']\n\n# 分割数据集\ntrain_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.2, random_state=465)\n\n# 4. 使用训练集来训练逻辑回归模型\nnum_steps = 1000\nlearning_rate = 0.01\nweights = logistic_regression(train_features, train_target, num_steps, learning_rate)\n\n# 5. 在测试集上评估模型性能\ntest_intercept = np.ones((test_features.shape[0], 1))\ntest_features = np.hstack((test_intercept, test_features))\ntest_predictions = sigmoid(np.dot(test_features, weights))\ntest_predictions = np.round(test_predictions)\n\naccuracy = accuracy_score(test_target, test_predictions)\nprecision = precision_score(test_target, test_predictions)\nrecall = recall_score(test_target, test_predictions)\n\nprint(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n\n# 6. 数据可视化处理（放到同一张图里的子图中）\n# 由于鸢尾花数据集有四个特征，我们可以选择任意两个特征来进行可视化处理。\n\nfeature_names = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\ntarget_names = ['setosa', 'versicolor/virginica']\n\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 15))\n\nfor idx, ax in enumerate(axes.flat):\n    i, j = idx // 2, idx % 2\n    ax.scatter(data[data['Species'] == 1][feature_names[i]], data[data['Species'] == 1][feature_names[j]], label='setosa')\n    ax.scatter(data[data['Species'] == 0][feature_names[i]], data[data['Species'] == 0][feature_names[j]], label='versicolor/virginica')\n    ax.scatter(test_features[test_target == 1][:, i+1], test_features[test_target == 1][:, j+1], marker='o', edgecolors='red', facecolors='none', label='setosa (predicted)')\n    ax.scatter(test_features[test_target == 0][:, i+1], test_features[test_target == 0][:, j+1], marker='x', color='red', label='versicolor/virginica (predicted)')\n    ax.set_xlabel(feature_names[i])\n    ax.set_ylabel(feature_names[j])\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n# 7. 尝试调整学习率、迭代次数等参数，观察对模型性能的影响\n# 你可以尝试不同的学习率和迭代次数，观察模型在测试集上的性能变化。\n\n\n\n结论\n\n于是我们就能得到如上的图片，并且控制台打印如下：\nStep 0, Log-Likelihood: -466.2106144434709\nStep 100, Log-Likelihood: -0.36586843732581664\nStep 200, Log-Likelihood: -0.261779308423121\nStep 300, Log-Likelihood: -0.2055975355203343\nStep 400, Log-Likelihood: -0.17016379364395556\nStep 500, Log-Likelihood: -0.14565669423300498\nStep 600, Log-Likelihood: -0.12763643409226652\nStep 700, Log-Likelihood: -0.11379421631281457\nStep 800, Log-Likelihood: -0.1028071299788049\nStep 900, Log-Likelihood: -0.09386101997678553\nAccuracy: 1.0, Precision: 1.0, Recall: 1.0\n\n图片分析\n图片中的子图：在图片中的每个子图中，横轴和纵轴分别表示两个特征，点的分布代表鸢尾花数据集中的样本分布。蓝色点表示’Setosa’类别，橙色点表示’Versicolor’和’Virginica’类别。红色圆圈（’o’）表示模型预测为’Setosa’类别的样本，红色叉（’x’）表示模型预测为’Versicolor’和’Virginica’类别的样本。\n分析子图的区分度：观察每个子图中的蓝色点和橙色点的分布情况，可以看出这两类样本在这两个特征上的区分度。如果两类样本在某个特征上有明显的分离，说明该特征对分类有较强的区分能力。\n分析预测结果：观察红色圆圈和红色叉的分布情况，它们代表了模型在测试集上的预测结果。如果模型分类准确，预测为’Setosa’类别的样本应该位于蓝色点的附近，预测为’Versicolor’和’Virginica’类别的样本应该位于橙色点的附近。如果红色圆圈和红色叉分布与蓝色点和橙色点重叠较少，说明模型的预测效果较好。\n\n控制台分析从控制台打印的日志可以看出模型在训练过程中的对数似然性（Log-Likelihood）逐步增大，并在迭代次数较少时就收敛到一个较小的负值。最后，模型在测试集上的评估结果显示准确率（Accuracy）、精确率（Precision）和召回率（Recall）均为1.0，即100%。\n对于对数似然性的变化：\n\n在训练初始阶段（Step 0到Step 100），对数似然性从一个较大的负值逐渐减小，说明模型的拟合效果在改进，损失逐渐减小。\n在训练中期（Step 100到Step 500），对数似然性下降幅度较大，模型在这个阶段学习到了较多的特征权重，开始更好地拟合训练集。\n在训练后期（Step 500到Step 900），对数似然性下降速度减缓，模型接近收敛。这时候模型已经较好地拟合训练集，但仍然可能存在一些小的误差。\n\n对于评估结果：\n\n准确率、精确率和召回率均为1.0，表示模型在测试集上的分类结果完美地匹配了真实标签。准确率表示模型正确分类样本的比例，精确率表示模型预测为正类的样本中真正为正类的比例，召回率表示模型正确识别正类样本的比例。这些指标都为1.0表明模型对于’Setosa’类别的预测完全正确，没有产生误分类。\n\n综合来看，这个结果可能存在一些问题：\n\n数据集可能过于简单：鸢尾花数据集相对简单，可能存在较强的线性关系，使得逻辑回归模型能够在这个数据集上表现得很好。\n过拟合：虽然在测试集上表现良好，但模型可能在训练集上过拟合，过度拟合了训练集的特点，导致在测试集上泛化能力较差。\n\n为了更好地评估模型性能，你可以进行交叉验证和尝试使用其他复杂度较高的数据集。交叉验证能够更全面地评估模型的泛化能力，复杂度较高的数据集能够更好地反映模型在现实场景中的表现。同时，尝试调整学习率、迭代次数等超参数，观察对模型性能的影响。这样可以更全面地评估模型的性能，并找到合适的模型和参数组合。\n","slug":"Secondary-classification-of-Iris","date":"2023-07-19T10:14:24.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"4c5f4ebe8f68ff24ce30098465671f7c","title":"分类与逻辑回归","content":"在机器学习的广袤领域中，分类问题犹如一片神秘的森林，吸引着众多探险家的目光。我们可以将自己想象成一位勇敢的森林导游，带领着各种生物来到分类问题的奇妙世界。\n什么是分类问题？在这片森林中，我们将聚焦于探讨分类问题。这与我们之前遇到的回归问题有些相似，但又有所不同。在分类问题中，我们要预测的目标值只有少数几种离散取值。我们现在先专注于二分类问题，也就是只能是0或者1。就像在森林中，我们要将每个生物划分为两类，比如“可爱的小动物”和“奇怪的怪兽”。0通常被称为负类，而1被称为正类，有时也用符号“-”和“+”来表示。在训练样本中，对于每个输入，我们都有相应的标签，也被称为标签。\n分类问题是机器学习中最常见的任务之一，我们的目标是构建一个模型，能够将新的未知数据准确地分类到正确的类别中。\n这个森林中的探险涉及以下关键要素：\n\n特征（Features）：每个生物都有一组特征，描述了它们的某些属性。这些特征可以是数字、文字、图像等形式，用于描述样本的特点。\n类别（Classes）：我们的目标是将每个生物分到预先定义的类别中。这些类别可以是二元的（例如，可爱的/奇怪的、是/否）或多元的（例如，红色/蓝色/绿色、动物种类）。\n训练数据（Training Data）：我们拥有一批标记好的样本，这些样本是已知类别的数据。我们将用这些数据来训练我们的模型，让它能够学习如何进行正确的分类。\n分类器（Classifier）：作为森林导游，我们需要一个特殊的工具，帮助我们将新生物正确地分类。这个工具就是分类器，它会根据生物的特征来做出分类决策，并将其划分到最有可能的类别中。一些常见的分类器包括逻辑回归、决策树、支持向量机等。\n\n我们可以使用分类问题的技术来构建垃圾邮件过滤器、图像识别器、情感分析器等各种实用的应用。\n了解了分类问题的本质和关键要素，我们就可以自信地踏入这片神奇的森林。接下来，让我们深入研究森林中一种重要的工具——逻辑回归，帮助我们更好地了解并解决分类问题。在这次奇妙的探险中，我们将发现更多的惊喜和乐趣！:deciduous_tree:\n逻辑回归：智慧护卫的二分类大师在机器学习的世界中，逻辑回归是一位智慧而灵活的护卫，专门擅长解决二分类问题。想象一下，他是我们探险中的得力助手，帮助我们在数据的海洋中，区分出两个不同的类别。无论是判断邮件是否为垃圾邮件，预测疾病是否会发生，还是决定客户是否会购买某个产品，逻辑回归都能忠实地指引我们找到正确答案。\n逻辑回归与线性回归的区别在分类问题中，我们通常会遇到一个挑战：目标值只能取0或者1，而线性回归的模型可能会给出超过这个范围的预测结果。为了解决这个问题，逻辑回归改变了我们的模型假设，引入了一个新的假设函数。这个函数使用了一个特殊的函数，也称为逻辑函数或Sigmoid函数，来确保预测结果总是在0和1之间。\n逻辑函数（Sigmoid函数）我们可以忽略是离散值的事实，使用我们之前的线性回归算法来尝试预测给定的。然而，很容易构造出在这种方法表现非常糟糕的例子。直观地讲，当我们知道时，的取值大于1或小于0是没有意义的。\n为了解决这个问题，让我们改变我们假设的形式。我们将选择：当这被称为逻辑函数或Sigmoid函数。以下是的绘图：\n\n\n我们可以看到，当趋近于正无穷时，趋近于1，而当趋近于负无穷时，趋近于0。这样的性质让我们的预测结果总是在合理的范围内，非常适合处理分类问题。此外，，因此也包括，始终保持在0和1之间。与之前一样，我们保持了的约定，使得。\n逻辑回归的拟合在逻辑回归中，我们需要拟合参数。与线性回归一样，我们使用了最大似然估计的方法来拟合参数。这意味着我们为分类模型赋予一组概率假设，然后通过最大似然估计来找到最合适的参数，使得我们的模型能够尽可能地拟合训练数据。 :detective:\n逻辑回归的参数拟合与最大似然估计在逻辑回归中，我们已经选择了逻辑函数作为我们的激活函数，它有助于将预测结果限制在0和1之间。同时，逻辑函数的导数具有一个非常有用的性质，这对我们进行参数拟合非常重要。\n我们将其写为：\n似然性与最大似然估计为了拟合逻辑回归模型的参数，我们需要定义一个衡量拟合程度的度量，这就是似然性（Likelihood）。对于逻辑回归而言，我们可以通过最大似然估计来找到最合适的参数。\n假设我们有个训练样本，每个样本的特征为，对应的标签为，其中。我们假设这些样本是独立生成的，即每个样本的生成与其他样本无关。然后，我们就可以写出参数的似然性。\n让我们假设：请注意，这可以更紧凑地写成假设这个训练示例是独立生成的，我们可以写出参数的似然性：\n为了计算方便，我们通常取似然性的对数，得到对数似然性（Log-Likelihood）：我们的目标是最大化对数似然性，即找到能够使得训练样本出现概率最大的参数。\n梯度上升算法我们如何最大化似然性？类似于我们在线性回归的推导中所做的，我们可以使用梯度上升算法。类似于线性回归中的梯度下降算法，梯度上升算法通过迭代更新参数来逐步寻找似然性的最大值。\n在向量表示法中，我们的更新将由 给出。（请注意更新公式中的正号而不是负号，因为现在我们要最大化函数而不是最小化。）\n\n\n\n\n\n\n\n\n\n这个更新公式表示在梯度上升算法中如何更新参数。在梯度上升算法中，我们希望最大化对数似然性，因为这是参数在训练数据上的似然性。梯度表示对数似然性关于参数的梯度向量，它告诉我们在当前参数的取值下，如何调整参数的方向，才能更好地拟合训练数据。\n具体来说，是学习率，表示每次更新参数的步长。在每一次迭代中，我们将当前的参数与学习率乘以梯度相加，得到新的参数。这样，我们逐步地沿着梯度的方向更新参数，直到达到一个满意的似然性最大值或者收敛。\n让我们从只有一个训练示例开始，我们可以计算对数似然性关于参数的偏导数，得到梯度：然后，我们使用以下更新规则来更新参数：其中，是学习率，用于控制每次更新的步长。\n可能有人看不懂，就用更易懂的表达来详细讲一下：\n\nClick to see more\n假设我们只有一个训练示例，其中是输入特征，是对应的标签。我们希望通过更新参数来使得我们的模型能够更好地拟合这个示例。\n首先，我们计算对数似然性关于参数的偏导数。这里需要用到逻辑函数的导数公式。\n推导过程如下：\n\n首先，我们计算对数似然性关于的偏导数：其中，表示通过逻辑函数将映射到0和1之间的预测结果。\n\n我们可以将上述偏导数的计算过程进行解释：\n\n首先，表示实际标签与预测值之间的差异。当预测值与实际标签一致时，差异为0，表示预测准确；当预测值与实际标签不一致时，差异为非零值，表示预测错误。\n其次，是预测错误的差异的方向，表示我们需要调整参数的方向，使得预测更加准确。\n最后，表示输入特征的第个分量，表示该特征对参数的影响程度。\n\n\n根据这个偏导数的计算结果，我们可以得到随机梯度上升法则的更新公式：其中，是学习率，用于控制每次参数更新的步长。这个公式告诉我们，在每一次更新中，我们将根据预测结果与实际标签的差异，乘以输入特征的对应分量，再乘以学习率，来更新参数。\n通过这样的参数更新过程，我们逐步调整参数的取值，使得我们的模型能够更好地拟合训练示例。这样，我们就可以使用逻辑回归来解决分类问题，并取得优秀的结果。\n\n\n\n\n如果将其与LMS更新规则进行比较，我们会发现它们看起来完全一样；但这并不是同一个算法，因为现在被定义为的非线性函数。尽管如此，令人惊讶的是，对于一个相当不同的算法和学习问题，我们最终得到了相同的更新规则。这是巧合吗，还是背后有更深层次的原因？我们将在讨论广义线性模型（GLM）时回答这个问题。\n任务任务，确实，只看不练可能什么都学不会，那么问题就来了，依旧是我让GPT生成的。\n作业任务一：逻辑回归代码实现与应用任务描述：学生需要使用Python或其他编程语言实现逻辑回归算法，并应用该算法来解决一个二分类问题。\n任务步骤：\n\n编写逻辑回归算法的代码实现，包括计算逻辑函数、计算对数似然性、梯度上升算法等关键部分。\n选择一个适合的二分类数据集，可以使用公开数据集或者自己构造一个数据集。\n将数据集分成训练集和测试集。\n使用训练集来训练你实现的逻辑回归模型。\n在测试集上评估你的模型性能，计算准确率、精确率、召回率等指标。\n最好进行数据可视化处理，让结果更直观。\n可选：尝试调整学习率、迭代次数等参数，观察对模型性能的影响。\n\n\n\n\n\n\n\nTIP\n\n准确率（Accuracy）： 准确率是最常用的模型性能指标之一，它表示模型正确预测的样本数与总样本数之间的比例。即：(预测正确的样本数) / (总样本数)。准确率越高，表示模型的整体性能越好。\n精确率（Precision）： 精确率是针对预测为正例的样本而言的，它表示在所有预测为正例的样本中，模型正确预测为正例的比例。即：(真正例数) / (真正例数 + 假正例数)。精确率高表示模型对正例的预测较准确。\n召回率（Recall）： 召回率是针对实际为正例的样本而言的，它表示在所有实际为正例的样本中，模型正确预测为正例的比例。即：(真正例数) / (真正例数 + 假负例数)。召回率高表示模型对正例的识别能力较强。\n\n\n\n我们这里可以使用一些著名的数据集，比如鸢尾花数据集\n作业任务二：逻辑回归性能优化任务描述：学生需要探索不同方法来优化逻辑回归算法的性能，并比较它们的效果。\n任务步骤：\n\n研究逻辑回归算法的原理和应用场景，了解逻辑回归的优缺点。\n选择一个适合的二分类数据集，可以使用公开数据集或者自己构造一个数据集。\n将数据集分成训练集和测试集。\n使用标准的逻辑回归算法来训练模型，并在测试集上评估模型性能，计算准确率、精确率、召回率等指标。\n尝试以下优化方法，并比较它们对模型性能的影响：\n特征缩放：尝试对特征进行缩放，例如使用标准化或归一化处理。\n多项式特征：尝试使用多项式特征来增加模型的复杂度。\n正则化：尝试使用L1或L2正则化来减少过拟合问题。\n不同的损失函数：尝试使用其他损失函数，如交叉熵损失函数。\n学习率调整：尝试使用不同的学习率，并观察其对模型训练的影响。\n\n\n分析不同优化方法对模型性能的影响，讨论哪些方法对提高模型性能效果更明显。\n\n","slug":"Classification-and-logistic-regression","date":"2023-07-18T09:41:28.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"90b9df300d80d6a498525ab2ef94faa4","title":"烟雨满楼山断续，人闲倚遍阑干曲","content":"窗外，暴雨纷纷，乌云低垂，苏州的梅雨季节如约而至。在这座古城的图书馆中，我坐在十几米高的落地窗前，眺望着那阴暗的天空，仿佛被乌云所包围，宛如置身于一个神秘而迷人的世界。\n耳机中传来舒缓怅寥的音乐，音符如烟，缓缓弥漫在空气中，将我渐渐带入一种无拘无束的心境。这样的境地，让我想起了一首曲子，曲调轻盈曼妙，与窗外的烟雨相得益彰。我静静倚在阑干之上，感受着外界的风雨，也感受着内心的起伏。\n雨滴轻拍着窗户，似乎在诉说着它们的故事。有时，它们轻柔细密，如琴弦上的音符，弹奏着悠扬的旋律；有时，它们密集而坚决，如大自然的鼓点，奏响着壮丽的交响。窗外的景象，让我不禁想起人生的变幻无常，有时充满温柔与宁静，有时又充满挑战与考验。\n思绪在雨中飘散，回忆与幻想交织在一起。曾经的美好时光如烟雾般逝去，我开始怀念那些曾经在雨中共度的时光。那时的我们，笑声回荡在雨中，雨水洒落在身上，却带不走我们的快乐与真诚。而今，人们似乎忙碌得已经忘记了停下脚步欣赏雨中的美好，忘记了雨后的清新与宁静。\n窗外的世界与内心的世界交错在一起，仿佛现实与幻想融为一体。滚滚闷雷与闪电，如同内心的冲动和矛盾。我感受到了自然的力量，也感受到了自己内心的深处。这样的时刻，仿佛让我与自己对话，让我重新审视人生的意义和价值。\n如画般的烟雨世界里，人生的意义何在？曾经的欢笑与忧伤，如同雨滴一般洒落在岁月的长河中，渐行渐远。我们追逐着时光的脚步，却不曾停下来，静心感受雨滴的触碰，细数人生的珍贵瞬间。\n社会喧嚣纷杂，人们忙于奔波，追逐物质的诱惑和名利的虚幻。而在这个纷扰的世界里，倚在窗前，感受着烟雨的滋润，心灵也逐渐清澈起来。沉浸在自然的怀抱中，如同一片飘游的落叶，在风雨中悠然自得。\n窗外的雨丝细密，宛如命运的细微变化，我们往往忽略了其中蕴含的深意。雨点在空中交织，汇聚成一片迷蒙的幕帘，将现实与梦幻交错。在这濛濛雨幕之中，我感到一种超越物质束缚的力量。人生不应仅仅是功利与得失，而是要追寻内心的平静与自由。\n或许，我们需要重新审视雨后的清新与宁静。风雨过后，自然焕发出崭新的生机，洗净尘埃，洒满希望。我们也应该学会洗去尘世的浮华，重拾内心的宁静。无论是风雨中的人生起伏，还是世俗纷扰中的烦恼，我们都应该保持一颗平和的心态，让内心如烟雨一般潋滟。\n窗前，我静观着烟雨中的景象，思绪渐渐清晰。雨滴拍打着窗户，如同敲打着内心的门扉，唤醒沉寂已久的思考。我明白，人生的意义并非只有在追逐外在的名利与成就中才能找到，而是要在心灵的寄托和内心的自由中感悟。\n窗外的烟雨依旧满山，而我在烟雨中寻找着自己。在这座古城的图书馆里，我凝视着窗外，瞻仰着世间的变幻。或许，我们需要时刻保持对内心的关照，让心灵在烟雨中沐浴，感受到生命的真谛。人生如烟雨，瞬间绽放，转瞬消散。而唯有在内心的澄明中，才能拥有永恒的宁静与自由。\n这烟雨满楼山断续，人闲倚遍阑干曲。我愿倚在窗前，继续聆听雨点的轻拍，感受大自然的魅力。我愿将思绪凝成文字，传达内心的触动与感悟。因为在这烟雨之中，我找到了生命的力量，找到了心灵的栖息地。\n他说：\n“人生如烟雨，一朝繁华，一念清欢。”\n","slug":"烟雨满楼山断续","date":"2023-07-17T09:23:37.000Z","categories_index":"","tags_index":"随笔","author_index":"General_K1ng"},{"id":"465c62620dad1684532695600a6cd19f","title":"局部加权线性回归*","content":"引言回顾最小二乘法的概率解释，我们曾深入研究过如何通过最小化误差的平方和，寻找最佳的模型参数。这个方法的确非常强大，但有时候我们需要更加灵活和精确的工具来解决特定的问题。\n于是，引入局部加权线性回归（Locally Weighted Linear Regression）。这个方法可以被视为我们探险旅程中的一把望远镜，帮助我们看到更加微妙的模式和趋势。它以一种非常独特的方式，通过为每个数据点赋予一定的权重，使得我们的模型更加关注那些在当前预测点附近的样本。\n这里的”局部加权”，就好像是一把神奇的放大镜，它能够放大我们感兴趣的那些数据点，并且根据它们的特点，为我们提供一种个性化的预测模型。无论是在金融领域，医疗研究，还是天气预测，局部加权线性回归都能够以其独特的魅力，帮助我们更好地理解和解决问题。\n这就像是在一片茂密的丛林中，我们只专注于离我们最近的树木和植被，而忽略了远处的景象。局部加权线性回归带给我们的是一种局部敏感性，使我们能够更加准确地预测和理解数据的变化。\n局部加权线性回归（LWS）欠拟合与过拟合考虑一个让我们一起探索的有趣问题：如何从输入预测输出呢？这就是我们今天要聊的局部加权线性回归（LWR）算法啦！想象一下，我们置身于一个神奇的数据世界中，我们希望找到一种方法，能够让我们的模型更好地适应这些数据点。\n嘿，看看这张魔法般的图片吧！最左边的图展示了我们试图通过一条直线去拟合这些数据点的情况。可是，你有没有发现，这些点并不完全落在这条线上呢？是不是感觉有些不太对劲？没错，这就是我们所谓的欠拟合。这位可爱的小模型童鞋似乎没有完全抓住数据中的结构，留下了一些没被捕捉到的东西。\n\n\n于是，我们心生了一个奇妙的想法：如果我们再加上一个特征，试试拟合这样的模型，会发生什么呢？嗯嗯，看中间的图，你会发现拟合效果好像稍微好了一些。看起来添加更多的特征会让拟合效果变得更好，是不是有些开心呢？但是，小心别太过头哦！右边的图就展示了一个过拟合的例子，我们使用了一个5阶多项式去拟合这些数据。看起来，尽管拟合曲线完全通过了数据点，但它在对不同房屋面积预测房价方面的表现可能并不好。所以，记住了，过于贪心可不是好事哦！\n权重好啦，让我们稍微放慢脚步，先来简单介绍一下局部加权线性回归（LWR）算法吧！想象一下，当我们拥有足够多的训练数据时，特征的选择变得不那么重要了。这里，LWR算法就是我们的小助手，帮助我们更好地拟合数据。虽然我们只是稍微提一下，但是不要担心，在后面的作业中，你将有机会深入探索LWR算法的一些特性。\n在传统的线性回归算法中，要在查询点处进行预测（也就是评估），我们会按照以下步骤进行：\n\n通过最小化来拟合参数。\n将作为我们的预测结果输出。\n\n然而，在局部加权线性回归算法中，我们将迈出一小步，踏入了一个可爱而神奇的世界。请跟紧我，我们一起看看LWR算法的魔法步骤：\n\n通过最小化来拟合参数。\n将作为我们的预测结果输出。\n\n在这里，是非负权重值。直观地说，如果对于特定的，很大，那么在选择时，我们会努力使尽可能小。如果很小，那么在拟合中，的误差项将被忽略。\n那么，如何确定这些神奇的权重呢？一个非常常见的权重选择方式是：\n\n\n\n\n\n\n\n\n\n如果x是向量值的，这个公式可以推广为或者，其中或的选择适当。\n请注意，权重值取决于我们希望预测的特定点。而且，如果很小，那么会接近1；如果很大，那么就会很小。所以，我们可以说，对于与查询点相近的训练样本，我们赋予它们更高的“权重”，而离查询点更远的样本则具有较低的“权重”。真是一种魔法般的思想！\n还要注意的是，虽然权重公式在形式上类似于高斯分布的密度函数，但与高斯分布并没有直接的关联，特别是不是一个随机变量，无论它是否服从正态分布或其他分布。（这是一种可爱的小魔法，不是吗？）\n带宽参数控制着训练样本权重随其与查询点之间距离的衰减速度。你知道吗？这个就是我们所说的带宽参数，它也是你在后面任务中将要实验的内容之一。它决定了魔法的强弱程度！\n现在，我们已经见识了第一个非参数算法的例子，它就是局部加权线性回归。相比之下，之前我们接触的（无权重的）线性回归算法被称为参数学习算法，因为它具有一组固定的有限参数（），这些参数与数据进行拟合。一旦我们拟合了参数并将它们存储起来，我们就不再需要保留训练数据来进行未来的预测。\n但是，和LWR不同哦！为了使用局部加权线性回归进行预测，我们需要保留整个训练集。这就是所谓的非参数，因为为了表示假设，我们需要存储与训练集规模成线性增长的信息量。\n特点LWR算法的魔力之一在于它的灵活性和适应性。它不仅仅是一个固定的模型，而是根据数据的特点和查询点的需求，为每个预测问题个性化地构建模型。这就像我们有一位聪明的小精灵，他根据数据的不同部分，以及与查询点的距离，灵活地调整预测模型，以获得最佳的结果。\n嗯，你或许会问，为什么要引入这种灵活性呢？这是因为在现实世界中，我们经常面对各种各样的数据模式和问题。有时，数据可能在某个区域内呈现出线性关系，而在另一个区域则呈现出非线性关系。有时，我们对于某些区域的预测更加关注，希望能够更准确地捕捉到那些重要的点。这时，LWR算法就展现出了它的独特优势。\n让我们回到我们的探险之旅。在LWR算法中，带宽参数起着至关重要的作用。这个参数控制着样本权重随其与查询点之间距离的衰减速度。想象一下，如果我们选择一个较小的，那么只有离查询点非常近的训练样本才会受到较高的权重，而离查询点较远的样本则会受到较低的权重。这就好像我们的小精灵更注重那些距离查询点更近的样本，认为它们对于预测更加重要。\n另一方面，如果我们选择一个较大的，那么样本的权重衰减速度将变得较慢，离查询点较远的样本仍然会有较高的权重。这就好像我们的小精灵更加关注整体的数据趋势，不过仍然保留了远离查询点的样本的影响。\n嘿，还记得我们之前提到的权重选择公式吗？，这是一个非常常见的选择。但也请注意，这只是一个示例，实际上还有其他的权重选择方式，可以根据具体问题进行调整和探索。\n我知道你一定迫不及待地想要实践一下LWR算法了。在接下来的作业中，你将有机会亲自尝试不同的带宽参数，并观察它对预测结果的影响。这将帮助你更好地理解LWR算法的运作原理和特性。\nLWR算法步骤LWR算法的执行包括以下步骤：\n\n拟合参数θ：通过最小化加权误差来拟合参数。这里，是一个非负的权重值，用于调整每个训练样本在拟合过程中的重要性。\n预测：使用得到的参数对新的查询点进行预测。计算，将其作为我们的预测结果输出。\n\n在这里，我们将通过逐步展开这些步骤，深入了解每个步骤的细节和原理。\n拟合参数在LWR算法中，拟合参数的关键在于最小化加权误差。这里的权重决定了每个训练样本的重要性，进而影响参数的拟合过程。\n要拟合参数，我们需要选择合适的权重。在LWR中，一个常见的权重选择方式是使用高斯核函数，就是刚刚我们见到的：复习一下，在这个公式中，是第个训练样本的特征值，是查询点的特征值，是带宽参数。这个公式的作用是根据查询点与训练样本之间的距离，赋予每个训练样本一个权重值。距离越近的样本将具有较高的权重，对参数的拟合起到更大的影响；而距离较远的样本则具有较低的权重，对参数的拟合影响较小。\n这里，带宽参数起着关键的作用。它决定了权重随着距离的衰减速度。较小的会导致权重衰减较快，只有距离查询点较近的样本会对参数拟合产生较大的影响；而较大的会导致权重衰减较慢，距离查询点较远的样本仍然具有较高的权重。\n为了拟合参数，我们可以使用各种数值优化方法，如梯度下降或正规方程法。在优化过程中，我们使用加权误差作为目标函数，并根据权重对样本进行加权。\n预测一旦我们拟合了参数，我们就可以使用它来对新的查询点进行预测。预测的过程非常简单，我们只需计算，并将其作为我们的预测结果输出。\n这个预测过程可以看作是使用已经拟合好的参数对新的输入进行线性组合的过程。通过将参数与特征值进行线性组合，我们得到了我们对于新查询点的预测结果。\n通过拟合参数和预测过程，LWR算法为我们提供了一种灵活而强大的方式来适应不同的数据模式和预测需求。它允许我们个性化地构建模型，并根据查询点的特征和数据的分布情况进行个性化的预测。\n任务这个任务是我让GPT给出来的，因为毕竟在斯坦福原有的课程当中，这节课选修课，我也没找到作业在哪（其实是我懒）。。。TT\n任务1：调整带宽参数在这个作业中，你将尝试调整带宽参数，并观察其对LWR算法的影响。你可以选择一些不同的值，比如0.1、1、10，并进行如下实验：\n\n对于每个值，使用LWR算法进行参数拟合，并得到相应的参数。\n使用得到的参数对测试集中的样本进行预测，并计算预测结果的均方误差（Mean Squared Error）。\n观察不同值下的预测结果和均方误差之间的关系。\n\n通过这个作业，你将能够理解带宽参数对LWR算法的影响。较小的值会导致模型更关注查询点附近的训练样本，可能会出现过拟合的情况；而较大的值则会使模型更加关注整体的数据趋势，可能会出现欠拟合的情况。你可以通过观察均方误差来评估不同值下模型的性能。\n这个任务我写了个小程序，有python环境的可以尝试一下：\n\nClick to see more\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n\ndef compute_weights(X_train, x, tau):\n    # 计算权重值\n    weights = np.exp(-(np.linalg.norm(X_train - x, axis=1) ** 2) / (2 * tau ** 2))\n    return weights\n\ndef fit_lwr(X_train, y_train, x, tau):\n    # 拟合参数θ\n    X = np.hstack((np.ones((X_train.shape[0], 1)), X_train))  # 添加常数项\n    weights = compute_weights(X_train, x, tau)\n    W = np.diag(weights)\n    theta = np.linalg.inv(X.T @ W @ X) @ X.T @ W @ y_train\n    return theta\n\ndef predict_lwr(X_train, y_train, X_test, tau):\n    predictions = []\n    for x in X_test:\n        theta = fit_lwr(X_train, y_train, x, tau)\n        x = np.hstack(([1], x))  # 添加常数项\n        y_pred = theta.T @ x\n        predictions.append(y_pred)\n    return np.array(predictions)\n\n# 生成示例数据集\nX, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 设置不同的带宽参数\ntau_values = [0.1, 1, 10]\n\n# 绘制数据集和拟合结果的图像\nplt.figure(figsize=(12, 8))\n\nfor i, tau in enumerate(tau_values):\n    plt.subplot(2, 2, i+1)\n\n    # 绘制训练数据集\n    plt.scatter(X_train, y_train, color='b', label='Training Data')\n\n    # 拟合参数θ\n    theta = fit_lwr(X_train, y_train, X_train, tau)\n    print(f\"带宽参数τ = {tau}，拟合得到的参数θ：{theta}\")\n\n    # 预测并计算均方误差\n    y_pred = predict_lwr(X_train, y_train, X_test, tau)\n    mse = np.mean((y_pred - y_test) ** 2)\n    print(f\"带宽参数τ = {tau}，测试集上的均方误差：{mse}\\n\")\n\n    # 绘制拟合曲线\n    x_plot = np.linspace(np.min(X_train), np.max(X_train), 100)\n    X_plot = np.hstack((np.ones((x_plot.shape[0], 1)), x_plot.reshape(-1, 1)))\n    y_plot = X_plot @ theta\n    plt.plot(x_plot, y_plot, label=f\"τ = {tau}\")\n\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.legend()\n    plt.title(f'Local Weighted Linear Regression (τ = {tau})')\n\nplt.tight_layout()\nplt.show()\n\n\n这段代码首先使用make_regression函数生成一个示例数据集，（运用了种子来进行随机数据集，便于复现），然后使用train_test_split将数据集划分为训练集和测试集。\n然后，compute_weights函数用于计算权重值，fit_lwr函数用于拟合参数，predict_lwr函数用于进行预测。\n在主程序中，我们设置了不同的带宽参数值[0.1、1、10]，然后对每个参数值进行拟合和预测。最后，我们计算并输出每个参数值下测试集上的均方误差。\n使用了plt.subplot函数来创建一个2x2的子图布局，并在每个子图中绘制不同带宽参数下的拟合结果。每个子图显示了训练数据集的散点图和相应的拟合曲线。\n图像如下\n\n\n你可以根据实际情况修改代码，使用自己的数据集和问题进行实验。通过观察不同带宽参数下的拟合效果和预测误差，你将能够更好地理解带宽参数对LWR算法的影响。\n\n\n作业2：应用LWR算法解决实际问题在这个作业中，你将应用LWR算法解决一个实际的问题。你可以选择感兴趣的领域，如房价预测、销量预测等，并按照以下步骤进行：\n数据集可以用我们之前用过的房价数据集，我放到这里：\n\nClick to see more\n# 数据集\nareas = np.array([2104, 1600, 2400, 1416, 3000, 1985, 1534, 1427, 1380, 1494, 1940, 2000, 1890, 4478, 1268, 2300, 1760, 1450, 3100, 2250, 2132, 2596, 1850, 2680, 1956, 1604, 2020, 2730, 2008, 1537, 2500, 1560, 2120, 2200, 1638, 2540, 2200, 2070, 2005, 1900, 2380, 1320, 2190, 2340, 2678, 1966, 1570, 1852, 2454, 2205, 2450])\nbedrooms = np.array([3,3,3,2,4,4,3,3,3,3,4,3,3,5,3,4,3,3,4,4,4,3,4,4,3,3,4,4,3,3,4,4,4,3,3,4,3,4,3,3,4,2,4,3,4,4,3,3,4,4,4])\nprices = np.array([400, 330, 369, 232, 540, 320, 267, 199, 245, 347, 334, 383, 339, 699, 259, 410, 350, 315, 590, 410, 399, 459, 325, 480, 349, 285, 365, 525, 375, 295, 450, 280, 389, 425, 315, 475, 408, 382, 350, 380, 420, 230, 394, 416, 540, 385, 279, 360, 485, 405, 450])\n\n\n\n\n收集和准备数据集：选择一个合适的数据集，并对数据进行预处理，确保数据的质量和一致性。\n划分数据集：将数据集划分为训练集和测试集，用于参数拟合和模型评估。\n使用LWR算法进行参数拟合：根据训练集使用LWR算法拟合参数。\n模型评估：使用测试集进行模型评估，计算预测结果的均方误差或其他适当的评估指标。\n结果分析和改进：分析预测结果，并根据需要对模型进行改进和调整。\n\n通过完成这个作业，你将能够将LWR算法应用到实际问题中，并通过实践加深对算法的理解。你可以探索不同的数据集和问题领域，进一步挖掘LWR算法的潜力。\n","slug":"Locally-weighted-linear-regression-optional-reading","date":"2023-07-17T07:42:47.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"85691702e05d151e003d7396dcbea440","title":"概率解释*","content":"引言当涉及到机器学习算法时，有时我们不仅仅希望通过数学的角度来理解它们的原理，还希望探索它们的概率解释。在这个部分中，我们将探讨线性回归算法的概率解释。\n你有没有想过，为什么我们在回归问题中使用平方误差作为成本函数？为什么我们假设模型的预测值和真实值之间存在高斯分布的误差？通过概率解释，我们可以更深入地理解线性回归模型的工作原理，并从统计学的角度来看待它。\n当我们谈论概率解释时，让我们以一个简单易懂的例子来说明。假设你是一位面包师傅，你烘焙面包的时间和温度是你最关注的因素。你想要根据面包的温度来预测它的烘焙时间。\n现在，假设你已经收集了一些数据，包括了面包的温度和相应的烘焙时间。你想要建立一个模型，根据温度来预测烘焙时间。这就是一个典型的线性回归问题。\n现在，让我们用概率解释来理解这个问题。我们可以将线性回归模型看作是在给定面包温度的条件下，对烘焙时间的条件分布进行建模。换句话说，我们想要找到一个条件分布，它告诉我们在已知温度的情况下，烘焙时间可能的取值范围。\n通过概率解释，我们可以得到一个关键的洞察力：我们不仅仅是在寻找一个点估计，即给定温度预测单个烘焙时间，而是在寻找一个整个概率分布，它表示了在给定温度下烘焙时间的不确定性。\n这种概率解释对于面包师傅来说非常有用。它不仅告诉你在给定温度下的预测烘焙时间，还告诉你该预测的不确定性范围。这样，你就可以更加自信地做出决策，控制烘焙时间，确保面包在最佳状态下烘焙。\n所以，通过概率解释，线性回归模型不仅可以帮助你做出预测，还可以提供预测的不确定性估计，使你更加有信心地应对面包烘焙的挑战。这就是概率解释为我们带来的额外好处。\n概率解释在面对回归问题时，为什么线性回归，尤其是最小二乘成本函数 ，是一个合理的选择呢？让我们通过一组概率假设来解释这一点，根据这些假设，最小二乘回归可以被推导为一种非常自然的算法。\n我们假设目标变量和输入之间存在以下关系：其中  是一个误差项，用于捕捉未建模的影响（例如，我们没有在回归中包含与预测房价高度相关的某些特征）或者随机噪声。我们进一步假设  是独立同分布（IID）地遵循均值为零、方差为  的高斯分布（也称为正态分布）。我们可以表示这个假设为 “”，即  的概率密度函数为：\n\n\n\n\n\n\n\n\n\n在机器学习和数学中，exp 表示指数函数，即自然指数函数，记作 ，其中  是自然对数的底数（欧拉数，约等于2.71828）。指数函数是一个常用的数学函数，它的定义是将底数  提升到给定的指数值。\n在上述笔记中，exp 是将指数函数应用于括号内的表达式。例如，在计算高斯分布的概率密度函数时，指数函数被用于计算  这个指数部分。\n指数函数具有将非负实数映射到正实数的特性，它在概率密度函数和其他数学模型中经常被使用，例如高斯分布、指数分布等。\n因此，我们可以表示给定  和由参数  控制的  的分布为：这表示了  在给定  的条件下，根据参数  的分布。需要注意的是，我们不应将  视为条件概率  的一部分，因为  不是一个随机变量。我们还可以将  的分布写为 。\n在给定 （设计矩阵，包含所有 ）和  的情况下， 的整体分布是什么样的？数据的概率由  给出。这个量通常被视为 （以及可能的 ）的函数，对于固定的  值。当我们希望明确将其视为  的函数时，我们将其称为似然函数：根据对 （从而对给定的  和 ）的独立性假设，我们还可以将其写成如下形式：现在，考虑到这个将  和  关联的概率模型，我们应该以何种方式合理地选择参数  来获得最佳估计？最大似然原理指出，我们应该选择能够使数据的概率最大化的 。换句话说，我们应该选择能够最大化  的 。\n与最大化  等价的是最大化  的任何严格递增函数。特别地，如果我们转而最大化对数似然函数 ，推导会更加简单：：因此，最大化  得到与最小化相同的结果：我们将这个结果记为 ，也就是最初的最小二乘代价函数。\n总结一下，在数据的先验概率假设下，最小二乘回归对应于找到参数  的最大似然估计。因此，在这种假设下，最小二乘回归可以被认为是一种非常自然的方法，它只是在进行最大似然估计而已。（需要注意的是，概率假设并不是使得最小二乘成为完全合理和恰当的过程所必需的，还可能存在其他自然的假设可以用来证明它的合理性。）\n此外，根据我们之前的讨论，我们的最终  的选择并不依赖于  的值，实际上，即使  是未知的，我们也会得到相同的结果。当我们讨论指数族和广义线性模型时，我们将再次利用这一事实。\t\n举例说明假设你正在研究房屋价格与其尺寸之间的关系。你收集了一些房屋的尺寸和实际售价的数据，并希望使用机器学习来构建一个预测模型。在这种情况下，你可以使用最小二乘回归作为你的算法，并通过概率解释来理解它的合理性。\n根据概率解释，我们假设房屋价格与尺寸之间的关系可以通过以下方程表示：其中  是第  个房屋的实际售价， 是对应的尺寸特征， 是我们要学习的参数，而  则表示误差项。\n根据概率假设，我们假设误差项  是从均值为零、方差为  的高斯分布中独立同分布地抽样得到的。这意味着我们认为误差是随机的，并且服从正态分布。这个假设可以让我们使用概率密度函数来描述误差项的分布情况。\n现在，我们可以根据这个概率模型来定义给定  的条件下， 的分布：这表示给定  的情况下， 的分布是一个以  为均值、 为方差的高斯分布。\n我们的目标是找到使得观测到的数据的概率最大化的参数 。也就是说，我们希望选择参数 ，使得数据的整体概率  最大化。\n根据最大似然估计的原理，我们可以通过最大化似然函数  或对数似然函数  来获得最佳参数估计。在最小二乘回归中，我们常使用对数似然函数来简化推导。\n通过计算对数似然函数 ，我们可以将最大似然估计转化为最小化残差平方和的问题，即最小化代价函数 ：这刚好是最小二乘回归中使用的成本函数。\n因此，根据概率解释，最小化最小二乘成本函数  相当于在给定概率模型的前提下，寻找参数  的最大似然估计。这说明了最小二乘回归的合理性和自然性。\n补充当使用最小二乘法进行机器学习中的回归任务时，并不需要对概率解释的细节了如指掌。嗯，你没听错，概率解释并不是机器学习中的“铁板钉钉”。\n实际上，对于许多问题来说，最小二乘法就足够了，它能够给出很好的结果，而不需要我们费神去琢磨概率解释的种种细节。毕竟，最小二乘法是机器学习中非常常见且有效的方法。\n但是，不可否认的是，在某些情况下，了解概率解释和统计学原理是很有帮助的。特别是当你处理带有噪声的数据时，概率解释可以告诉你模型的不确定性和置信度。这样一来，你就能更好地理解和解释你的模型结果了。\n总而言之，了解最小二乘法是机器学习中的重点，而对概率解释的深入探究则是可选的。如果你只是想简单地应用最小二乘法来完成回归任务，那了解一些基本的原理和实现就足够了。当然，如果你对概率解释和统计学概念感兴趣，那么对于某些特定问题来说，深入研究概率解释会给你带来更深入的理解和分析能力。\n","slug":"Probabilistic-interpretation","date":"2023-07-17T06:02:56.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"8bd50a6b3cea7ecea2a8063e70b7409b","title":"正规方程","content":"引入在机器学习和线性回归中，我们经常需要通过训练数据来学习参数，以便建立一个能够准确预测目标变量的模型。前面我们已经介绍了梯度下降法，这是一种常用的优化算法，可以帮助我们找到最小化成本函数的参数值。\n除了梯度下降法，还有一种有趣而简洁的方法可以解决线性回归问题，它被称为正规方程（The Normal Equations）。正规方程提供了一种通过代数方法直接求解最优参数的方式，而不需要像梯度下降法那样迭代更新。\n让我们以一个生动的例子来理解正规方程。假设你是一名厨师，你想要制作一道美味的蛋糕。你知道蛋糕的味道取决于配料的种类和数量。你希望找到一个最佳的配料组合，使得蛋糕的口感和味道达到最佳。\n为了解决这个问题，你决定进行一系列实验。你准备了不同数量和种类的配料，并且每次制作蛋糕后，让一群品尝师评价蛋糕的口感。你记录下每个实验中使用的配料种类和数量，以及对应的评分。\n现在，你的目标是通过这些实验数据，找到一种最佳的配料组合，以获得最佳的蛋糕口感。你想要建立一个线性模型，通过配料的数量和种类预测口感评分。这个问题就可以转化为一个线性回归问题。\n正规方程提供了一种直接求解线性回归参数的方法。它的原理类似于代数方程的求解过程。通过对训练数据进行数学运算，我们可以得到一个公式，可以直接计算出最优的参数值。\n正规方程不需要像梯度下降法那样进行迭代更新，因此在某些情况下，它可能更加高效。然而，正规方程也有一些限制，例如当特征数量非常大时，计算复杂度会增加。\n在接下来的部分，我们将详细介绍正规方程的原理和应用。正规方程为我们提供了一种有趣而直接的方式来解决线性回归问题，让我们一起探索吧！\n正规方程正规方程是一种通过代数方法直接求解线性回归参数的方法，而不需要像梯度下降法那样进行迭代更新。它的原理是通过最小化成本函数，找到使得预测值与实际值之间差异最小的参数值。\n为了理解正规方程的原理，让我们再回到蛋糕制作的例子。你已经进行了一系列实验，记录了不同配料组合的口感评分。现在，你想要找到最佳的配料组合，使得蛋糕的口感评分最高。\n回忆一下线性回归模型的表示形式：。我们的目标是找到一组最优的参数，使得尽可能接近实际的口感评分。\n我们定义成本函数来衡量预测值与实际值之间的差异。对于线性回归问题，我们通常使用平方差误差（SSE）作为成本函数，即，其中是训练样本的数量。\n现在，我们的目标是找到最优的参数，使得成本函数最小化。而正规方程就提供了一种求解最优参数的解析解。具体来说，我们通过对关于参数的导数进行求解，并将其设置为零来最小化。\n在进行矩阵表示时，我们将训练样本的特征向量表示为矩阵，其中每一行代表一个样本的特征，每一列代表一个特征维度。类似地，我们将实际值表示为向量。那么，线性回归模型可以写成矩阵形式：。\n应用矩阵微积分的概念，我们可以求解成本函数关于参数的导数。这个导数称为梯度（gradient），用表示。当梯度为零时，我们得到正规方程的解。\n通过代数计算，我们可以得到正规方程的表达式：。这个表达式直接给出了最优参数的解析解。\n正规方程的优点在于它不需要进行迭代更新，可以直接得到最优参数的解析解。然而，它的计算复杂度取决于特征的数量，当特征数量非常大时，求解逆矩阵的计算可能变得耗时。\n在实际应用中，我们可以根据问题的特点选择使用梯度下降法还是正规方程。梯度下降法适用于大规模数据集和高维特征空间，而正规方程适用于小规模数据集和低维特征空间。\n矩阵导数矩阵导数是矩阵微积分中的重要概念，用于描述函数对矩阵变量的导数。在矩阵导数中，我们将函数从一个行列的矩阵映射到实数，定义为。为了求解矩阵导数，我们需要计算函数相对于矩阵的偏导数。矩阵导数本身也是一个行列的矩阵，其中元素表示函数对的偏导数。\n例如，假设是一个2行2列的矩阵，函数：定义为：在这个例子中，表示矩阵的元素。\n我们可以通过计算偏导数来得到矩阵导数的表达式。根据定义，我们计算对每个的偏导数，然后将它们组合成矩阵的形式。对于我们的例子，我们可以得到如下结果：这个结果展示了矩阵导数的计算方式。每个元素都是相应偏导数的结果。例如，元素是对的偏导数，元素是对的偏导数，以此类推。\n最小二乘法再探讨借助矩阵导数的工具，现在让我们通过闭式解来找到使最小化的的值。我们首先将用矩阵-向量表示法重新书写。\n给定一个训练集，定义设计矩阵为一个行列的矩阵（实际上是行列，如果我们包括截距项），其行包含训练示例的输入值：此外，让是一个维向量，包含来自训练集的所有目标值：现在，由于，我们可以很容易地验证：因此，利用向量的性质，我们有 ：最后，为了最小化，让我们找到它相对于的导数。因此，我们有：在第三步中，我们使用了的事实，在第五步中使用了和的事实，其中是对称矩阵。为了最小化，我们将其导数设为零，得到正规方程：因此，最小化的值可以通过以下方程的闭式解给出：\n\n\n\n\n\n\n\n\n\n请注意，在上述步骤中，我们隐含地假设是可逆矩阵。在计算逆矩阵之前，可以进行检查。如果线性无关的样本数量少于特征数量，或者特征不是线性无关的，则将不可逆。即使在这种情况下，也有可能通过额外的技术来“修正”这种情况，但为了简洁起见，我们在这里省略了这些内容。\n举例我们再回到最开始那个做烘焙的例子，假设你是一位厨师，想要研究面包的烘焙时间和温度之间的关系。你收集了一系列实验数据，记录下了烘焙时间和使用的温度。现在，你想要找到一个数学模型来预测未来的烘焙时间。这时，线性回归和最小二乘法就能派上用场了。\n现在，让我们使用矩阵导数的工具来重新表达成本函数。首先，我们定义设计矩阵，它是一个行列的矩阵，其中每一行包含一个实验样本的特征值（在我们的例子中就是温度）。我们还定义目标向量，它是一个维向量，包含对应每个实验样本的烘焙时间。\n我们收集了一些数据，记录了不同温度下烘焙面包所需的时间。现在我们要使用这些数据来训练一个线性回归模型，以便我们可以根据温度来预测烘焙时间。\n首先，让我们创建一个虚构的数据集（这是我用程序随机生成的）。假设我们有以下数据：\n\nClick to see more\n\n\n\n温度（摄氏度）\n烘焙时间（分钟）\n\n\n\n168.7\n41.4\n\n\n197.5\n52.2\n\n\n186.6\n45.0\n\n\n179.9\n43.7\n\n\n157.8\n42.8\n\n\n157.8\n32.0\n\n\n152.9\n31.9\n\n\n193.3\n45.8\n\n\n180.1\n41.0\n\n\n185.4\n48.7\n\n\n\n\n\n\n\n\n现在，我们将数据表示为输入特征矩阵和目标变量向量。是一个包含温度特征的矩阵，是对应的烘焙时间。\n接下来，我们将为矩阵添加截距项列。这样，矩阵的第一列将始终为1，以表示截距(intercept)。\n现在，我们可以使用正规方程来求解参数。正规方程的公式为：\n这个公式会给出使得模型最优拟合数据的参数值。\n控制台会输出：\nIntercept: -19.32\nSlope: 0.35\n\n\n\n\n\n为了应用这个公式，我们需要使用Python进行计算。我们可以使用NumPy库来进行矩阵运算。让我们来看看如何在Python中计算正规方程的闭式解：\n\nClick to see more\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)  # 设置随机种子，以便结果可复现\n\n# 创建输入特征 X（温度）和目标变量 y（烘焙时间）\nX = np.random.uniform(150, 200, 10).reshape(-1, 1)  # 温度范围在150到200之间\ny = 10 + 0.2 * X + np.random.normal(0, 5, 10).reshape(-1, 1)  # 烘焙时间=10 + 0.2*温度 + 噪声\n\n# 添加截距项列（全为1）到 X 矩阵中\nX = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n\n# 计算参数 theta\ntheta = np.linalg.inv(X.T @ X) @ X.T @ y\n\nintercept = theta[0][0]\nslope = theta[1][0]\nprint(f\"Intercept: {intercept:.2f}\")\nprint(f\"Slope: {slope:.2f}\")\n\n\n# 绘制数据点\nplt.scatter(X[:, 1], y, label=\"Data\")\n\n# 绘制拟合的线性回归模型\nx_line = np.linspace(150, 200, 100)\ny_line = intercept + slope * x_line\nplt.plot(x_line, y_line, color=\"red\", label=\"Linear Regression\")\n\n# 添加标签和图例\nplt.xlabel(\"Temperature\")\nplt.ylabel(\"Baking Time\")\nplt.legend()\n\n# 显示图形\nplt.show()\n\n\n\n\n运行这段代码，我们会得到参数的值。在这个例子中，我们会得到两个参数：截距项参数和温度参数。这些参数表示烘焙时间与温度之间的线性关系。\n一旦我们求解出参数，我们就可以使用它来进行预测。给定一个新的温度值，我们可以通过计算来预测对应的烘焙时间。\n这节的内容会比较难以理解，希望可以反复阅读，如有疑问欢迎提出，说实话，我都有些没看懂。\n","slug":"The-normal-equations","date":"2023-07-17T04:09:05.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"eee27f691084ae51bee755f5fe41f548","title":"LMS算法","content":"欢迎来到新的一部分！现在我们将介绍一种非常有趣的算法，它被称为最小均方（Least Mean Squares，LMS）算法。这是一种用于优化线性回归模型的算法，它可以帮助我们找到最佳的参数组合，使得我们的预测结果与实际观测值之间的差异最小化。\nLMS算法实际上是一个非常聪明的算法，它的灵感来自于我们人类在学习过程中的一种思维方式。想象一下，当你在学习骑自行车或者学习弹吉他时，你并不会一次就掌握所有技巧。相反，你会不断地试验、调整和改进，直到你的动作越来越接近完美。\nLMS算法的原理也是类似的。它通过逐步调整模型的参数来最小化成本函数，就像我们逐步调整我们的动作来提高技能一样。在每一步中，LMS算法会计算出当前参数设置下的成本函数值，并根据这个值来调整参数，以便使下一次迭代的预测结果更接近实际观测值。这个过程就像是在不断地微调模型，让它的预测能够更准确地拟合实际数据。\n一个有趣的比喻是，想象你是一名音乐家，正在调音吉他。你会先弹奏一根弦，然后通过调整琴弦的张力来使音高趋近于理想的音高。LMS算法的工作方式类似于这个过程，它会在每一步中微调模型的参数，使得预测结果逐渐接近实际观测值，就像音高逐渐趋近理想音高一样。\nLMS算法是一种非常强大和常用的优化算法，特别适用于解决线性回归问题。它不仅可以用于预测骑行时间，还可以应用于各种其他领域，如金融、医疗和天气预测等。通过不断地微调参数，LMS算法帮助我们找到最佳的模型参数，使我们的预测结果更加准确和可靠。\nLMS算法我们继续探讨LMS算法！让我们首先回顾一下我们的目标：选择合适的参数来最小化成本函数。为了实现这一目标，我们需要一个搜索算法，该算法从一个”初始猜测”开始选择，然后通过迭代改变以使逐渐变小，直到我们收敛到使最小化的值。\n\n\n\n\n\n\n\n\n\n我们使用符号“a := b”来表示一种操作（在计算机程序中），该操作将变量a的值设置为等于变量b的值。换句话说，这个操作会用b的值覆盖a的值。相反，当我们在断言一个事实时，即a的值等于b的值，我们会写成“a = b”。\n那么，我们如何在每次迭代中更新参数呢？这就是LMS算法的精髓所在。它使用一种称为梯度下降的方法，以最陡的下降方向更新参数。具体而言，在每次迭代中，我们将参数更新为，其中是学习率。\n为了更好地理解LMS算法的更新规则，让我们以一个简单的例子来说明。假设我们只有一个训练样本，其中是我们的输入特征，是对应的目标值。我们的目标是根据输入预测出目标值。我们将使用线性模型来进行预测。\n这是一个非常自然的算法，它重复地朝着最陡的下降方向迈出一步。为了实现这个算法，我们需要计算出右侧的偏导数项。让我们首先计算出在只有一个训练样本的情况下的结果，这样我们就可以忽略的定义中的求和符号。为了更新参数和，我们需要计算成本函数对于每个参数的偏导数。通过计算，我们得到偏导数的表达式为：。这表明参数的更新量与误差项成比例，以及输入特征。根据这个结果，我们可以得到LMS算法的更新规则：这个规则被称为LMS更新规则（LMS代表“最小均方差”），也被称为Widrow-Hoff学习规则。这个规则具有几个看起来自然而直观的特性。例如，更新的幅度与误差项成比例；因此，例如，如果我们遇到一个训练样本，我们的预测几乎与的实际值相匹配，那么我们发现几乎不需要改变参数；相反，如果我们的预测与有较大的误差（即相距较远），那么参数将会有较大的变化。\n梯度下降法当我们使用机器学习算法解决问题时，经常需要最小化一个函数。梯度下降法（Gradient Descent）是一种常用的优化算法，用于找到函数的最小值。\n让我们通过一个生动的例子来解释梯度下降法。假设你是一位登山爱好者，目标是从山顶下到山脚的最短路径。你置身于山顶，但是你没有任何线索告诉你应该往哪个方向走。你面前一片云雾茫茫，你看不到山脚。然而，你手上有一个高度计可以告诉你当前的海拔高度。\n你的目标是找到一条最快的下山路径。你知道山的地形图呈现出一种斜率或坡度。你也知道下山的最陡峭方向就是当前位置的负梯度方向。\n梯度下降法的思想类似于登山者的行动。你观察当前位置的海拔高度，并朝着最陡峭的下坡方向迈出一步。然后，你再次观察新位置的海拔高度，继续朝着最陡峭的下坡方向迈出一步。你不断重复这个过程，逐步接近山脚。\n在数学中，我们使用类似的思想来最小化函数。假设我们有一个函数，我们希望找到它的最小值。梯度下降法通过迭代地计算函数的梯度，沿着梯度的反方向更新参数，以使函数的值逐渐减小。\n函数的梯度是一个向量，指示函数在给定点上最陡峭的上升方向。我们的目标是朝着最陡峭的下降方向前进，因此我们朝着梯度的反方向更新参数。\n\n\n\n\n\n\nTIP\n这个过程可以表示为以下步骤：\n\n初始化参数：选择一个初始的参数值作为起点。\n计算梯度：计算函数在当前参数值处的梯度。梯度告诉我们函数在该点上升的方向和速度。\n更新参数：根据梯度的方向和一个称为学习率的调整参数，更新参数值。\n重复步骤2和步骤3，直到达到停止条件。停止条件可以是达到最大迭代次数、参数变化很小或函数值达到某个阈值。\n\n\n\n通过迭代更新参数，梯度下降法能够找到函数的局部最小值或全局最小值，这取决于函数的性质和初始参数的选择。\n梯度下降法在机器学习中扮演重要角色，它是许多算法的基础，例如线性回归、逻辑回归和神经网络。它允许我们通过最小化成本函数来调整模型的参数，使模型更好地拟合训练数据。\n批量梯度下降我们已经推导出了适用于单个训练样本的LMS更新规则。但是，对于包含多个训练样本的训练集，我们可以通过将坐标更新组合成向量形式来简化更新规则：\n​\t\t\t\t重复直到收敛 {}\n通过将坐标的更新组合成向量θ的更新，我们可以以更简洁的方式重新编写更新式（1.1）：这种形式的更新规则被称为批量梯度下降，因为它在每次迭代中考虑整个训练集。需要注意的是，对于线性回归的优化问题，由于成本函数是一个凸二次函数，不存在局部最小值，只有一个全局最小值。因此，梯度下降算法总是会收敛到全局最小值（前提是学习率不要设置得太大）。\n\n\n当然python的代码如下，你可以自己尝试\n\nClick to see more\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 定义二次函数\ndef quadratic_function(x, y):\n    return x**2 + y**2\n\n# 定义二次函数的偏导数\ndef gradient(x, y):\n    return np.array([2*x, 2*y])\n\n# 定义梯度下降函数\ndef gradient_descent(gradient, initial_point, learning_rate, num_iterations):\n    path = [initial_point]\n    point = initial_point\n\n    for _ in range(num_iterations):\n        grad = gradient(*point)\n        point = point - learning_rate * grad\n        path.append(point)\n\n    return np.array(path)\n\n# 定义绘制等高线图的函数\ndef plot_contour(func):\n    x = np.linspace(-5, 5, 100)\n    y = np.linspace(-5, 5, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = func(X, Y)\n\n    plt.figure(figsize=(8, 6))\n    plt.contour(X, Y, Z, levels=20)\n    plt.colorbar()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Contour Plot')\n    plt.grid(True)\n\n# 设置初始点、学习率和迭代次数\ninitial_point = np.array([3.0, 4.0])\nlearning_rate = 0.1\nnum_iterations = 20\n\n# 运行梯度下降算法\npath = gradient_descent(gradient, initial_point, learning_rate, num_iterations)\n\n# 绘制等高线图和路径\nplot_contour(quadratic_function)\nplt.plot(path[:, 0], path[:, 1], '-ro')\nplt.show()\n\n\n\n首先，图像展示了一个二维平面，其中轴和轴代表二次函数的输入变量的取值范围。二次函数由公式 给出，其中等高线表示函数值相等的点。等高线的形状呈现出圆形，因为函数是关于和的平方和的形式。\n其次，等高线的颜色表示函数值的大小。颜色条(colorbar)位于图像右侧，它显示了颜色与函数值的对应关系。颜色越深表示函数值越小，而颜色越浅表示函数值越大。在这个示例中，我们选择了20个等高线水平线，因此你可以看到等高线从内部圆开始，逐渐向外部圆扩展。\n接下来，红色的路径表示梯度下降算法的路径。我们使用初始点 (3.0, 4.0)作为起始点，并选择学习率为0.1，执行了20次迭代。梯度下降算法根据当前点的梯度信息来更新下一个点的位置，直到达到指定的迭代次数。红色路径显示了从初始点开始，沿着梯度下降方向逐步更新点的位置的过程。你可以看到路径开始在较陡峭的地方，然后逐渐向梯度变小的区域移动，最终趋近于函数的最小值(0, 0)。\n面积与房价我们继续研究梯度下降算法的不同变体！让我们先来看看批量梯度下降算法在拟合房屋价格预测模型时得到的结果。根据我们之前的数据集，通过运行批量梯度下降，我们得到了参数值和，它们可以用于构建线性模型来预测房屋价格，其中表示房屋的居住面积。如果我们绘制作为（面积）的函数，并结合训练数据，我们会得到上面的图表。这个模型给出了一个大致符合数据趋势的直线。\n\n\n当然，代码如下\n\nClick to see more\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 数据集\nareas = np.array([2104, 1600, 2400, 1416, 3000, 1985, 1534, 1427, 1380, 1494, 1940, 2000, 1890, 4478, 1268, 2300, 1760, 1450, 3100, 2250, 2132, 2596, 1850, 2680, 1956, 1604, 2020, 2730, 2008, 1537, 2500, 1560, 2120, 2200, 1638, 2540, 2200, 2070, 2005, 1900, 2380, 1320, 2190, 2340, 2678, 1966, 1570, 1852, 2454, 2205, 2450])\nprices = np.array([400, 330, 369, 232, 540, 320, 267, 199, 245, 347, 334, 383, 339, 699, 259, 410, 350, 315, 590, 410, 399, 459, 325, 480, 349, 285, 365, 525, 375, 295, 450, 280, 389, 425, 315, 475, 408, 382, 350, 380, 420, 230, 394, 416, 540, 385, 279, 360, 485, 405, 450])\n\n# 数据归一化（特征缩放）\nareas_normalized = (areas - np.mean(areas)) / np.std(areas)\nprices_normalized = (prices - np.mean(prices)) / np.std(prices)\n\n# 添加偏置项\nX = np.column_stack((np.ones(len(areas_normalized)), areas_normalized))\n\n# 初始化参数\ntheta = np.zeros(2)\nalpha = 0.01\niterations = 1500\n\n# 定义代价函数\ndef compute_cost(X, y, theta):\n    m = len(y)\n    h = np.dot(X, theta)\n    J = (1 / (2 * m)) * np.sum((h - y) ** 2)\n    return J\n\n# 批量梯度下降\ndef gradient_descent(X, y, theta, alpha, iterations):\n    m = len(y)\n    J_history = []\n    for _ in range(iterations):\n        h = np.dot(X, theta)\n        theta = theta - (alpha / m) * np.dot(X.T, h - y)\n        cost = compute_cost(X, y, theta)\n        J_history.append(cost)\n    return theta, J_history\n\n# 运行批量梯度下降算法\ntheta, J_history = gradient_descent(X, prices_normalized, theta, alpha, iterations)\n\n# 绘制拟合曲线\nx_values = np.linspace(-2, 2, 100)\ny_values = theta[0] + theta[1] * x_values\n\n\n# 绘制数据点和拟合曲线\nplt.scatter(areas_normalized, prices_normalized, label='Training Data')\nplt.plot(x_values, y_values, color='red', label='Linear Regression')\nplt.xlabel('Normalized Area')\nplt.ylabel('Normalized Price')\nplt.title('House Prices vs. Area (Linear Regression)')\nplt.legend()\nplt.show()\n\n\n\n\n继续考虑卧室数量接下来，如果我们考虑卧室数量作为输入特征之一，我们可以使用批量梯度下降来拟合模型并得到参数值，，。通过将卧室数量纳入考虑，我们可以构建一个更复杂的模型来预测房屋价格。\n\n\n代码如下\n\nClick to see more\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# 数据集\nareas = np.array([2104, 1600, 2400, 1416, 3000, 1985, 1534, 1427, 1380, 1494, 1940, 2000, 1890, 4478, 1268, 2300, 1760, 1450, 3100, 2250, 2132, 2596, 1850, 2680, 1956, 1604, 2020, 2730, 2008, 1537, 2500, 1560, 2120, 2200, 1638, 2540, 2200, 2070, 2005, 1900, 2380, 1320, 2190, 2340, 2678, 1966, 1570, 1852, 2454, 2205, 2450])\nbedrooms = np.array([3,3,3,2,4,4,3,3,3,3,4,3,3,5,3,4,3,3,4,4,4,3,4,4,3,3,4,4,3,3,4,4,4,3,3,4,3,4,3,3,4,2,4,3,4,4,3,3,4,4,4])\nprices = np.array([400, 330, 369, 232, 540, 320, 267, 199, 245, 347, 334, 383, 339, 699, 259, 410, 350, 315, 590, 410, 399, 459, 325, 480, 349, 285, 365, 525, 375, 295, 450, 280, 389, 425, 315, 475, 408, 382, 350, 380, 420, 230, 394, 416, 540, 385, 279, 360, 485, 405, 450])\n\n# 特征缩放（归一化）\nareas_normalized = (areas - np.mean(areas)) / np.std(areas)\nbedrooms_normalized = (bedrooms - np.mean(bedrooms)) / np.std(bedrooms)\nprices_normalized = (prices - np.mean(prices)) / np.std(prices)\n\n# 构建设计矩阵\nX = np.column_stack((np.ones(len(areas_normalized)), areas_normalized, bedrooms_normalized))\n\n# 初始化参数\ntheta = np.zeros(3)\nalpha = 0.01\niterations = 1500\n\n# 定义代价函数\ndef compute_cost(X, y, theta):\n    m = len(y)\n    h = np.dot(X, theta)\n    J = (1 / (2 * m)) * np.sum((h - y) ** 2)\n    return J\n\n# 批量梯度下降\ndef gradient_descent(X, y, theta, alpha, iterations):\n    m = len(y)\n    J_history = []\n    for _ in range(iterations):\n        h = np.dot(X, theta)\n        theta = theta - (alpha / m) * np.dot(X.T, h - y)\n        cost = compute_cost(X, y, theta)\n        J_history.append(cost)\n    return theta, J_history\n\n# 运行批量梯度下降算法\ntheta, J_history = gradient_descent(X, prices_normalized, theta, alpha, iterations)\n\n# 输出最终回归表达式\ntheta0 = theta[0]\ntheta1 = theta[1]\ntheta2 = theta[2]\nprint(f\"最终回归表达式: hθ(x) = {theta0:.2f} + {theta1:.2f}x1 + {theta2:.2f}x2\")\n\n# 绘制拟合曲面\nx1_values = np.linspace(-2, 2, 100)\nx2_values = np.linspace(-2, 2, 100)\nx1, x2 = np.meshgrid(x1_values, x2_values)\ny_values = theta0 + theta1 * x1 + theta2 * x2\n\n# 绘制数据点和拟合曲面\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(areas_normalized, bedrooms_normalized, prices_normalized, c='b', marker='o')\nax.plot_surface(x1, x2, y_values, color='r', alpha=0.5)\nax.set_xlabel('Normalized Area')\nax.set_ylabel('Normalized Bedrooms')\nax.set_zlabel('Normalized Price')\nax.set_title('House Prices vs. Area and Bedrooms (Linear Regression)')\nplt.show()\n\n\n\n\n最终控制台会输出\n最终回归表达式: hθ(x) = 0.00 + 0.93x1 + 0.04x2\n\n\n\n你可以看到，通过添加更多的特征，我们可以得到更准确的房屋价格预测模型。这说明特征选择对于机器学习模型的性能至关重要。\n随机梯度下降除了批量梯度下降，还有一种非常有效的替代方法，它被称为随机梯度下降。这种方法的算法如下：\nLoop{    for  to ,{\n​\t\t\t\t\t\t​\t\t}\n}\n通过将坐标的更新组合成向量θ的更新，我们可以以更简洁的方式重新编写更新式:在这个算法中，我们遍历整个训练集，并且在每次遇到一个训练样本时，只使用该样本的误差梯度来更新参数。这个算法被称为随机梯度下降或增量梯度下降。与批量梯度下降不同的是，随机梯度下降可以立即开始取得进展，并且在每个样本上都继续进行更新。这使得随机梯度下降比批量梯度下降更快地将参数接近最小值。然而，需要注意的是，随机梯度下降可能无法收敛到真正的最小值，参数会在最小值周围波动，但是在实践中，这些接近最小值的参数值通常是对真实最小值的合理近似。因此，尤其是在训练集很大的情况下，随机梯度下降比批量梯度下降更受欢迎。\n通过这些不同的梯度下降算法，我们可以有效地拟合参数，从而得到准确的预测模型。在接下来的部分，我将继续更新我在机器学习部分自学的笔记，希望各位可以一起交流！\n","slug":"LMS","date":"2023-07-16T09:18:07.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"b6c077dbe8f34883367bdebb8559e973","title":"探索机器学习的魅力：从斯坦福大学CS229课程开始","content":"机器学习是当今科技领域的热门话题，而斯坦福大学的CS229课程则是深入学习机器学习的绝佳门户。这篇文章将带您踏上机器学习的征程，通过探索CS229课程中的第一个主题——监督学习，揭开这个令人着迷的领域的神秘面纱。无论您是初学者还是有一定经验的机器学习从业者，本文将带您以生动活泼且专业的方式了解监督学习的基本原理、应用领域以及最新的研究动态。\n（当然我也是小白，这只是我在学习过程当中的笔记 &gt;_&lt;）\n监督学习：预测房价的例子让我们从几个监督学习问题的例子入手。假设我们有一个数据集，其中包含了50栋房屋的居住面积和价格。我们可以将这些数据制成表格如下：\n\nClick to see more\n\n\n\n居住面积（feet^2）\n价格（1000$s）\n\n\n\n2104\n400\n\n\n1600\n330\n\n\n2400\n369\n\n\n1416\n232\n\n\n3000\n540\n\n\n1985\n320\n\n\n1534\n267\n\n\n1427\n199\n\n\n1380\n245\n\n\n1494\n347\n\n\n1940\n334\n\n\n2000\n383\n\n\n1890\n339\n\n\n4478\n699\n\n\n1268\n259\n\n\n2300\n410\n\n\n1760\n350\n\n\n1450\n315\n\n\n3100\n590\n\n\n2250\n410\n\n\n2132\n399\n\n\n2596\n459\n\n\n1850\n325\n\n\n2680\n480\n\n\n1956\n349\n\n\n1604\n285\n\n\n2020\n365\n\n\n2730\n525\n\n\n2008\n375\n\n\n1537\n295\n\n\n2500\n450\n\n\n1560\n280\n\n\n2120\n389\n\n\n2200\n425\n\n\n1638\n315\n\n\n2540\n475\n\n\n2200\n408\n\n\n2070\n382\n\n\n2005\n350\n\n\n1900\n380\n\n\n2380\n420\n\n\n1320\n230\n\n\n2190\n394\n\n\n2340\n416\n\n\n2678\n540\n\n\n1966\n385\n\n\n1570\n279\n\n\n1852\n360\n\n\n2454\n485\n\n\n2205\n405\n\n\n2450\n450\n\n\n\n\n我们可以用图表来展示这些数据：\n\n\n有了这样的数据，我们可以提出一个问题：基于波特兰其他房屋的居住面积，如何预测它们的价格呢？\n为了在接下来的讨论中使用统一的符号，我们将使用表示“输入”变量（在这个例子中是生活区域的大小），也称为输入特征；表示我们试图预测的“输出”或目标变量（价格）。对于一对，我们称其为训练示例（training example）。我们将学习的数据集，由n个训练示例的列表；构成，被称为训练集（training set）。请注意，上标“(i)”仅用于表示训练集的索引，与求幂无关。我们还将使用表示输入值的空间，表示输出值的空间。在这个例子中，。\n为了更正式地描述监督学习问题，我们的目标是给定一个训练集，学习一个函数：，使得成为y的“良好”预测器。从图表中可以看出，这个过程如下所示：\n\n\n当我们试图预测的目标变量是连续的，例如在房屋价格的例子中，我们将这种学习问题称为回归问题。回归问题的目标是建立一个模型，能够对连续的目标变量进行预测。通过分析各种特征和输入变量之间的关系，我们可以推断出目标变量的数值。在回归问题中，我们通常尝试拟合一条曲线或平面，以最好地表示数据的趋势和模式。\n相比之下，当目标变量只能取少量离散值时（比如，根据居住面积预测一个住宅是房子还是公寓），我们将这种学习问题称为分类问题。分类问题的目标是根据输入变量的特征将样本分配到预定义的离散类别中。在分类问题中，我们建立一个分类器，该分类器根据输入变量的特征预测目标变量的类别。为了将不同类别的样本区分开来，分类问题通常涉及使用统计技术或机器学习算法构建决策边界。\n线性回归让我们使我们的住房示例更加丰富，加入一个稍微复杂的数据集，我们还知道每栋房子的卧室数量。\n在这个数据集中，除了居住面积外，我们还有每个房子的卧室数量。我们可以将数据表示为以下表格：\n\nClick to see more\n\n\n\n居住面积（feet^2）\n卧室数量\n价格（1000$s）\n\n\n\n2104\n3\n400\n\n\n1600\n3\n330\n\n\n2400\n3\n369\n\n\n1416\n2\n232\n\n\n3000\n4\n540\n\n\n1985\n4\n320\n\n\n1534\n3\n267\n\n\n1427\n3\n199\n\n\n1380\n3\n245\n\n\n1494\n3\n347\n\n\n1940\n4\n334\n\n\n2000\n3\n383\n\n\n1890\n3\n339\n\n\n4478\n5\n699\n\n\n1268\n3\n259\n\n\n2300\n4\n410\n\n\n1760\n3\n350\n\n\n1450\n3\n315\n\n\n3100\n4\n590\n\n\n2250\n4\n410\n\n\n2132\n4\n399\n\n\n2596\n3\n459\n\n\n1850\n4\n325\n\n\n2680\n4\n480\n\n\n1956\n3\n349\n\n\n1604\n3\n285\n\n\n2020\n4\n365\n\n\n2730\n4\n525\n\n\n2008\n3\n375\n\n\n1537\n3\n295\n\n\n2500\n4\n450\n\n\n1560\n4\n280\n\n\n2120\n4\n389\n\n\n2200\n3\n425\n\n\n1638\n3\n315\n\n\n2540\n4\n475\n\n\n2200\n3\n408\n\n\n2070\n4\n382\n\n\n2005\n3\n350\n\n\n1900\n3\n380\n\n\n2380\n4\n420\n\n\n1320\n2\n230\n\n\n2190\n4\n394\n\n\n2340\n3\n416\n\n\n2678\n4\n540\n\n\n1966\n4\n385\n\n\n1570\n3\n279\n\n\n1852\n3\n360\n\n\n2454\n4\n485\n\n\n2205\n4\n405\n\n\n2450\n4\n450\n\n\n\n\n在这里，我们将表示为二维向量，包括居住面积和卧室数量。例如，表示训练集中第个房子的居住面积，表示卧室数量。（在设计学习问题时，我们可以自行选择包括哪些特征。如果我们收集住房数据，还可以考虑包括其他特征，如壁炉数量、卫生间数量等。稍后我们将详细讨论特征选择，但现在我们将特征视为给定的。）\n为了进行监督学习，我们需要决定如何在计算机中表示函数/假设。作为初始选择，我们假设我们决定将近似为的线性函数：在这个表达式中，是参数（也被称为权重），用于参数化从到的线性函数空间。当没有混淆的风险时，我们将在中省略θ的下标，并将其简化为。为了简化表示法，我们还引入一个约定，将设置为1（这是截距项），这样我们可以表示为：在上面的表达式中，我们将和都视为向量，其中是输入变量的数量（不包括）。现在，给定一个训练集，我们如何选择或学习参数？一个合理的方法似乎是使得尽可能接近，至少对于我们拥有的训练样本来说是如此。为了形式化这个问题，我们定义了一个函数，用于衡量每个值对应的与相应的之间的接近程度。我们定义了成本函数：让我们逐步解释成本函数的每个部分：\n\n 代表我们的模型对于输入样本  的预测值。\n 是对应于输入样本  的实际观测值。\n 表示我们的模型预测值与实际观测值之间的差异，即误差。\n 将误差进行平方，这是为了消除误差的正负号，并将大误差的影响放大，以更好地衡量它们对总体误差的贡献。\n 对所有训练样本的误差平方进行求和，得到总体误差的度量。\n 为了计算方便，我们对总体误差进行了归一化，除以2。这不会影响最终优化的结果，因为我们的目标是最小化成本函数，而不是具体的数值。\n\n因此，成本函数  衡量了模型预测值与实际观测值之间的差异的平方和的一半。我们的目标是通过调整参数  的值，使得成本函数的值最小化，从而使模型的预测尽可能接近实际观测值。\n如果听不懂，没关系，我们来举一个非常简单的例子来理解成本函数到底是什么玩意。当我们建立一个模型时，我们希望它能够根据输入数据预测出正确的结果。然而，在训练模型时，我们的预测可能与实际结果有一些差距。成本函数的作用就是衡量这些预测差距的大小。\n假设你是一名学生，每天都要骑自行车去上学。你决定根据骑行时间来预测你到达学校所需的时间。你记录了过去一周的骑行时间和实际到达学校的时间，如下所示：\n\n\n\n骑行时间（分钟）\n到达时间（分钟）\n\n\n\n20\n25\n\n\n25\n30\n\n\n30\n35\n\n\n35\n40\n\n\n40\n45\n\n\n现在，你想建立一个模型，根据骑行时间预测到达时间。你选择线性函数来表示模型，即假设到达时间与骑行时间之间存在一种线性关系。\n假设我们的模型为 ，其中  表示到达时间的预测值， 表示骑行时间。我们的目标是找到最佳的参数  和 ，使得模型的预测结果尽可能接近实际观测值。\n为了衡量模型的预测与实际观测值之间的差距，我们使用成本函数。在这个例子中，我们使用均方误差（Mean Squared Error，MSE）作为成本函数。MSE 的计算方式是将每个预测值与对应的实际观测值之间的差距平方，并取所有差距平方的平均值。\n现在，我们来计算一下成本函数的值。假设我们选择了一组参数  和 。我们可以将这些参数代入模型，并计算出每个骑行时间对应的预测值。\n\n\n\n骑行时间（分钟）\n到达时间（分钟）\n预测到达时间（分钟）\n差距（预测-实际）\n差距平方\n\n\n\n20\n25\n30\n-5\n25\n\n\n25\n30\n35\n-5\n25\n\n\n30\n35\n40\n-5\n25\n\n\n35\n40\n45\n-5\n25\n\n\n40\n45\n50\n-5\n25\n\n\n现在，我们将差距平方的平均值作为成本函数的值。在这种情况下，成本函数的计算如下：我们的目标是通过调整参数  和  的值，使得成本函数的值最小化。这意味着我们希望找到最佳的参数组合，使得模型的预测结果与实际观测值之间的差距最小化。\n希望这个例子能够更加清晰地解释成本函数的概念。成本函数用于衡量模型预测与实际观测之间的差距，并帮助我们找到最佳的参数组合。\n如果你之前接触过线性回归，你可能会注意到这个熟悉的最小二乘成本函数，它导致了普通最小二乘回归模型。无论你之前是否见过它，我们将继续讨论，并最终展示它是一个更广泛的算法家族中的特例。同时，这个成本函数的意义是衡量我们的预测与实际数据之间的差距。较小的成本值表示我们的预测较接近真实数据，而较大的成本值表示预测与真实数据之间的差距较大。\n如何有任何疑问或者建议欢迎在评论区评论&gt; &lt;!!!\n","slug":"初识机器学习","date":"2023-07-16T07:02:18.000Z","categories_index":"","tags_index":"Machine Learning,笔记","author_index":"General_K1ng"},{"id":"d99fcab655887d4444838841a030a1e7","title":"正项级数","content":"这是一个非常重要的课题，我们将开始学习关于正级数，请专心听讲。\n正项级数的定义正项级数是一种无穷级数，其项均为正实数。它的形式如下：其中为级数的第个项。\np级数p级数是指形如的级数，其中是一个正实数。该级数以分母的指数来命名。\np级数具有以下性质：\n\n如果，则p级数收敛。\n如果，则p级数发散。\n\n\n\n这个结果的证明基于积分测试。为了理解为什么这是正确的，我们可以考虑函数。这个函数在区间上连续、正值且递减，因此我们可以应用积分测试来得到级数的结果：对积分进行计算，我们得到：$$\\int_{1}^{\\infty} \\frac{1}{x^p}dx  \\left{\\right.$$因此，p级数收敛当且仅当，当时发散。\n对于，p级数的和可以用黎曼ζ函数（Riemann zeta function）来表示，它的定义为：黎曼ζ函数具有许多有趣的性质，并与数论和复分析等其他数学领域有着密切的联系。\n敛散性检验有几个用于确定正项级数收敛或发散的收敛性测试。以下是其中几个常用的测试：\n1. 比较判别法比较判别法是一种通过将待定级数与已知收敛性的另一个级数进行比较来确定级数的收敛性或发散性的方法。比较法的规则如下：\n假设  和  是具有正项的级数，并且对所有 ，满足 。\n\n如果  收敛，则  也收敛。\n如果  发散，则  也发散。\n\n换句话说，如果级数  的项始终小于或等于级数  的项，并且  收敛，那么  必定收敛。相反地，如果  发散，那么  也一定发散。\n比较测试经常用于将给定的级数与 p-级数进行比较，因为 p-级数的收敛性是众所周知的。具体而言，如果我们有一个形如  的级数，以及另一个级数 ，其中的项始终小于或等于 ，那么我们可以使用比较测试来确定  的收敛性。\n\n\n\n\n\n\n\n\n\n\n\n我们常用的一些参考无限级数有：\n\n几何级数\n调和级数\np-级数\n\n2. 极限比较判别法极限比较判别法是用于确定级数的收敛性或发散性的另一种方法。与比较法类似，它涉及将给定的级数与已知收敛性的另一个级数进行比较。然而，极限比较测试在选择要进行比较的级数方面更加灵活。\n假设  和  是具有正项的级数。令 ，其中  是一个有限的正数或者是 。\n\n如果 ，则  和  要么都收敛，要么都发散。\n如果  并且  收敛，则  也收敛。\n如果  并且  发散，则  也发散。\n\n极限比较测试在选择要进行比较的级数方面更加灵活，因为我们只需要项的比值收敛到一个有限的正数。这意味着我们通常可以找到一个更易处理的级数来进行比较，相较于比较测试的情况而言。\n3. 比值判别法比值判别法是一种用于判断级数的收敛性或发散性的测试方法。比值法规定如下：\n假设  是一个具有正项的级数，令 （这个极限可能存在也可能不存在）。\n\n如果 ，则  绝对收敛。\n如果  或 ，则  发散。\n如果  或极限不存在，则比值测试无法确定收敛性或发散性，我们需要使用其他测试方法。\n\n从直观上讲，比值测试将级数的项与具有公比  的几何级数的项进行比较。如果 ，那么级数的项的衰减速度比一个收敛的几何级数的项更快，因此该级数收敛。如果 ，那么级数的项的增长速度比一个发散的几何级数的项更快，因此该级数发散。如果 ，那么级数的项的衰减速度与一个收敛的几何级数的项相同，因此测试是不确定的。\n4. 根值判别法根值判别法是一种用于判断级数的收敛性或发散性的测试方法。根值判别法规定如下：\n假设  是一个具有正项的级数，令 （这个极限可能存在也可能不存在）。\n\n如果 ，则  绝对收敛。\n如果  或 ，则  发散。\n如果  或极限不存在，则根值测试无法确定收敛性或发散性，我们需要使用其他测试方法。\n\n从直观上讲，根值测试将级数的项与一个收敛的几何级数的项进行比较，该几何级数的公比为 。如果 ，那么级数的项的衰减速度比一个收敛的几何级数的项更快，因此该级数收敛。如果 ，那么级数的项的增长速度比一个发散的几何级数的项更快，因此该级数发散。如果 ，那么级数的项的衰减速度与一个收敛的几何级数的项相同，因此测试是不确定的。\n这章节的内容非常重要，因为我们随后提到的许多概念和扩展都是基于这一章的结论，所以一定要仔细阅读。\n","slug":"正项级数","date":"2023-07-15T09:07:26.000Z","categories_index":"","tags_index":"笔记,Math","author_index":"General_K1ng"},{"id":"8c5051666bb20a88e04fe467e55f2664","title":"无穷级数的性质","content":"欢迎来到无穷级数的奇妙世界！在这一章中，我们将探索无穷级数的性质，包括收敛、发散以及我们可以对级数进行的代数运算。理解这些性质对于数学和科学的许多领域都是至关重要的，它使我们能够做出准确的预测，解决重要的问题，并发展新的计算方法。所以，让我们一起深入探索无穷级数的惊人性质吧。\n几个重要的无穷级数除了探索无穷级数的性质，我们还将介绍和研究一些在数学中最重要的级数。其中包括几何级数，在微积分中有许多重要的应用，以及调和级数，它是一个经典的发散级数的例子。我们还将研究其他重要的级数，如交错级数和泰勒级数，在数学和科学的许多领域都有广泛的应用。所以，准备好一起探索无穷级数的性质，以及数学中一些最重要的级数吧！\n几何级数几何级数是一种特殊类型的无穷级数，其中每一项都是前一项的常数倍（说白了就是等比数列）。几何级数的一般形式为其中  是第一项， 是公比。几何级数在数学中很重要，在科学和工程中也有许多应用。\n推导通项公式为了推导出几何级数的求和公式，我们从考虑级数的部分和开始。设  为级数的前  项和，那么我们可以将方程的两边都乘以  得到将第二个方程从第一个方程中减去，我们得到这可以简化为如果 ，我们可以将两边都除以  得到这个公式给出了几何级数的前  项和。\n要求解无穷几何级数的和，我们取  趋向于无穷的极限：如果 ，那么当  趋向于无穷时， 趋向于零，所以极限简化为这就是当  时无穷几何级数的和的公式。\n几何级数的敛散性几何级数的收敛性取决于公比  的值。我们可以将几何级数分为三类：\n\n如果 ，那么级数绝对收敛。这意味着级数收敛且和是有限的。\n如果 ，那么级数可能收敛，也可能发散。在这种情况下，我们需要观察级数的项的行为来确定收敛性或发散性。\n如果 ，那么级数发散。这意味着级数的和是无穷的。\n\n例如，考虑级数\n这是一个公比为 ， 的几何级数。由于 ，该级数绝对收敛，和为\n\n\n\n\n\n\n\n\n\n这个图像生动地展示了为什么这个几何级数的和是2。当然，为了方便起见，我们让级数的第一项从  开始。每次将图像分成两半并取剩余部分的一半，即四分之一，依此类推，我们发现图像的总面积仍然是1，直到无穷项。\n几何级数的应用几何级数在数学、科学和工程中有许多应用。例如，它们可以用来模拟指数增长或衰减，如人口增长或放射性物质的衰变。它们还可以用于计算某些类型的积分和近似函数。\n结论几何级数是微积分中的一个基本概念，在各个领域中都有许多应用。理解几何级数的收敛和发散对于使用闭合解找到级数的和非常重要。\n调和级数调和级数是数学中一个众所周知的级数，它在许多不同的背景下自然出现，包括微积分、数论和物理学。特别地，它是以下形式的级数：\n调和级数之所以有趣，是因为它是一个发散的级数，也就是说它没有有限的和。这可以通过检查级数的部分和来看出，随着添加更多的项，部分和会无限增长。\n敛散性的证明为了证明调和级数的发散性，我们可以使用积分测试。积分测试表明，如果函数  对于所有  是正的、递减的和连续的，并且对于所有 ，有 ，那么级数  和不定积分  要么都收敛，要么都发散。\n对于调和级数，我们可以选择 ，它满足积分测试的条件。不定积分  可以计算为：由于该积分发散，调和级数也必定发散。\n\n\n可以看到，每个矩形的面积都是 ，所以前  个矩形的总面积是 。随着  的增加，矩形的面积逐渐接近 ，因此阶梯状的图形逐渐趋近于斜率为  的曲线。最终，当  趋向于无穷时，矩形的面积趋向于 ，阶梯状的图形趋向于曲线 。\n这种可视化可以帮助我们更好地理解级数的行为以及它与自然对数的关系。\n当然，如果你无法理解这个图像以及以上所说的证明过程，没关系，你只需要记住调和级数是发散的。\n调和级数的应用调和级数的发散性在数学和科学中有许多重要的应用，例如：\n\n调和级数在数论中被用来研究质数的分布。级数的发散意味着质数是无穷多的，这是数论中的一个基本结果。\n调和级数的发散性在物理学中也有重要的应用，特别是在电场的研究中。点电荷在电场中的电势能与点电荷与电场中所有其他电荷之间距离的倒数之和成正比。这个和等价于调和级数，它的发散性意味着点电荷的电势能是无限大的。\n\n结论总之，调和级数是数学和科学中重要而有趣的级数。它是一个发散的级数，也就是说它没有有限的和。我们可以使用积分测试来证明级数的发散性，而级数的发散性在数论和物理学中有重要的应用。\n无穷级数的一些性质\n如果给定的无穷级数  收敛，那么任何形式为  的级数，其中  是常数，也将收敛。这个性质可以通过序列极限的定义得出。\n\n\n\n\n\n\n\n\n\n\n\n我们可以看到  收敛，并且  也收敛，它们的收敛性与  相同。\n因此，这个定理可以用来证明  收敛，因为它是通过将  乘以常数 0.5 而得到的。\n\n为了证明这个性质，我们可以设  是级数  的部分和序列， 是级数  的部分和序列。那么我们有：和将第一个等式乘以 ，我们得到：现在，设  是序列  的极限，即 。由于级数  收敛，我们知道极限  是有限的。因此，根据极限的代数性质，我们有：类似地，序列  的极限为：因此，级数  也收敛，其和为 。\n综上所述，无穷级数的线性性质表明，如果给定的无穷级数收敛，那么通过将原级数的项乘以一个常数得到的任何级数也将收敛，其和将是该常数与原级数和的乘积。这个性质可以通过序列极限的定义和极限的代数性质得到。\n\n\n\n如果  和  收敛，那么  也收敛。\n\n\n\n\n\n\n\n\n\n\n\n\n\n我们可以看到  和  都收敛，而  也收敛，它们的收敛性与  和  相同。\n因此，这个定理可以用来证明  收敛，因为它是  和  的和。\n\n要证明这个结果，我们可以使用以下步骤：\n\n令 。那么  是由  和  对应的项相加得到的级数。\n\n由于  和  都收敛，它们的部分和序列  和  也收敛。即， 和 。\n\n我们想要证明  收敛。为了做到这一点，我们需要证明  的部分和序列  收敛。也就是说，我们需要证明  存在。\n\n我们可以将  表示为  和  的形式：\n\n根据极限的性质，我们有：\n\n因此，我们已经证明了  的部分和序列  收敛，因此级数  收敛。\n\n由于  被定义为 ，所以这个结果对于  和  两种情况都成立。也就是说，如果  和  收敛，那么  和  也收敛。\n\n\n\n\n\n对于仍然在任意添加括号后保持收敛的无穷级数\n\n​\t\t如果您有Python环境，我强烈建议您运行我提供的”Verifying_Property_3” Python文件，您可以看到在不同括号下的图像的差异。\n\n要了解这个结论为什么成立，考虑一个收敛于极限的无穷级数。也就是说，偏和数列，其中，当趋向于无穷时收敛于。\n现在，假设我们以任意方式给级数的项添加括号，也就是说，我们用括号将某些项分组，但不改变项的顺序。例如，我们可以写成：或者或者任何其他方式的分组。\n让我们用表示新的偏和数列，其中是前个分组的和。例如，在上述第一种分组中，我们有，，，依此类推。\n现在，考虑级数中的任意两个相邻的分组。我们将第一个分组中的项称为，将第二个分组中的项称为。那么这两个分组的和为：根据加法的结合律，我们可以重新排列这个和为：也就是说，我们可以以任何希望的方式将这些项分组，得到的和仍然相同。因此，新的分组的偏和数列与原始的偏和数列相同。换句话说，在收敛级数的项上添加括号不会改变级数的极限。\n因此，我们已经证明了对于收敛的无穷级数，在其项上添加任何括号后，级数仍然收敛，且极限不变。\n\n\n​\t\t需要注意的是，这个性质只适用于收敛的级数。对于发散的级数，重新排列项可能会导致不同的收敛性质。\n\n通过删除、添加或更改有限项，级数的收敛性不会改变，但和可能会改变。\n\n\n要正式证明这个性质，让我们考虑一个无穷级数，其中是级数的第个项。我们想要证明，如果我们通过添加、删除或更改任意有限数量的项来修改这个级数，级数的收敛性或发散性不会改变。\n首先，让我们考虑添加或删除有限数量的项的情况。设为原级数的原和，为修改后级数的新和。我们可以将表示为两个级数的和：第一个级数是原级数删除或添加有限数量的项后得到的，第二个级数是由被删除或添加的项组成的有限级数。形式上，我们可以写成：这里，是正整数，是非负整数，是修改后级数的第个项。\n由于原级数收敛于，我们有：现在，让我们考虑修改后的级数。修改后级数的第一部分收敛于与原级数相同的极限，因为它们之间只有有限数量的项不同。第二部分是一个有限级数，因此它收敛于一个有限和。因此，修改后的级数也收敛于和。\n因此，我们已经证明，如果我们通过添加或删除有限数量的项来修改一个无穷级数，级数的收敛性或发散性不会改变。\n接下来，让我们考虑更改有限数量的项的情况。假设我们通过将第个项更改为来修改级数，其中。设和分别为原和和修改后的和。那么，我们可以写成：由于和之间的差是一个有限数，级数的收敛性或发散性不会改变。然而，级数的实际和是不同的。\n总而言之，”通过删除、添加或更改有限项，级数的收敛性不会改变，但和可能会改变”是无穷级数理论中的一个基本结果。它告诉我们，我们可以通过添加、删除或更改有限数量的项来修改一个无穷级数，而不影响其收敛性或发散性。然而，级数的实际和可能会有所不同。\n\n\n\n级数收敛的必要条件是。换句话说，如果是一个收敛的级数，那么。\n\n\n要证明这个结果，假设是一个收敛的级数。根据定义，这意味着偏和序列收敛到某个有限极限。\n我们可以将级数的第个项表示为两个相邻偏和的差：将两边取极限，当时，我们得到：这里我们利用了序列收敛于的事实，因此和。\n因此，我们已经证明了如果是一个收敛的级数，那么。\n值得注意的是，这个结果的逆命题不一定成立。也就是说，仅仅因为，并不意味着是收敛的。例如，调和级数是发散的，尽管。因此，条件只是收敛的必要条件，而不是充分条件。\n\n\n当然，我们要求你不必记住或完全理解这些性质的推导过程，这些只是为了帮助你理解。在考试中，你只需要记住加粗的文字，这应该不难，对吗？\n","slug":"无穷级数的性质","date":"2023-07-15T07:46:56.000Z","categories_index":"","tags_index":"笔记,Math","author_index":"General_K1ng"},{"id":"fca5d0f1b5b938fde80803f8dc301aa3","title":"我的博客之旅：学习、分享与成长","content":"\n\n\n\n\n\n\n\n\n欢迎来到我的博客！我是金洪来，目前就读于西交利物浦大学，专业是信息与计算科学（ICS）。在我充满好奇心和激情的探索中，我决定搭建这个博客，与大家分享我的学习经历、见解和成果。对我来说，这是一次意义非凡的旅程，我希望能够与你们一起成长、互相学习和建立联系。\n自我介绍我是一个对新兴技术充满热情的学生，同时也热爱广泛阅读。在我信息与计算科学的学习中，我不仅学习了编程语言和算法，还探索了许多有趣的领域，如人工智能、数据科学和网络安全。我迷恋于技术的力量和它所带来的无限可能。通过博客，我希望能够与大家分享我的学习经验、项目经历以及对技术和生活的思考。\n搭建博客的动机我为什么突然决定搭建博客呢？首先，学习新技术时，我发现将所学知识记录下来是一种非常有效的方法，可以帮助我更好地理解和巩固所学内容。通过博客，我可以分享这些知识，并与志同道合的人们交流和互动，从中收获更多的见解和观点。其次，博客可以成为一个平台，让我与专业人士和技术大佬们建立联系。我希望能够从他们的经验中获得启发和指导，进一步提升自己的技术能力和职业发展。最重要的是，我相信博客是一个分享和学习的场所，我希望能够通过博客与读者们一起成长，共同进步。\n目标受众我的博客欢迎所有学生、志同道合的朋友、技术大佬和技术小白们。无论你是正在学习编程的初学者，还是想深入了解某个领域的专业人士，我都希望我的博客能够为你提供有价值的信息和灵感。在我的博客中，你将找到关于技术教程、项目经验、学习心得，以及我对生活和工作的见解和体验分享。我希望能够为你提供有趣且实用的内容，无论你身在何处、经历如何，我都希望我的博客能够成为你学习和成长的伙伴。\n博客内容计划在我的博客中，你将看到各种内容，涵盖了技术教程、项目经验、学习笔记以及我对生活和工作的见解和体验。我计划分享一些简单易懂的技术教程，帮助初学者快速入门。通过清晰的代码示例和详细的解释，我希望能够让你轻松理解复杂的概念和技术。同时，我也会分享一些我在项目中遇到的挑战和解决方法，以及学习过程中的心得体会。我相信通过分享这些经验，我们可以相互帮助和共同成长。此外，我会不定期地分享一些生活感悟和工作经验，希望能够给你带来一些启发和思考。\n学习和成长通过搭建博客，我期望自己能够不断学习和成长。通过与读者的互动和交流，我相信我可以提升自己的学习能力和沟通技巧。我希望能够给你提供有价值的内容和支持，帮助你解决问题、开拓思路。同时，我也希望能够通过博客启发更多的人，鼓励他们追求自己的梦想，勇敢地探索未知的领域。在这个博客的旅程中，我将不断挑战自我，拓宽自己的知识边界，并将这些经验和成长与大家分享。\n结语感谢你花时间阅读我的博客介绍。我希望你能够加入我的博客之旅，与我一起探索技术的奇妙世界，共同成长和学习。如果你对我的博客感兴趣或有任何问题、建议或想法，请随时与我联系。我期待与你在博客中互动，并希望我的博客能够对你有所帮助。谢谢！\n","slug":"我的第一篇博客","date":"2023-07-14T09:16:53.000Z","categories_index":"","tags_index":"随笔","author_index":"General_K1ng"}]